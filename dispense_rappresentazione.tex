\documentclass[11pt]{article}


\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[a4paper]{geometry}
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{array}
\usepackage{xy}
\usepackage{multicol}
%\usepackage{slashbox}
\usepackage{fancyhdr}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[T1,OT1]{fontenc} 
\usepackage[nohug,small]{diagrams}
\usepackage{bm}
\usepackage{faktor}

%\usepackage{genyoungtabtikz}
\usepackage{ytableau}

\usepackage{grffile}
\usepackage{tikz}
\usepackage{pgf,tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc}

\usetikzlibrary{arrows}
\topmargin 0cm
\oddsidemargin 0cm
\evensidemargin 0cm
\textwidth 16.5cm
\textheight	23.5cm
\marginparwidth 2cm
\marginparpush 2cm



\title{Dispense del corso di Teoria della Rappresentazione}
\author{Fabio Zoratti}
\date{\today}



\makeindex

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{post}[thm]{Postulato}
\newtheorem*{cor}{Corollario}

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{prob}{Problema}[section]
\newtheorem{exercise}{Esercizio}[section]
\newtheorem{hint}{Suggerimento}[section]
\newtheorem{sol}{Soluzione}[section]
\newtheorem*{rem}{Osservazione}

\theoremstyle{remark}
\newtheorem*{note}{Nota}





\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}
\newcommand*\quot[2]{{^{\textstyle #1}\big/_{\textstyle #2}}}


\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\Supp}{Supp}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\ord}{ord}




\newcommand{\tridiag}[6]{
	  \begin{diagram}
	  #1 & \rTo^{#2}  & #3        \\
	     & \rdTo_{#6} & \dTo>{#4}   \\
	     &          & #5
	  \end{diagram}
}  
\newcommand{\quaddiag}[8]{
	\begin{diagram}
	#1     & \rTo^{#2} & #3 \\
	\dTo<{#6} &         & \dTo>{#4} \\
	#7     & \rTo^{#8} & #5
	\end{diagram}
}
















\begin{document}
\maketitle
\tableofcontents



\newpage
\section{Teoria dei gruppi}

\begin{defn}[Gruppo] Un gruppo è un insieme dotato di un'operazione binaria $\cdot : G\times G \to G$ che gode delle seguenti proprietà:
\begin{enumerate}
	\item Associatività: presi comunque $a,b,c\in G$ vale che $(a\cdot b)\cdot c = a\cdot(b\cdot c)$
	\item Esiste $e\in G$, chiamato \emph{unità}, o \emph{identità}, o \emph{elemento neutro}, tale che $\forall a\in G$ vale $e\cdot a = a = a\cdot e$
	\item Per ogni $a\in G$ esiste un $a'$ tale che $a'\cdot a$ e $a\cdot a'$ sono unità, ovvero si comportano come l'elemento $e$ al punto precedente.
		  Un tale $a'$ si dice \emph{inverso} di $a$.
\end{enumerate}
Per comodità di solito si omette il puntino. Se $G$ è finito, $card(G) = n$, si dice che $G$ ha \emph{ordine} $n$.
\end{defn}

\paragraph{Esempi}
\begin{enumerate}
	\item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}$ con l'operazione di somma.
	\item $\mathbb{Q}^*, \mathbb{R}^*, \mathbb{C}^*$ con l'operazione di moltiplicazione (senza lo 0).
	\item $GL_n(\mathbb{R})$ oppure $GL(V)$
	\item $f:I\to I $ biunivoca, con $I$ insieme e con l'operazione di composizione. Nel caso in cui $I$ sia un insieme finito, tanto vale scegliere $I = \{1,2,3,\ldots, n\}$. In tal caso questo gruppo si chiama $S_n$.
\end{enumerate}

\paragraph{Alcuni teoremi elementari}
\begin{enumerate}
	\item L'unità $e$ è unica.
	
	Dimostrazione: supponiamo che $e$ ed $e'$ siano entrambe unità. Allora vale
	
	\[e = ee' = e' \]
	
	\item Dato $a\in G$, l'inverso di $a$ è unico (e usualmente si denota con $a^{-1}$).

	Dimostrazione: supponiamo che $a', a''$ siano entrambi inversi di $a$. Allora
		
	\[(a' a)a'' = a'(aa'') \implies e a'' = a' e \implies a'' = a' \]
	
	\item Dati $a_1, a_2, \ldots, a_n$, il prodotto $a_1\cdot a_2 \cdots\cdot a_n$ è ben definito senza bisogno di parentesi.
	\item Se $ab = e$, allora anche $ba = e$, dunque $a$ e $b$ sono uno l'inverso dell'altro.

	Dimostrazione: $ba = bae = babb^{-1} = beb^{-1} = bb^{-1} = e$.
	
	\item Dato un intero positivo $k$ e un elemento $a\in G$, definiamo $a^k=\underbrace{a\cdot a \cdots\cdot a}_{k\text{ volte}}$.
	Inoltre poniamo $a^0 = e$ e infine $a^{-k} = (a^{-1})^k$, così abbiamo definito le potenze con esponente in $\Z$.
	Non è difficile dimostrare che, se $k,h$ sono interi (non necessariamente positivi), valgono le usuali proprietà: 
	\[a^{k+h} = a^k \cdot a^h \quad\quad\quad (a^k)^h = a^{kh} \]
	Però non è vero in generale che $(ab)^k = a^kb^k$ (sarebbe vero se l'operazione fosse commutativa). Osserviamo infine che 
	\[ (ab)^{-1} = b^{-1}a^{-1}\]
	infatti $(ab)(b^{-1} a^{-1}) = a(bb^{-1})a^{-1} = aea^{-1} = aa^{-1} = e$.
\end{enumerate}



\begin{defn}[Sottogruppo]
Sia $G$ un gruppo, $H\subseteq G$ si dice sottogruppo di $G$ se:
\begin{itemize}
	\item $e\in H$
	\item $x,y\in H \implies xy\in H$
	\item $x\in H \implies x^{-1}\in H$
\end{itemize}
e si indica $H \leq G$. In altre parole $H$ è sottogruppo se,
ereditando l'operazione di $G$, è esso stesso un gruppo.
\end{defn}

\begin{exmp}[Sottogruppo generato da un elemento]
Sia $G$ un gruppo e $a$ un suo elemento. L'insieme delle potenze di $a$, ovvero $\{a^k | k\in\Z\}$, è un sottogruppo di $G$,
che di solito viene denotato con $\langle a\rangle$.
\end{exmp}

\begin{rem}
Se $G$ è gruppo, $a\in G$ ed esiste un intero $n>0$ tale che $a^n=e$, allora tutti gli elementi di $\langle a\rangle$ sono della forma
$a^k$ per qualche $0\le k < n$. Infatti se si considera un qualsiasi $a^s$ con $s\in\Z$, si può scrivere $s=nq+r$ con $0\le r<n$.
Allora \[a^s=a^{nq+r} = (a^n)^qa^r = e^qa^r = a^r\]
Se $n$ è il minimo intero positivo tale che $a^n=e$, allora si dice che $a$ ha \emph{ordine} $n$. In tal caso è facile verificare che
l'insieme $\langle a\rangle$ contiene esattamente $n$ elementi distinti, ovvero $a^0, a^1, \dots a^{n-1}$.
Infatti, se fosse $a^i=a^j$ con $0\le i<j<n$, allora $a^{j-i}=e$, che sarebbe assurdo siccome $0<j-i<n$.

Se $\langle a\rangle$ è finito (il che è certo se ad esempio $G$ è finito) allora di sicuro esiste $n>0$ tale che $a^n=e$. Infatti basta prendere $0\le i < j$
tali che $a^i = a^j$ e osservare che $a^{j-i} = e$. Questi $i$ e $j$ esistono per forza perché se tutte le potenze fossero distinte allora $\langle a\rangle$ sarebbe infinito.
\end{rem}


\begin{defn}[Sottogruppo normale]
Sia $G$ un gruppo, $H \leq G$ si dice \emph{normale} in $G$ se
\[
	\forall h\in H, \forall g\in G\qquad ghg^{-1}\in H
\]
e si indica $H \trianglelefteq G$.
\end{defn}

\begin{defn}[Laterale]
	Sia $G$ un gruppo e $H<G$ un suo sottogruppo, definiamo \textit{laterale destro} o \textit{classe laterale destra} di $H$
	un sottoinsieme di $G$ del tipo
	\[
		gH=\{ gh\ |\ h\in H\}
	\]
\end{defn}

\begin{defn}[Quoziente]\label{defn:quoziente}
	Sia $G$ un gruppo, $H<G$, chiamiamo \textit{quoziente} di $G$ per $H$ l'insieme delle classi laterali di $H$, che indicheremo con $G/H$, ovvero
	\[
		G/H=\{gH\ |\ g\in G\}
	\]
	dove vengono identificati gli insiemi uguali (infatti non è detto che se $g,g'\in G$, con $g\neq g'$, allora $gH\neq g'H$).
\end{defn}

\begin{rem}
	Dato $G$ un gruppo, $H<G$, non è difficile mostrare che tutte le classi laterali di $H$ in $G$ hanno la stessa cardinalità, in particolare hanno tutte la cardinalità della classe $eH$, ma $eH=H$ come insieme, quindi tutte le classi laterali di $H$ hanno la stessa cardinalità di $H$.
\end{rem}


\begin{thm}
	Il quoziente di un gruppo $G$ per un suo sottogruppo $H$ fornisce una partizione di $G$: per ogni $g\in G$ esiste un unico $H$-laterale destro $g'H$ tale che $g\in g'H$. 
\end{thm}
\begin{proof}
	Si osserva che $g\in gH$ visto che $gH=\{gh\ |\ h\in H\}$ e che $e\in H$, se si avesse che $g\in \alpha H$ allora $g=\alpha h_1$ per qualche $h_1$. Si osserva allora che i due laterali coinciderebbero:
	\[
		\alpha H=\{ \alpha h\ |\ h\in H\} = \{ \alpha h_1 h\ |\ h\in H \} = \{ gh\ |\ h\in H\} = gH
	\]
\end{proof}

\begin{thm}[Teorema di Lagrange]
	Sia $G$ un gruppo finito, $H<G$, allora $|H|$ divide $|G|$ e, in particolare, $\displaystyle |G/H|=\frac{|G|}{|H|}$; il numero $|G/H|$ viene chiamato \textit{indice} di $H$ in $G$.
\end{thm}


\begin{defn}[Gruppo quoziente]
	Sia $G$ gruppo, $H\trianglelefteq G$ (osservare che si richiede che il sottogruppo sia \textit{normale}), allora chiameremo \textit{gruppo quoziente} di $G$ su $H$ l'insieme quoziente come l'abbiamo definito \eqref{defn:quoziente} munito della seguente operazione:
	\[
		(g_1H)\cdot(g_2H)=g_1g_2H
	\]
	Non riportiamo la dimostrazione del fatto che l'operazione così definita rispetti effettivamente gli assiomi dei gruppi.
\end{defn}

\begin{rem}
	Attenzione a non farsi ingannare: ci si può chiedere se, dato $G$ gruppo con $H,K\trianglelefteq G$ tali che $H\cong K$, si possa concludere che $G/H\cong G/K$. Questo in generale è \textbf{falso}! Come esempio si può prendere $G=\Z$, $H=5\Z$ e $K=7\Z$, dove
	\[
		5\Z=\{ 5t|\ t\in \Z\}\qquad 7\Z=\{ 7t|\ t\in \Z\}
	\]
	Infatti evidentemente $H\cong \Z\cong K$, ma $G/H\cong \Z_5$ mentre $G/K\cong \Z_7$\footnote{Esistono controesempi anche con gruppi finiti.}.
\end{rem}


\begin{defn}[Classi di coniugio]
Sia $G$ un gruppo, $x \in G$, la classe di coniugio di $x$ è l'insieme $\{ gxg^{-1} | g\in G \}$. Si dimostra facilmente che le classi di coniugio di tutti gli elementi di $G$ formano una partizione del gruppo stesso. Si osserva inoltre che un sottogruppo è normale se e solo se è unione di classi di coniugio (\textsc{Attenzione:} è raro che unendo a caso classi di coniugio si ottenga un sottogruppo).
\end{defn}


\begin{exmp}[Le classi di coniugio di $GL_n(\C)$]
Nel caso del gruppo $GL_n(\C)$ due matrici stanno nella stessa classe di coniugio se e solo se sono simili, quindi per ogni classe di coniugio esiste un rappresentante canonico che è la forma di Jordan di una qualsiasi matrice nella classe (con opportune convenzioni sull'ordine dei blocchi e degli autovalori).
\end{exmp}

\begin{defn}[Centro di un gruppo]
	Sia $G$ un gruppo, il \textit{centro} di $G$ si indica con $Z(G)$ ed è il sottoinsieme degli elementi che commutano con tutto $G$:
	\[
		Z(G)=\{ h\in G\ |\ hg=gh\ \forall g\in G \}
	\]
	\`E immediato verificare che $Z(G)$ è un sottogruppo normale di $G$.

\end{defn}

\begin{defn}[Prodotto diretto di gruppi]
Siano $G$ e $H$ gruppi. Si definisce prodotto diretto di $G$ e $H$ il gruppo formato dall'insieme $G \times H = \{ (g, h) | g \in G, h \in H\}$ con l'operazione componente per componente, ovvero separatemente per i due gruppi di partenza.
\end{defn}


\begin{defn}[Omomorfismo (isomorfismo) di gruppi]
Siano $G$ ed $H$ gruppi, un'applicazione $\varphi:G\to H$ si dice \textit{omomorfismo di gruppi} se
\[
	\forall g_1,g_2\in G\qquad \varphi(g_1 g_2)=\varphi(g_1)\varphi(g_2)
\]
dove la prima moltiplicazione è fatta in $G$ mentre la seconda in $H$.
Se $\varphi$ è bigettiva, allora si dice \textit{isomorfismo}, e i due gruppi si dicono \emph{isomorfi}.
Indichiamo con $\Hom(G,H)$ l'insieme degli omomorfismi da $G$ ad $H$.
\end{defn}

\begin{defn}
	Siano $G$ e $H$ gruppi, $f:G\to H$ un omomorfismo di gruppi, allora definiamo
	\begin{gather*}
		\Ker f = \{g\in G\ |\ f(g)=e_H\}\\
		\Imm f =\{ h\in H\ |\ \exists g\in G\text{ t.c. }f(g)=h\}
	\end{gather*}
	Non è difficile verificare che sia $\Ker f$ che $\Im f$ sono sempre sottogruppi rispettivamente di $G$ e di $H$, inoltre si può osserevare che $\Ker f$ è un sottogruppo normale di $G$.
\end{defn}

\begin{rem}
	Non è difficile dimostrare che, dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, esso è \textit{iniettivo} se e solo se $\Ker f = \{e\}$.
\end{rem}


\begin{thm}[Primo teorema di omomorfismo]\label{alg:primo_teo_omo}
	Dati due gruppi $G$ e $H$ e un omomorfismo $f:G\to H$, vale che
	\[ G/\Ker f \cong \Imm f\]
\end{thm}
\begin{rem}
	Se la $f$ del teorema precedente è iniettiva, allora $G/\Ker f\cong G$ e quindi $G\cong \Imm f$. Invece se $f$ è surgettiva, allora $G/\Ker f\cong H$.
\end{rem}



\begin{defn}[Azione di un gruppo su un insieme] Sia $G$ un gruppo e $I$ un insieme. Chiamiamo azione $a$ di $G$ su $I$ una funzione $a:G\times I \to I$ che rispetti la regola di composizione, ovvero che se $h,g\in G$ e $i \in I$, valga
\[ a(h,a(g,i)) = a(hg, i) \]
Normalmente si usa una notazione abbreviata in cui invece di scrivere $a(g,i)$ si scrive direttamente $g\cdot i$ o addirittura $gi$


\label{defn:azione}
\end{defn}


\begin{defn}[Azione transitiva]
Un'azione di un gruppo $G$ su un insieme $I\neq \emptyset$ si dice \textit{transitiva} se $\forall\ i,j\in I\ \exists s\in G$ t.c. $j=s\cdot i$.
\label{defn:azione transitiva}
\end{defn}


\begin{defn}[Orbita di un elemento]
Sia $G$ un gruppo che agisce sull'insieme $I$, dato $x\in I$ si chiama \textit{orbita} di $x$ in $G$ l'insieme $\Orb_{G}(x)=\{ g\cdot x\ |\ g\in G \}$, se il gruppo utilizzato è chiaro si può scrivere semplicemente $\Orb(x)$. Si osserva subito che un'azione è transitiva se e solo se induce una unica orbita.
\label{defn:orbita}
\end{defn}


\begin{rem} Le orbite di un gruppo $G$ sull'insieme $I$ formano una partizione dell'insieme. La verifica non è difficile. Vale inoltre la formula $|\Orb_G(x)| \cdot |\Stab_G(x)| = |G|$
\end{rem}



\begin{rem}
	Un gruppo $G$ può agire su se stesso per coniugio, ovvero dati $g\in G$ (qui $G$ è pensato come gruppo che agisce) e $x\in G$ ($G$ pensato come insieme),
	si pone $g\cdot x = gxg^{-1}$. Non è difficile verificare che si tratta davvero di una azione.
	Osserviamo che le classi di coniugio sono le orbite degli elementi generate mediante l'azione per conugio.
\end{rem}




\begin{defn}[Azione semplicemente transitiva]
Un'azione di $G$ su un insieme $I\neq \emptyset$ si dice \textit{semplicemente transitiva}
se presi comunque $i,j\in I$ esiste un unico $s\in G$ tale che $j=s\cdot i$.
\end{defn}


\begin{defn}[Funzione $G$ equivariante]
Dato un gruppo $G$ che agisce su due insiemi $I$ e $J$, una funzione $\phi: I \to J$ si dice $G$ equivariante se 
\[ \phi(s \cdot_I i) = s \cdot_J \phi(i) \qquad \forall s \in G, \ \ \forall i \in I \]
\end{defn}





\newpage
\subsection{Proprietà dei gruppi ciclici}

\begin{defn}[Gruppo ciclico] Un gruppo $G$ si dice ciclico se esiste un elemento $a\in G$ tale che ogni
elemento di $G$ è una potenza di $a$, ovvero $G=\langle a\rangle$. Si dice che $a$ è un generatore di $G$.
\end{defn}

\begin{rem}
Sia $G$ un gruppo ciclico di cardinalità $n$ e generatore $a$. Allora $n$ è il più piccolo intero positivo tale che $a^n = e$,
e ogni elemento di $G$ si scrive in modo unico come $a^k$ con $0\le k < n$.
\end{rem}

\begin{exmp}[Radici dell'unità]
Dato $n>0$ intero, l'insieme $\mu_n\subset \C^*$ delle radici $n$-esime dell'unità è un gruppo ciclico con $n$ elementi.
\end{exmp}

\begin{rem} Se $n$ è un intero positivo esiste (a meno di isomorfismo) un unico gruppo ciclico di cardinalità $n$.
Abbiamo già visto che esiste (basta considerare $\mu_n$),
inoltre dati due gruppi ciclici di cardinalità $n$ e generatori rispettivamente $a$ e $b$ è immediato costruire un
isomorfismo $f:\langle a\rangle\to\langle b\rangle$ ponendo $f(a^k) = b^k$ per $0\le k < n$.
\end{rem}


\begin{prop} Sia $C_n$ un gruppo ciclico di cardinalità $n$. Allora
\[ n = card(\Hom(C_n,\C^*))\]
\end{prop}
\begin{proof} Sia $a$ un generatore di $C_n$. Fissato $\omega\in\mu_n$ posso definire
una funzione $f:C_n\to\C^*$ ponendo $f(a^k) = \omega^k$ per $0\le k < n$.
Verifichiamo che $f\in \Hom(C_n, \C^*)$. A tal fine prendiamo due elementi di $C_n$, che sono della forma $a^k, a^h$ per certi interi $0\le k,h < n$.
\[f(a^k \cdot a^h) = f(a^{k+h}) = \omega^{k+h} = \omega^k \omega^h = f(a^k)f(a^h)\]
Dunque $f$ è omomorfismo. Variando la scelta di $\omega\in\mu_n$ si producono effettivamente $n$ omomorfismi differenti (infatti se $\omega$ cambia allora cambia anche $f(a)$).
Mostriamo che non ci sono altri omomorfismi oltre a questi.
Sia $f\in \Hom(C_n,\C^*)$. Visto che $a^n=e$, deve valere $f(a)^n = f(a^n) = 1$. Allora $f(a)$ deve essere una radice $n$-esima
dell'unità, che chiamiamo $\omega$. A questo punto il fatto che $f$ è omomorfismo implica che $f(a^k) = \omega^k$ per ogni intero $k$.
\end{proof}


\subsection{Proprietà dei gruppi abeliani}
\begin{defn}[Gruppo abeliano] Un gruppo $G$ si dice abeliano se l'operazione di gruppo è commutativa, cioè $\quad\forall a,b\in G\quad ab=ba$.
\end{defn}

\begin{rem} Un gruppo ciclico è sempre abeliano.
\end{rem}

Potrebbe essere utile conoscere il seguente risultato, la cui dimostrazione richiederebbe una conoscenza più approfondita della teoria dei gruppi.
\begin{thm}Ogni gruppo abeliano finito è isomorfo al prodotto diretto di gruppi ciclici.
\end{thm}

\begin{rem} Sia $G$ un gruppo abeliano. Allora 
\[ |G| = card(\Hom(G,\C^*))\]

La dimostrazione si ottiene ricordando che $G$ è prodotto diretto di gruppi ciclici e facendo un ragionamento simile a quello
della proposizione analoga per gruppi ciclici.
Se invece $G$ non è abeliano allora nella formula precedente all'uguale va sostituito un $>$.
\end{rem}




\subsection{Proprietà del gruppi simmetrici}
Il gruppo simmetrico $S_n$ è stato introdotto come l'insieme delle funzioni bigettive da $\{1,2,\dots,n\}$ in sé, dotato dell'operazione di composizione.
Dunque $S_n$ agisce in modo naturale su $\{1,2,\dots,n\}$, permutandone gli elementi. Per descrivere un elemento $\sigma \in S_n$ 
è spesso conveniente usare la notazione di prodotto di cicli disgiunti, che ora descriviamo informalmente.

Si comincia a costruire la lista $(1, \sigma(1), \sigma^2(1), \dots)$. Visto che abbiamo a disposizione un numero finito di elementi,
ad un certo punto sarà $\sigma^k(1) = 1$. Allora se scriviamo $(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ tutti i numeri tra le 
parentesi saranno diversi tra loro (questo segue dal fatto che $\sigma$ è bigettiva). Inoltre ognuno dei numeri scritti viene mandato da $\sigma$
nel numero immediatamente successivo nella lista, e l'ultimo numero viene mandato nel primo. Quello che abbiamo appena scritto è un \emph{ciclo}.
\`E anche possibile che la lista sia semplicemente $(1)$, il che vorrebbe dire che $1$ viene lasciato fisso da $\sigma$.
Se avessimo cominciato il procedimento con $\sigma(1)$ al posto di $1$ avremmo ottenuto $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$, che
descrive ugualmente bene il modo in cui $\sigma$ sposta gli elementi scritti. Anche se la scrittura è diversa, per noi
$(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ e $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$ sono esattamente lo stesso ciclo,
e un ragionamento analogo vale per gli altri numeri facenti parte della lista: non importa da quale si parte.
Può darsi che non tutti i numeri da $1$ a $n$ compaiano nel ciclo appena scritto: in tal caso si prende un numero ancora non scritto e si ricomincia da capo
da lui, creando una nuovo ciclo, e si continua così finché non sono stati scritti tutti i numeri.
Alla fine ci ritroviamo un elenco di cicli che sono necessariamente disgiunti per via della bigettività di $\sigma$.
\`E facile convincersi che in questo modo si descrive completamente $\sigma$. Inoltre a meno di variare
l'ordine con cui sono scritti i cicli e di cambiare i ``punti di partenza'' dei singoli cicli questa scrittura come cicli disgiunti è unica.
Riassumiamo quanto detto nel seguente teorema:

\begin{thm}
Ogni elemento $\sigma \in S_n$ si scrive in modo unico come prodotto di cicli disgiunti a meno dell'ordine dei fattori e a meno di 
cambiare il modo in cui i singoli cicli sono presentati.
\end{thm}

Spesso nella scrittura in cicli disgiunti si tralasciano i cicli di lunghezza uno. Ad esempio $(3,5,7)(4,1)\in S_{12}$ ha perfettamente senso:
i numeri $1,3,4,5,7$ vengono ``spostati'' da $\sigma$ nel modo descritto e tutti gli altri vengono lasciati fissi.

I cicli, più che essere delle liste, vanno pensati come elementi di $S_n$, ovvero come funzioni bigettive da $\{1,\dots,n\}$ in sé.
In quanto tali possono essere moltiplicati, nel senso di composizione delle funzioni. Ad esempio
\[(1,2,3)\cdot(3,5)\]
chiaramente non è la scrittura come prodotto di cicli disgiunti di un elemento di $S_n$, in quanto appunto i due cicli scritti
non sono disgiunti. Ma il loro prodotto ha perfettamente senso, e usando la stessa convenzione
che si usa di solito per la composizione di funzioni vanno fatti ``agire'' da destra a sinistra.
Ad esempio il prodotto scritto manda il numero $5$ nel numero $1$ (infatti il ciclo a destra manda $5$ in $3$, il quale viene mandato in $1$ dal ciclo
a sinistra). Se lo volessimo scrivere come prodotto di cicli disgiunti otterremmo:
\[(1,2,3,5)\]

\begin{thm}
Ogni elemento $\sigma \in S_n$ si può scrivere come prodotto di \emph{trasposizioni}, ovvero cicli di lunghezza $2$, non necessariamente disgiunti.
\end{thm}
\begin{proof}
Considerato il teorema sulla decomposizione in cicli, basta mostrare la tesi nel caso in cui $\sigma$ è un ciclo.
Supponiamo $\sigma = (a_1, a_2, \dots, a_k)$. Allora è facile verificare che
\[\sigma = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Qualcuno potrebbe essere turbato dal caso in cui il ciclo ha lunghezza $1$, ossia $\sigma$ è l'identità.
In tal caso possiamo dire che $\sigma$ è il prodotto di un insieme vuoto di trasposizioni.
Chi fosse ancora turbato potrebbe scrivere, almeno nel caso $n\ge 2$, $\sigma = (1,2)(1,2)$.
\end{proof}

Osserviamo che il teorema precedente assicura solo l'esistenza di una scrittura come prodotto di trasposizioni
ma non l'unicità. In effetti questa non sussiste, infatti se in fondo ad un prodotto di trasposizioni aggiungo $(1,2)(1,2)$
allora il risultato non cambia. Per avere un esempio leggermente più sofisticato:
\[(2,1)(2,3) = (1,3)(1,2)\]
Tuttavia quello che non cambia è la parità del numero di trasposizioni, come precisato dal seguente teorema.
\begin{thm}
Siano $\tau_1, \tau_2, \dots, \tau_t, \sigma_1, \sigma_2, \dots, \sigma_s$ trasposizioni in $S_n$. Supponiamo che
\[\tau_1\tau_2\dots\tau_t = \sigma_1\sigma_2\dots\sigma_s\]
Allora $s\equiv t \mod 2$.
\end{thm}
\begin{proof}[Cenno di dimostrazione]
Definiamo la seguente funzione $f:S_n\to\N$:
\[f(\rho) = card\left(\left\{\quad(a,b)\in\{1,\dots,n\}^2 \quad | \quad a < b, \quad\rho(a) > \rho(b) \quad\right\}\right)\]
Non è difficile verificare che se $\tau\in S_n$ è una trasposizione allora $f(\rho)$ e $f(\tau\rho)$ hanno parità diversa.
Il risultato segue immediatamente visto che $f(\tau_1\tau_2\dots\tau_t) = f(\sigma_1\sigma_2\dots\sigma_s)$.
\end{proof}

\begin{defn}[Segno di una permutazione]
Il teorema appena visto permette di definire il \emph{segno} di ogni elemento $\sigma\in S_n$, che si pone uguale a $1$
se $\sigma$ si scrive come prodotto di un numero pari di trasposizioni, si pone uguale a $-1$ altrimenti.
\end{defn}

\begin{prop}
Il segno di un ciclo di lunghezza $k$ è esattamente $(-1)^{k-1}$
\end{prop}
\begin{proof}
Abbiamo già visto un modo in cui un ciclo di lunghezza $k$ si può scrivere come prodotto di trasposizioni:
\[(a_1, a_2, \dots, a_k) = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Dunque la tesi segue immediatamente.
\end{proof}

\begin{defn}
L'insieme degli elementi di $S_n$ aventi segno $+1$ è un sottogruppo di $S_n$,
chiamato \emph{gruppo alterno} e indicato con $A_n$.
\end{defn}


\subsection{Proprietà dei gruppi diedrali}

\begin{defn}[Gruppo diedrale]
Consideriamo in $\R^2$ un poligono regolare di $n$ lati con centro nell'origine.
L'insieme $D_n$ delle isometrie di $\R^2$ che mandano il poligono in sé è un gruppo con l'operazione di composizione.
Si verifica che questo gruppo ha $2n$ elementi, di cui $n$ rotazioni (ovvero elementi di $O_2(\R)$ con determinante $1$)
e $n$ riflessioni (ovvero elementi di $O_2(\R)$ con determinante $-1$).
Inoltre, detta $\rho$ una rotazione di $2\pi/n$ (che ha ordine $n$, e per inverso ha $\rho^{n-1}$) e $\sigma$ una qualunque riflessione (che ha ordine $2$), esse generano
il gruppo $D_n$, che si può presentare nel seguente modo: $$D_n=\langle\rho,\sigma|\rho^n=\sigma^2=id,\ \sigma\rho\sigma=\rho^{-1}\rangle$$
\end{defn}

\begin{rem}
 Le $n$ potenze distinte di $\rho$ sono tutte e sole le rotazioni di $D_n$, mentre gli elementi della forma $\sigma\rho^{i},\ i=0,1,..,n-1$ 
 sono tutte e sole le riflessioni. 
\end{rem}

\begin{rem}
 Si dimostra facilmente che la relazione $\sigma\rho\sigma=\rho^{-1}$ è verificata da qualsiasi rotazione $\rho$
 e qualsiasi riflessione $\sigma$.
\end{rem}











\newpage
\section{Algebra lineare}
In questa sezione diamo alcune definizioni e teoremi di algebra lineare che sono stati utilizzati nel corso o che sono utili per avere una visione d'insieme di certi argomenti. Non saranno presenti le dimostrazioni che possono essere trovate su molti libri di algebra lineare.
\begin{thm}[Diagonalizzazione simultanea]
\label{thm:diag_sim}
	Date due matrici $M, N\in \mathcal{M}(n,n,\K)$, diremo che sono \textit{simultaneamente diagonalizzabili} se esiste una base comune di autovettori per entrambe.\\
	Date $M, N\in \mathcal{M}(n,n,\K)$, se esse commutano e sono entrambe diagonalizzabili allora sono simultaneamente diagonalizzabili.
\end{thm}
\begin{cor}
	Date $M_1,\ldots,M_k \in \mathcal{M}(n,n,\K)$, se $M_iM_j=M_jM_i\ \forall\ i, j$ e ogni $M_i$ è diagonalizzabile, allora esiste una base comune di autovettori per tutte quante.
\end{cor}


\begin{defn}[Ideale di un endomorfismo]
	Se $p(x)=a_n x^n+\ldots+a_0$, allora scriviamo $p(f)$ per intendere $a_nf^n+\ldots+a_0f^0$ dove $f^0=\Id$ e $f^k=\underbrace{f\circ\ldots\circ f}_{k \text{ volte}}$.\\
	Sia $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$. Definiamo \textit{ideale di $f$} l'insieme
	\[
		I(f)=\left\{ p(x)\in \K[x]\ |\ p(f)=0 \right\}
	\]
	

\end{defn}


\begin{thm}[Teorema di decomposizione primaria]
\label{thm:dec_primaria}
	Siano $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$ e $q(x)\in I(f)$. Sia $q=q_1\cdot\ldots\cdot q_k$ tale che $MCD(q_i,q_j)=1\ \forall\ i\neq j$, allora $V=\Ker(q_1(f))\oplus\dots\oplus \Ker(q_k(f))$ e gli addendi sono $f$-invarianti.\\
	In particolare se $f$ è triangolabile e $\lambda_1,\ldots,\lambda_k$ sono gli autovalori di $f$ con molteplicità algebrica rispettivamente $\alpha_1,\ldots,\alpha_k$, allora $V=\Ker\left((f-\lambda_1 \Id)^{\alpha_1}\right)\oplus\dots\oplus \Ker\left((f-\lambda_k \Id)^{\alpha_k}\right)$.
\end{thm}

\begin{thm}[Forma canonica di Jordan]
	Sia $M\in \mathcal{M}(n,n,\K)$ una matrice triangolabile, siano $\lambda_1,\ldots,\lambda_k$ i suoi autovalori, allora $M$ è simile alla sua \textit{forma canonica di Jordan} che è nella forma
	\begin{align*}
		&\begin{pmatrix}
			J_1 & & \\
			& \ddots & \\
			& & J_t
		\end{pmatrix}		
		&\text{ dove }J_i=\begin{pmatrix}
		                  	\lambda & 1 & & & \\
							& \lambda & 1 & & \\
		                  	& & \ddots & \ddots & \\
		                  	& & & \ddots & 1 & \\
		                  	& & & & \lambda
		                  \end{pmatrix} \text{ per qualche }\lambda \in \left\{ \lambda_1,\ldots,\lambda_k \right\}
	\end{align*}
	La dimensione e il numero di blocchi di ciascun tipo sono univocamente determinati dalla matrice $M$, ne segue che la forma canonica di Jordan è unica a meno di permutazione dei blocchi e dunque, scelta una convenzione sull'ordine dei blocchi, essa è un sistema completo di invarianti per similitudine: due matrici sono simili se e solo se hanno la stessa forma di Jordan.

\end{thm}
\begin{defn}[Forma hermitiana]
Siano $V,W$ due $\C$-spazi vettoriali, una funzione $h:V\times V\to W$ si dice \textit{forma hermitiana} se $\forall v,w,z\in V,\ \forall \alpha \in \C$ vale che
\begin{gather*}
	h(v,w) = \overline{h(w,v)}\\
	h(\alpha v, w) = \alpha h(v,w)\\
	h(v+z,w)=h(v,w)+h(z,w)
\end{gather*}
\end{defn}

\begin{defn}
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{definita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V, v\neq 0$ si ha che $\phi(v,v)>0$ (rispettivamente $\phi(v,v)<0$), ossarvare che $\phi(v,v)\in \R\ \forall\ v\in V$, dunque ha senso chiedere che sia maggiore o minore di $0$.\newline
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{semidefinita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V$ si ha che $\phi(v,v)\geq 0$ (rispettivamente $\phi(v,v)\leq 0$)
\end{defn}

\begin{thm}
	Ogni forma hermitiana definita positiva su uno spazio vettoriale $V$ di dimensione finita ammette una \textit{base ortonormale}, ovvero esiste una base $\{v_1,\ldots,v_n\}$ di $V$ tale che $\phi(v_i,v_j)=\delta_{ij}$. 
\end{thm}




\newpage
\section{Algebra multilineare}
\subsection{Alcune generalizzazioni di algebra lineare}

\begin{defn}[Base di uno spazio vettoriale]
Sia $V$ un $\K-$spazio vettoriale e $I$ un insieme; una base di $V$ è una funzione $e: I \to V$ tale che 
per ogni $v \in V$ esiste un'unica funzione $a: I \to \K$ a supporto finito per cui vale $v=\sum_{i\in I}a_i e_i$.
Questa definizione è compatibile con la definizione di base come insieme di vettori generatori linearmente indipendenti.

Spesso useremo una notazione del tipo $\{e_i\}_{i\in I}$ per indicare una base di uno spazio vettoriale $V$. Ciò sottintende
una funzione $e:I\to V$ che manda $i\to e_i$, in accordo con la definizione che abbiamo appena dato.
\end{defn}

Alcuni dei risultati a cui arriveremo sono validi anche per $I$ infiniti, ma per semplicità consideriamo solo
spazi vettoriali finitamente generati (per cui esiste una base $e: I \to V$ con $I$ insieme finito).
Questo primo lemma dovrebbe essere noto a chiunque abbia un minimo di familiarità con l'algebra lineare:
\begin{lemma}
Siano $V,W$ dei $\K-$spazi vettoriali, sia $e:I\to V$ una base di $V$ e $f: I \to W$ una funzione. Allora $\exists!\  \phi: V \to W$ lineare tale che
\[\phi(e_i) = f_i \]
Inoltre $\phi$ è un isomorfismo $\Leftrightarrow$ $f$ è una base.
\end{lemma}

\begin{lemma}
Dato $I$ insieme, esiste uno spazio vettoriale $V$ con base una certa $e:I\to V$.
\end{lemma}
\begin{proof}
Definisco il seguente insieme, che è in modo naturale un $\C-$spazio vettoriale:
\[ \C^I = \{ v:I\to\C \quad|\quad v \text{ ha supporto finito}\}\]
Ora è facile osservare che $e:I\to\C^I$ definita da $e_i(j) = \delta_{i,j}$ è una base.
\end{proof}


\subsection{Prodotto tensoriale}

\begin{defn}[Prodotto tensoriale]
   Siano $V, W$ due $\mathbb{C}$-spazi vettoriali. Si dice prodotto tensore di $V$ e $W$, 
   e si indica come $V\otimes W$, uno spazio vettoriale con una funzione bilineare 
   $\otimes: V \times W \to V\otimes W$ tale che per ogni data funzione bilineare $h: V\times W \to  Z$,
   esiste unica $\phi: V\otimes W \to Z$ lineare per cui $\phi(v \otimes w)=h(v,w)$. Ovvero questa $\phi$ fa commutare il diagramma:
   \[\tridiag{V\times W}{ \otimes }{V \otimes W}{\phi}{Z}{h}\]
   Questa proprietà viene detta proprietà universale del prodotto tensoriale e la funzione $\otimes: V \times W \to V\otimes W$
   viene detta funzione universale.
\label{defn:prodotto tensoriale}
\end{defn}

\begin{exmp} $V=W=V\otimes W = \C$ e come $\otimes$ prendo il prodotto ovvero $v\otimes w=vw$.
\end{exmp}
\begin{exmp} $W$ un qualsiasi $\K-$spazio vettoriale, $V=\K$. Allora posso prendere $\K\otimes W = W$ e come $\otimes$ prendo il prodotto per scalari $\alpha\otimes w=\alpha w$.

Infatti se $h:\K\times W\to Z$ è una funzione bilineare, allora mi basta definire $\phi:W\to Z$ ponendo $\phi(w) = h(1, w)$. Si vede facilmente
che così il diagramma commuta e non esiste nessun'altra $\phi$ che fa commutare il diagramma.
\end{exmp}

\begin{prop}
Se ho due prodotti tensoriali $V \otimes W$ e $V \overline{\otimes} W$, allora esiste un unico isomorfismo 
$\phi: V \otimes W \to V \overline{\otimes} W$ tale che, fissati comunque $v\in V, w\in W$, valga
\[ \phi (v\otimes w) = v \overline{\otimes} w\]
\end{prop}
\begin{proof}
Considero i seguenti due diagrammi:
\[
\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ V \overline{\otimes} W }{\overline{\otimes}} \qquad
\tridiag{V\times W}{ \overline{\otimes} }{V \overline{\otimes} W}{}{V \otimes W}{\otimes}
\]
La proprietà universale per il prodotto tensore $\otimes$ dice che $\exists !\phi:V\otimes W\rightarrow V\overline{\otimes}W$ lineare che fa commutare il primo diagramma: analogamente $\exists !\psi:V\overline{\otimes}W\rightarrow V\otimes W$ lineare che fa commutare il secondo diagramma. Ora consideriamo i seguenti due diagrammi:
\[
\begin{diagram}
            &                            & V\otimes W               \\
            & \ruTo^{\otimes}            & \dTo>{\phi}              \\
 V\times W  & \rTo^{\overline{\otimes}}  &  V\overline{\otimes} W   \\
            & \rdTo_{\otimes}            & \dTo>{\psi}              \\
            &                            & V\otimes W               \\
\end{diagram}
\qquad
\begin{diagram}
            &                            & V\overline{\otimes} W    \\
            & \ruTo^{\overline{\otimes}} & \dTo>{\psi}              \\
 V\times W  & \rTo^{\otimes}             &  V\otimes W              \\
            & \rdTo_{\overline{\otimes}} & \dTo>{\phi}              \\
            &                            & V\overline{\otimes} W    \\
\end{diagram}
\]
Ma noi conosciamo già un'applicazione lineare che fa commutare i due diagrammi ``più grandi'': l'identità. Quindi per unicità possiamo concludere che $\phi\psi=\psi\phi=id$, ovvero $\phi$ e $\psi$ sono una l'inversa dell'altra e in particolare $\phi$ è un isomorfismo.
\end{proof}

Si può dimostrare inoltre che dati due spazi vettoriali $V$ e $W$ esiste sempre 
un loro prodotto tensoriale, dunque abbiamo il seguente risultato:
\begin{thm}
$V\otimes W$ esiste ed è unico a meno di isomorfismo.
\end{thm}


\begin{note}
\`E importante notare che non tutti gli elementi $z \in V \otimes W$ si scrivono come $z = v \otimes w$. In particolare, per fare un esempio concreto che mostra che questa cosa non funziona, prendiamo $W = V^*$. Vedremo fra poco che $V\otimes V^*$ è canonicamente isomorfo allo spazio delle applicazioni bilineari da $V$ in $\C$, che sappiamo scriverlo come matrici $n\times n$. Tuttavia se un elemento si scrive in termini di matrici come $z = v\otimes w$, allora la matrice associata a $z$ in una base avrà rango al massimo 1, ben lontano da coprire tutto lo spazio.
\end{note}


\begin{prop}
L'insieme degli elementi di $V\otimes W$ della forma $v\otimes w$ con $v\in V, w\in W$ genera tutto lo spazio $V\otimes W$.
\end{prop}

\begin{proof} Consideriamo il sottospazio generato dagli elementi della forma $v\otimes w$:
\[\langle\{v\otimes w:v\in V,w\in W\}\rangle=\langle A\rangle\]
Vogliamo quindi dimostrare che $\langle A\rangle=V\otimes W$. Consideriamo il diagramma:
\[\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ \langle A\rangle }{h}\]
dove $h(v,w)=v\otimes w$ è ovviamente un'applicazione bilineare. Quindi $\exists !\psi:V\otimes W\rightarrow \langle A\rangle$ che fa commutare il diagramma. Detta $i:\langle A\rangle\rightarrow V\otimes W$ l'inclusione consideriamo il seguente diagramma:
\[
\begin{diagram}
            &                       & V\otimes W               \\
            & \ruTo^{\otimes}       & \dTo>{\psi}              \\
 V\times W  & \rTo^{h}              & \langle A \rangle   \\
            & \rdTo_{i\circ h}      & \dTo>{i}              \\
            &                       & V\times W               \\
\end{diagram}
\]
tuttavia il diagramma grande commuta anche con $id_{V\otimes W}$ e quindi $i\circ\psi=id_{V\otimes W}\Rightarrow i$ è surgettiva $\Rightarrow \langle A\rangle=V\otimes W$.
\end{proof}


\begin{defn}[Prodotto tensoriale di mappe lineari]
Date $f:V \to V'$ e $g:W \to W'$ funzioni lineari, si definisce prodotto tensoriale tra $f$ e $g$ l'unica funzione lineare $f \otimes g : V \otimes W \to V' \otimes W'$ tale che $(f \otimes g)(v \otimes w)=f(v) \otimes g(w)$ $\forall v\in V, w\in W$.

Una funzione con tale proprietà esiste ed è unica poiché l'applicazione $V\times W \to V'\otimes W'$ che manda $(v,w)$ in $f(v)\otimes g(w)$ è bilineare.
\end{defn}

\begin{rem}
$id_V \otimes id_W = id_{V\otimes W}$
\end{rem}


\begin{prop}
Siano $V$ e $W$ $\K-$spazi vettoriali e sia $\{e_i\}_{i\in I}$ una base di $V$. Allora ogni elemento di $V\otimes W$ si scrive in modo unico come:
\[ \sum_{i\in I} e_i \otimes w_i\]
con $w_i\in W$.
\end{prop}
\begin{proof}
Sia $x$ un elemento di $V\otimes W$.
Visto che gli elementi della forma $v\otimes w$ generano l'intero spazio $V\otimes W$, possiamo scrivere
\[ x = \sum_{j=1}^N v_j\otimes w_j \]
scegliendo opportunamente $N\in \N$, $v_j\in V$, $w_j\in W$ per $j=1\dots N$. Visto che $\{e_i\}_{i\in I}$ è base di $V$:
\[ x = \sum_{j=1}^N v_j\otimes w_j  = \sum_{j=1}^N \left(\sum_{i\in I} a_{i,j}e_i\right)\otimes w_j = \sum_{i,j} a_{i,j}(e_i\otimes w_j) = \sum_{i\in I} e_i\otimes\tilde w_i\]
dove si è posto $\tilde w_i = \sum_j a_{i,j}e_j$.
Quindi siamo riusciti ad ottenere una scrittura del tipo che volevamo. Resta da mostrare che questa scrittura è unica,
ovvero che gli elementi $\tilde w_i$ sono univocamente determinati.

Consideriamo la base $\{e_i^*\}_{i\in I}$ di $V^*$ duale rispetto a $\{e_i\}_{i\in I}$. Fissato un $k\in I$ possiamo considerare l'applicazione
lineare $e_k^*\otimes id_W: V\otimes W \to \K\otimes W$. Valutandola in $x$ otteniamo:
\[ (e_k^*\otimes id_W) \left(\sum_{i\in I} e_i\otimes\tilde w_i \right) = \sum_{i\in I} e_k^*(e_i)\otimes\tilde w_k = 1\otimes \tilde w_k\]
Ma $\K\otimes W$ è isomorfo in modo naturale a $W$, e questo isomorfismo porta l'elemento $1\otimes \tilde w_k$ in $\tilde w_k$.
Quest'ultimo, pertanto, è univocamente determinato.
\end{proof}

\begin{prop}
Se $\{e_i\}_{i\in I}$ è una base di $V$ e $\{f_j\}_{j\in J}$ è una base di $W$ allora $\{e_i \otimes f_j\}_{(i,j)\in I\times J}$ è una base di $V \otimes W$.
\end{prop}
\begin{proof}
Dato $x\in V\otimes W$ sappiamo che si può scrivere in modo unico $x = \sum_{i\in I} e_i\otimes w_i$.
Ora, per ogni $i\in I$, il vettore $w_i$ si scrive in modo unico come $w_i = \sum_{j\in J} a_{i,j}f_j$.
Da questo segue abbastanza facilmente che $x$ si scrive in modo unico come $\sum_{i,j} a_{i,j}(e_i\otimes f_j)$.
\end{proof}

\begin{cor}
$\dim(V \otimes W) = \dim V \cdot \dim W$
\end{cor}


\begin{prop}
Siano $V$ e $W$ spazi vettoriali. Allora $V^*\otimes W$ è isomorfo allo spazio vettoriale $\Hom(V,W)$ delle applicazioni lineari da $V$ a $W$.
\end{prop}
\begin{proof}
Definiamo $\theta: V^*\times W \to \Hom(V,W)$ come la funzione che manda la coppia $(f, w)\in V^*\times W$ nella funzione  $h_{f,w}$ definita da:
$h_{f,w}(v) = f(v)w$ per ogni $v\in V$. Non è difficile osservare che $\theta$ è bilineare, quindi induce una funzione lineare
$\phi : V^*\otimes W \to \Hom(V,W)$ tale che, dati comunque $f\in V^*, w\in W, v\in V$, soddisfa $\phi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\phi$ è un isomorfismo di spazi vettoriali.

Fissiamo $\{v_i\}_{i\in I}$ una base di $V$, $\{v_i^*\}_{i\in I}$ la base duale, $\{w_j\}_{j\in J}$ una base di $W$.
Ora abbiamo 
\[\phi(v_i^*\otimes w_j)(v_k) = \delta_{i,k} w_j\]
Ciò vuol dire che, se scriviamo la matrice dell'applicazione $\phi(v_i^*\otimes w_j):V\to W$ secondo le basi date, questa presenta un $1$ all'incrocio
tra l'$i-$esima colonna e la $j-$esima riga, mentre è nulla altrove.
Dunque $\phi$ manda la base $\{v_i^*\otimes w_j\}_{(i,j)\in I\times J}$ dello spazio $V^*\otimes W$ in una base dello spazio $\Hom(V,W)$,
quindi è un isomorfismo.
\end{proof}

\begin{rem}
In particolare, ponendo $W=V$, otteniamo che $\End(V)$ è isomorfo a $V^*\otimes V$.

Esiste un'applicazione in un certo senso ``naturale'' $t:V^*\otimes V \to \K$, dove $\K$ è il campo degli scalari, definita da
$t(f\otimes v) = f(v)$.
Se indichiamo con $\phi$ l'isomorfismo $V^*\otimes V\to \End(V)$ che abbiamo definito nel corso della precedente dimostrazione,
otteniamo una funzione lineare $\tr = t\circ \phi^{-1}: \End(V)\to \K$. Non è difficile vedere che questa $\tr$ che abbiamo appena definito 
coincide con la classica funzione ``traccia''. Il modo in cui l'abbiamo definita noi rende evidente il fatto che la traccia non dipende
dalla base scelta per scrivere la matrice di un endomorfismo.
\end{rem}


\begin{thm}
Se $f:V\to V$ e $g:W\to W$ sono endomorfismi di spazi vettoriali, allora vale la formula
\[\tr(f\otimes g) = \tr(f) \tr(g)  \]
\label{thm: tracciaprodotto}
\end{thm}
\begin{proof}
Sia $\{v_i\}_{i\in I}$ una base di $V$, $\{w_j\}_{j\in J}$ una base di $W$, e come al solito indichiamo gli elementi delle basi duali aggiungendo un asterisco.
\begin{align*}
 \tr(f\otimes g) &= \sum_{(i,j)\in I\times J} (v_i^*\otimes w_j^*) (f\otimes g)(v_i\otimes w_j) =\\
                 &= \sum_{(i,j)\in I\times J} v_i^*(f(v_i))\cdot w_j^*(g(w_j)) =\\
                 &= \left(\sum_{i\in I} v_i^*(f(v_i))\right) \cdot \left(\sum_{j\in J} w_j^*(g(w_j))\right) =\\
                 &= \tr(f) \cdot \tr(g)
\end{align*}
\end{proof}
\begin{proof}[Dimostrazione alternativa per $\C-$spazi vettoriali]
Iniziamo a considerare il caso in cui sia $f$ che $g$ siano diagonalizzabili: prendendo due basi $a:I\rightarrow V$ , $b:J\rightarrow W$ di autovettori rispettivamente per $f$ e per $g$, si verifica facilmente la verità della proposizione nella base indotta su $V\otimes W$ (ovvero in quella formata dagli $a_i\otimes b_j$).

Ora, essendo la traccia una funzione continua e le matrice diagonalizzabili dense nello spazio delle matrici, la proprietà affermata dal lemma si estende al caso generale per continuità.
\end{proof}

\begin{thm}
Sia $V$ spazio vettoriale. Allora $V^*\otimes V^*$ è isomorfo allo spazio vettoriale delle forme bilineari $V\times V\to \C$.
\end{thm}


\subsection{Prodotto esterno e prodotto simmetrico}

\begin{defn}[Applicazione $n-$lineare simmetrica/alternante]
 Una applicazione $\phi: V^n \to Z$ si dice $n-$lineare se è lineare in ogni componente dopo aver fissato le altre $n-1$.

 Inoltre $\phi$ si dice simmetrica se $\phi(v_{s(1)},\ldots,v_{s(n)})=\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$, mentre si dice 
 alternante se $\phi(v_{s(1)},\ldots,v_{s(n)})=\mathrm{sgn}(s)\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$.
\end{defn}

\begin{prop}
  Un'applicazione $n-$lineare $h: V^n \to W$ è alternante se e solo se, presi comunque $v_1,\dots,v_n\in V$
  non tutti distinti tra loro, si ha che $h(v_1,\dots,v_n)=0$.
\end{prop}

\begin{prop}
  Sia $h: V^n \to W$ un'applicazione $n-$lineare alternante. Se i vettori $v_1,\dots,v_n$ sono linearmente dipendenti allora
  $h(v_1,\dots,v_n)=0$.
\end{prop}


\begin{defn}[Prodotto esterno]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto esterno è uno spazio vettoriale indicato con $\bigwedge^n V$
dotato di una funzione $n-$lineare alternante $\wedge: V^n \to \bigwedge^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1\wedge v_2\wedge\ldots\wedge v_n \in \bigwedge^n V$, tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare alternante, 
esiste un'unica $\phi: \bigwedge^n V \to Z $ lineare per cui vale $\phi(v_1\wedge v_2\wedge \ldots \wedge v_n)=h(v_1,\ldots,v_n)$.
\label{defn:prodotto esterno}
\end{defn}

\begin{defn}[Prodotto simmetrico]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto simmetrico è uno spazio vettoriale indicato con $S^n V$
dotato di una funzione $n-$lineare simmetrica $V^n \to S^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1 v_2\ldots v_n \in S^n V$, tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare simmetrica, 
esiste un'unica $\phi: S^n V \to Z $ lineare per cui vale $\phi(v_1 v_2 \ldots v_n)=h(v_1,\ldots,v_n)$.
\label{defn:prodotto simmetrico}
\end{defn}


\begin{prop}[Proprietà dei prodotti esterni e simmetrici]
Sia $V$ uno spazio vettoriale e $n$ un numero naturale.
\begin{itemize}
\item $\bigwedge^nV$ e $S^nV$ esistono.
\item $\bigwedge^nV$ e $S^nV$ sono unici a meno di un unico isomorfismo.
\item $\bigwedge^nV$ è generato dai vettori del tipo $v_1\wedge \dots \wedge v_n$, e analogamente 
      $S^nV$ è generato dai vettori del tipo $v_1\dots v_n$.
\end{itemize}
\end{prop}


\begin{defn}[Potenza simmetrica e potenza esterna di un'applicazione lineare]
Sia $f: V\to V$ un endomorfismo di uno spazio vettoriale. Definiamo 
$\bigwedge^k f : \bigwedge^k V \to \bigwedge^k V$ la funzione che manda $v_1 \wedge \ldots \wedge v_n$ in $f(v_1) \wedge \ldots \wedge f(v_n)$

In modo analogo si definisce la potenza simmetrica.
\end{defn}


\begin{rem} Siano $V$, $W$, $Z$ spazi vettoriali.
\begin{itemize}
   \item $\bigwedge^k \Id_V = \Id_{\bigwedge^kV}$
   \item Date due applicazioni lineari $f:V\to W$ e $g:W\to Z$ allora $\bigwedge^k(g\circ f) = \bigwedge^kg\circ\bigwedge^kf$
\end{itemize}
\end{rem}

\begin{thm}[Dimensione del prodotto esterno]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme
\[E=\{e_{i_1} \wedge e_{i_2}\wedge \ldots \wedge e_{i_k} \quad|\quad 1 \leq i_1 < i_2 <\ldots< i_k \leq n\}\]
è una base di $\bigwedge^k V$ di cardinalità $|E|= \binom {n}{k}$.
\label{thm:prodotto esterno}
\end{thm}


\begin{thm}[Dimensione del prodotto simmetrico]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme 
\[E=\{e_{i_1} \wedge e_{i_2}\wedge\ldots \wedge e_{i_k} \quad|\quad 1 \leq i_1 \leq i_2 \leq\ldots\leq i_k \leq n\}\]
è una base di $S^k V$ di cardinalità $|E|= \binom {n+k-1}{k}$.
\label{thm:prodotto simmetrico}
\end{thm}

Sia $V$ uno spazio vettoriale di dimensione $n$. Allora sappiamo che:
\begin{itemize}
\item $\dim V\otimes V = n^2$
\item $\dim S^2V = \frac{n(n+1)}{2}$
\item $\dim \bigwedge^2V = \frac{n(n-1)}{2}$
\end{itemize}
Possiamo definire in modo abbastanza naturale un'applicazione lineare $V\otimes V\to \bigwedge^2V\oplus S^2V$,
mandando il vettore $v\otimes w$ nella coppia $(v\wedge w, vw)$. In effetti questa applicazione è un isomorfismo.
\begin{proof}
Per uguaglianza delle dimensioni degli spazi di partenza e di arrivo, mi basta dimostrare che è surgettiva.
Per ogni coppia $v\in V$, $w\in W$ abbiamo che:
\[\frac{v\otimes w + w\otimes v}{2} \to (0, vw)\]
\[\frac{v\otimes w - w\otimes v}{2} \to (v\wedge w, 0)\]
Quindi $\bigwedge^2V$ e $S^2V$ sono contenuti nell'immagine. Ma da questo segue subito la tesi.
\end{proof}


\begin{thm}
Sia $V$ spazio vettoriale. Allora $S^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari simmetriche $V\times V\to \C$.
\end{thm}
\begin{thm}
Sia $V$ spazio vettoriale. Allora $\bigwedge^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari antisimmetriche $V\times V\to \C$.
\end{thm}


\begin{prop}
Sia $f: V \to V$ un endomorfismo di uno spazio vettoriale. Allora vale
\[ 
\begin{cases}
\tr(\bigwedge^2 f ) = \dfrac{(\tr(f))^2 - \tr(f^2)}{2} \\
\tr(S^2 f ) = \dfrac{(\tr(f))^2 + \tr(f^2)}{2} \\
\end{cases}
\]
\label{thm:tracciasymalt}
\end{prop}


\begin{proof} Sia $\{e_1,\ldots,e_n\}$ una base di $V$, allora $\exists a_{ij}: f(e_j)=\sum_i a_{ij}e_i$. Se $i<j$
\begin{gather*}
\bigwedge^2f(e_i\wedge e_j)=f(e_i)\wedge f(e_j)=\left(\sum_k a_{ki}e_k\right)\wedge\left(\sum_l a_{lj}e_l\right)=\\
=\sum_{k,l}a_{ki}a_{lj}(e_k\wedge e_l)=\sum_{k<l}(a_{ki}a_{lj}-a_{li}a_{kj})(e_k\wedge e_l)\\
\Rightarrow \tr(\bigwedge^2f)=\sum_{i<j}(a_{ii}a_{jj}-a_{ij}a_{ji})=\frac{1}{2}\sum_{i,j}(a_{ii}a_{jj}-a_{ij}a_{ji})= \dfrac{(\tr(f))^2 - \tr(f^2)}{2}
\end{gather*}
Per $S^2f$ si dimostra in maniera analoga. 

\end{proof}













\newpage
\section{Prime proprietà delle rappresentazioni}

\begin{defn}[Rappresentazione]
	Sia $G$ un gruppo e $V$ uno spazio vettoriale. Una funzione $\rho: G \to GL(V)$ che manda ciascun elemento del gruppo in un'applicazione lineare invertibile di $V$
	si dice rappresentazione di $G$ se è un omomorfismo di gruppi, ovvero in parole semplici deve rispettare la regola di composizione. In formule, se $s, t \in G$ deve valere
	\[ \rho(st) v = \rho(s)\rho(t) v \qquad \forall v \in V, \quad \forall s,t \in G\]
\end{defn}

La dimensione di $V$ viene detta grado della rappresentazione.
Per il momento consideriamo solo rappresentazioni su spazi vettoriali complessi, ovvero supponiamo che $V$ sia $\C-$spazio vettoriale.

Con un piccolo abuso di linguaggio, spesso indicheremo con il nome di ``rappresentazione'' direttamente $V$, se è chiaro quale sia 
la rappresentazione $\rho$ che stiamo considerando. Spesso indicheremo gli spazi vettoriali
su cui è definita una rappresentazione $\rho$ con nomi del tipo $V_\rho$, dove il pedice ci ricorda la rappresentazione che stiamo considerando.

\begin{rem}
Detto in altri termini, una rappresentazione di un gruppo $G$ è un'azione lineare di $G$ su uno spazio vettoriale $V$.
Se non c'è ambiguità (cioè se è chiaro quale rappresentazione stiamo considerando)
possiamo scrivere semplicemente $g\cdot v$ in luogo di $\rho(g)v$ per allegerire la notazione.
\end{rem}

\begin{rem}
$\rho(G)$ è evidentemente un sottogruppo di $GL(V)$, quindi esistono sempre inversi, potenze e valgono tutte le cose che sono vere per i gruppi.
\end{rem}


\begin{exmp}
   La rappresentazione banale, di grado qualsiasi, indicata con $\rho_1$ che manda qualsiasi elemento di $g$ nell'identità di $GL(V_{\rho_1})$, ovvero
	\[ \rho(s ) = \Id_{V_{\rho_1}} \qquad \forall s \in G\]
\end{exmp}
\begin{exmp}
   Dato $S_n$, il segno di un elemento $s\in S_n$ è una rappresentazione di grado 1. Infatti si ha $sgn(st) = sgn(s) sgn(t)$.
\end{exmp}
\begin{exmp}
   L'azione naturale di $S_n$ sui vettori della base. Prendiamo quindi $G = S_n$ e uno spazio vettoriale di dimensione $n$, che quindi è isomorfo a $\C^n$. Prendiamo la base canonica di $\C^n$ e la indichiamo con $\{e_i\}_{i=1}^n$. Descriviamo la rappresentazione $\rho: S_n \to GL(\C^n)$ dicendo cosa fa agli elementi della base: per linearità si estenderà a tutto lo spazio.
	\[ \rho(s) e_i = e_{s(i)}\]
	Notare che in questo caso $\deg(\rho) = n$. Notiamo inoltre che se rappresentiamo nella base canonica le matrici associate a $\rho(s)$ queste matrici sono unitarie. Inoltre, ogni colonna (e anche ogni riga) contiene esattamente un 1 e tutti gli altri sono 0.
	
	Prendiamo come esempio $S_3$ e vediamo cosa succede. Notiamo innanzitutto che $ |S_3| = 3! = 6$
	FINISCI DI SCRIVERE
\end{exmp}







\begin{prop}
Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\forall g \in G$ la matrice $\rho(g)$ ammette una base di autovettori in $V_\rho$, ovvero è diagonalizzabile. Inoltre, tutti gli autovalori di $\rho(g)$ sono radici $n-$esime dell'unità.

\begin{note} Per ogni matrice in generale la base è diversa, quindi le varie matrici in generale \emph{non} sono simultaneamente diagonalizzabili.
Però se $G$ è abeliano tutte le matrici $\rho(s)$ sono simultaneamente diagonalizzabili. 
\end{note}
\label{prop:diagonalizzabilita rappresentazioni}
\end{prop}

\begin{proof} Se $G$ è un gruppo finito, allora esiste un intero positivo $k$ tale che $g^k = e$. Dato che $\rho:G\to GL(V_\rho)$ mantiene queste proprietà in quanto omomorfismo, dovrà essere $\rho(g)^k = id$.

Visto che il polinomio minimo di $\rho(g)$ non ha radici multiple, con il teorema di decomposizione primaria \eqref{thm:dec_primaria} si mostra facilmente che $\rho(g)$ è diagonalizzabile. Inoltre da questa formula è anche evidente che tutti gli autovalori di $\rho(g)$ hanno modulo $1$ e in particolare saranno radici $k-$esime dell'unità.

Ricordiamo un teorema di algebra lineare per mostrare che se $G$ è abeliano allora
tutte le matrici $\rho(g)$ sono simultaneamente diagonalizzabili:
due endomorfismi di uno spazio vettoriale diagonalizzabili sono simultaneamente diagonalizzabili se e solo se commutano tra loro.
\end{proof}




\begin{defn}[Omomorfismo di rappresentazioni]
Siano $\rho$ e $\sigma$ due rappresentazioni di $G$ su $V_{\rho}$ e $V_{\sigma}$ rispettivamente. Un omomorfismo di spazi vettorali $\varphi:V_{\rho}\to V_{\sigma}$ si dice \textit{omomorfismo di rappresentazioni} se
\[
	\forall\ a\in G, \forall\ v\in V_{\rho}\quad \varphi(\rho(a)(v)) = \sigma(a)(\varphi(v))
\]
oppure equivalentemente
\[
	\forall\ a\in G\quad \varphi\circ \rho(a) = \sigma(a)\circ \varphi
\]


%Siano $G, H$ due gruppi e $\rho: G \to V_\rho$ e $\sigma: H \to V_\sigma$ due loro rappresentazioni. Una funzione lineare da $V_\rho \to V_\sigma$ \footnote{Ovvero un omomorfismo da $V_\rho$ a $V_\sigma$} è un omomorfismo di rappresentazioni se rispetta la regola di composizione


%\[ \qquad \forall v,w \in V_\rho, V_\sigma\]


\end{defn}



\begin{defn}[Rappresentazioni isomorfe]
Due rappresentazioni si dicono \textit{isomorfe} se esiste un omomorfismo di rappresentazioni tra di loro che è anche bigettivo.
\end{defn}




\paragraph{Rappresentazioni di grado 1}\label{par:rappr_deg_1}
Dato un gruppo $G$, le sue rappresentazioni di grado $1$ sono per definizione
omomorfismi che vanno da $G$ all'insieme degli isomorfismi di $\C-$spazi vettoriali di dimensione $1$.
Senza perdere di generalità possiamo supporre che lo spazio vettoriale sia proprio $\C$. Dunque le
rappresentazioni di grado $1$ non sono altro che omomorfismi $G\to\C^*$.
\begin{thm}
Gli omomorfismi $G\to\C^*$ sono rappresentazioni di $G$ tra loro non isomorfe.
\end{thm}
\begin{proof}
Siano $\rho:G\to\C^*$ e $\sigma:G\to\C^*$ rappresentazioni isomorfe. Allora
esiste $\varphi:\C\to\C$ isomorfismo di $\C$ (ovvero $\varphi\in\C^*$) tale che per ogni $g\in G$ vale $\varphi \rho(g) = \sigma(g) \varphi$.
Visto che $\C^*$ è commutativo abbiamo allora che $\rho(g) = \sigma(g)$ per ogni $g\in G$, il che vuol dire che 
in realtà $\rho$ e $\sigma$ sono proprio la stessa rappresentazione.
\end{proof}

Negli esercizi sarà necessario trovare le possibili rappresentazioni di un gruppo $G$ (che qui supporremo finito), un buon punto di partenza è cercare per prima cosa le rappresentazioni di grado 1. Per fare questo c'è un metodo generale (indicheremo con $\rho$ la rappresentazione cercata e con $\mu_m$ il sottoinsieme di $\C$ che contiene le radici $m$-esime dell'unità):\footnote{stiamo cercando un omomorfismo di gruppi da $G$ a $GL(\C)=\C^*$}:
\begin{enumerate}
	\item cercare i generatori del gruppo $G$, che indicheremo con $g_1, \ldots, g_k$
	\item per ogni generatore $g_i$ trovare il suo ordine $n_i$ (ovvero il minimo intero $n_i$ tale che $g_i^{n_i}=e$)
	\item imporre che $\rho(g_i)\in \mu_{n_i}$ per ogni $i=1,\ldots,k$
	\item imporre infine che $\rho:G\to GL(\C)$ sia veramente un omomorfismo di gruppi (per ora abbiamo solo posto delle condizioni necessarie), per fare ciò bisogna controllare che le \textit{relazioni} con cui può essere presentato il gruppo siano rispettare nell'immagine\footnote{questo in parole povere significa che tutte le regole con cui vengono moltiplicati gli elementi devono essere rispettate, per dare una spiegazione formale servirebbero i prodotti liberi che però non sono necessari per questo corso}.
\end{enumerate}
Questo definisce un omomorfismo da $G$ a $\C^*$: dato $h\in G$ t.c. $h=g_{i_1}^{a_1}\cdots g_{i_t}^{a_t}$, allora $\rho(h) = \rho(g_{i_1})^{a_1}\cdots \rho(g_{i_t})^{a_t}$.
\begin{rem}
	Non è detto che tutte le rappresentazioni che si ottengono siano non isomorfe, questo metodo solamente le produce tutte.
\end{rem}


\begin{exmp}[Rappresentazioni di grado 1 di $C_n$]
Dato $G=C_n$, prendiamo un suo generatore $g$ (un qualsiasi elemento di ordine $n$), imponiamo che $\rho(g)\in \mu_n$ (ovvero $\rho(g)=e^{2k\pi i/n}$ per qualche $k\in \{0,\ldots,n-1\}$). Visto che $\forall h\in G\ \exists k\in \N$ t.c. $h=g^k$, ho già definito $\rho$ su ogni elemento di $G$.\newline
Ora abbiamo definito tutte le rappresentazioni di $C_n$ di grado 1, ma sono tutte distinte? In effetti mostriamo che in questo case il processo che abbiamo operato ha prodotto tutte rappresentazioni non isomorfe. Supponiamo di avere due rappresentazioni $\rho_1, \rho_2$ trovate con il metodo descritto sopra, supponiamo inoltre di avere $\varphi:\rho_1\to\rho_2$ un isomorfismo di rappresentazioni, vediamo che questo è assurdo: se fosse un isomorfismo di rappresentazioni dovrebbe valere che $\forall x\in \C, \forall g\in G$ $\varphi( \rho_1(g)x ) = \rho_2(g)( \varphi(x) )$, ma $\rho_1(g)$ è la moltiplicazione per uno scalare e $\varphi$ è lineare, quindi vorrebbe dire che $\rho_1(g)\varphi( x ) = \rho_2(g)( \varphi(x) )$, preso $x\neq 0$ si ottiene che $\rho_1(g)=\rho_2(g)\forall g\in G$, e questo è assurdo perchè differiscono almeno su un generatore di $G$.
\end{exmp}

L'esempio di prima era particolarmente semplice quindi non abbiamo dovuto faticare troppo, però il procedimento descritto è abbastanza laborioso con gruppi più complicati, vediamo un altro metodo che, conoscendo un quoziente ciclico di $G$, fornisce alcune rappresentazioni di grado 1 del gruppo.
\subparagraph{}
Sia $G$ un gruppo, $H\trianglelefteq G$ t.c. $G/H$ sia ciclico, allora trovando le rappresentazioni di $G/H$ di grado 1 siamo capaci di ricostruire delle rappresentazioni di grado 1 di $G$: consideriamo $\pi:G\to G/H$ la proiezione al quoziente, ovvero $\pi(g)=gH$, e un omomorfismo di gruppi $\rho:G/H\to \C^*$ (la rappresentazione di $G/H$), allora come omomorfismo $\sigma:G\to GL(\C)$ (ovvero una rappresentazione di $G$) prendiamo l'omomorfismo che fa commutare il seguente diagramma:
\[\tridiag G \pi {G/H} \rho {GL(\C)} \sigma\]
ovvero $\sigma(g)=\rho(\pi(g))$.

Questo metodo è particolarmente potente perché, per ogni rappresentazione $\sigma:G\to GL(\C)$, l'immagine è un gruppo ciclico\footnote{\'E abbastanza facile convincersene ricordando che ogni elemeno di $G$ deve essere mandato in una radice dell'unità.}, quindi, in virtù del primo teorema di omomorfismo\eqref{alg:primo_teo_omo}, l'immagine di una qualsiasi rappresentazione $\sigma$ di grado 1 è un quoziente ciclico di $G$.
Questo implica che in realtà il metodo esposto è in grado di trovare \textbf{tutte} le rappresentazioni di $G$ di grado $1$.


\begin{exmp}[Rappresentazioni di grado 1 di $S_3$]

\end{exmp}

\begin{exmp}[Rappresentazioni di grado 1 di $C_n \times C_n$]


(generalizzazione a prodotto di $C_{n_i}$)
\end{exmp}

\begin{rem}
	\'E possibile generalizzare ulteriormente il secondo procedimento descritto. Sia $G$ un gruppo, $H\trianglelefteq G$ tale che conosciamo le rappresentazioni (irriducibili) di $G/H$, allora riusciamo a ricostruire delle rappresentazioni di $G$ allo stesso modo: sia $\rho:G/H\to GL(V)$ rappresentazione di $G/H$, $\pi:G\to G/H$ la proiezione al quoziente, si considera lo stesso omomorfismo $\sigma$ di prima: $\sigma(g) = \rho(\pi(g))$.
\end{rem}













\newpage
\subsection{Operazioni con le rappresentazioni}

\begin{defn}[Somma di rappresentazioni]
  Date due diverse rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \ \sigma: G \to GL(V_\sigma)$ si può definire la rappresentazione somma $\rho + \sigma$ definita sullo spazio vettoriale $V_\rho \oplus V_\sigma$ definita in modo ovvio

  \[ (\rho + \sigma)(g) v = \rho(g) \pi_{V_\rho}(v) + \sigma(g) \pi_{V_\sigma}(v) \qquad \forall v \in V_\rho \oplus V_\sigma \]

  Questa definizione ha senso, infatti proiettando $v$ sui due spazi di partenza le due rappresentazioni sono definite e rimangono nello spazio di partenza. Si può poi fare la somma se riportiamo il tutto nello spazio più grande.
\label{defn:somma di rappresentazioni}
\end{defn}
Matricialmente $(\rho+\sigma)_g$ si rappresenta come
\[ [(\rho+\sigma)_g]= \begin{bmatrix}
[\rho_g] & 0\\ 
0 & [\sigma_g]
\end{bmatrix} \]
dove si intende ovviamente che la base ha i primi vettori in $V_{\rho}$, i secondi in $V_{\sigma}$.\\
\textsc{Osservazioni:}

\begin{enumerate}
\item $\deg(\rho+\sigma)=\deg(\rho)+\deg(\sigma)$
\item $\rho + \sigma \cong \sigma + \rho$
\item $\rho + (\sigma + \tau) \cong (\rho + \sigma ) + \tau$
\item Esiste l'elemento neutro che è la rappresentazione di grado 0 ma non esiste l'inverso.
\item se $\deg(\rho)=1$ allora $\rho(g^{-1})=\rho(g^{-1})^t=\rho^*(g)\Rightarrow \rho^*(g)=\rho(g)^{-1}$.

\end{enumerate}





\begin{defn}[Prodotto di rappresentazioni]
  Date due rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \sigma: G \to GL(V_\sigma)$ possiamo definire il prodotto di rappresentazioni che si indica con $\rho \otimes \sigma$  ma anche con $\rho\sigma$\footnote{Quest'ultima notazione può portare a confusione in quanto può essere scambiata con la composizione se le due rappresentazioni sono definite sullo stesso spazio, quindi cercheremo di evitarla.} definita sullo spazio $V_\rho \otimes V_\sigma$ tale che

  \[ \rho \otimes \sigma(g) (v \otimes w) = \rho(g) v \otimes \sigma(g) w \qquad \forall v \in V_\rho, w \in V_\sigma\]

  Non è restrittivo dare la definizione solo per gli elementi di $V_\rho \otimes V_\sigma$ decomponibili, in quanto sappiamo che sono una base dello spazio. Gli altri si otterranno per linearità.
  
\label{defn:prodotto di rappresentazioni}
\end{defn}


\textsc{Osservazioni:}


\begin{enumerate}
\item $1\otimes \rho \cong \rho$
\item $\rho \otimes \sigma \cong \sigma \otimes \rho$
\item $0 \otimes \rho \cong 0$
\item $\rho \otimes (\sigma \otimes \tau) \cong (\rho \otimes \sigma)\otimes \tau$
\item $\rho \otimes (\sigma_1 + \sigma_2) \cong \rho \otimes \sigma_1 + \rho \otimes \sigma_2$

\end{enumerate}





\begin{defn}[Rappresentazione duale]
Sia $\rho$ una rappresentazione di $G$ su $V_\rho$. Allora la rappresentazione duale $\rho^*$ è la rappresentazione di $G$ su $V_\rho ^*$ tale che $\rho^*(s)=\rho(s^{-1})^t$
\label{defn:rappresentazione duale}
\end{defn}

\begin{defn}[Definizione equivalente di duale]
Sia $\rho$ una rappresentazione di $G$ su $V$. Definiamo la rappresentazione duale $\rho^*$ come l'unica rappresentazione tale che
\[\rho_g^*(f)(\rho_g(v))=f(v)\ \ \forall g\in G,\forall v\in V,\forall f\in V^*\]
\end{defn}

\begin{note}
$\rho^*(s)=\rho(s^{-1})^t=\left(\rho(s)^{-1}\right)^t=\left(\rho(s)^t\right)^{-1}$. Inoltre, notare che la presenza di inverso e trasposto fa in modo che $\rho^*(s)$ sia una rappresentazione.
\end{note}


\begin{rem}
	Vale che $(\rho + \sigma)^* \cong \rho^* + \sigma^*$
\end{rem}

\begin{proof}
Consideriamo la funzione $\Theta : (V_\rho \oplus V_\sigma)^*\to V_\rho ^* \oplus V_\sigma ^*$ definita da
\[ \Theta(f) = (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) \]
per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$, dove $\imath_{V_\rho}$ e $\imath_{V_\rho}$ sono le immersioni di $V_\rho$ e $V_\sigma$ dentro la loro somma diretta.
\`E facile osservare che si tratta di un'applicazione lineare. Consideriamo anche
la funzione $\Xi: V_\rho ^* \oplus V_\sigma ^* \to (V_\rho \oplus V_\sigma)^*$ definita da
\[ \Xi(h,k) = h\circ\pi_{V_\rho} + k\circ\pi_{V_\sigma} \]
per ogni coppia di funzionali $h\in V_\rho ^*$, $k\in V_\sigma ^*$,
dove $\pi_{V_\rho}$ e $\pi_{V_\sigma}$ indicano come al solito le proiezioni sui sottospazi. 
Anche $\Xi$ è lineare, inoltre è facile osservare che $\Theta$ e $\Xi$ sono una l'inversa dell'altra. Dunque sono degli 
isomorfismi. Resta da mostrare che $\Theta$ è omomorfismo di rappresentazioni. Prendiamo $g\in G$: dobbiamo mostrare che 
\[ (\rho^*+\sigma^*)(g) \circ \Theta = \Theta \circ (\rho + \sigma)^*(g) \]
ovvero che per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$ vale
\[ [(\rho^*+\sigma^*)(g)] (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che si riscrive:
\[ ([\rho^*(g)](f\circ \imath_{V_\rho}), [\sigma^*(g)](f\circ \imath_{V_\sigma})) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che è equivalente a:
\[ (f\circ \imath_{V_\rho} \circ [\rho(g^{-1})], f\circ \imath_{V_\sigma}\circ[\sigma(g^{-1})]) = (f\circ [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho} , f\circ[(\rho+\sigma)(g^{-1})]\circ \imath_{V_\sigma}) \]
Per ottenere la tesi basta osservare che valgono
\[ \imath_{V_\rho} \circ [\rho(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho}\]
\[\imath_{V_\sigma} \circ [\sigma(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\sigma}\]
Notiamo infine che l'isomorfismo trovato è canonico, ovvero non dipende da alcuna scelta delle basi.
\end{proof}





\begin{defn}[Rappresentazione regolare]
  Consideriamo un gruppo $G$, per semplicità finito, e consideriamo uno spazio vettoriale $V_\rho$ di dimensione $|G|$ su $\C$. Una base di questo spazio ha sicuramente dimensione $|G|$. Possiamo indicare gli elementi della base con $e_g, \ \ \forall g \in G$. Un generico vettore di questo spazio si scrive quindi come

  \[ v = \dsum_{g \in G} a_g e_g \]

  Dove $a_g$ sono dei numeri complessi. Possiamo definire una rappresentazione di $G$ su questo spazio in questo modo:

  \[ \rho(h)v = \rho(h) \dsum_{g \in G} a_g e_g = \dsum_{g\in G} a_g \rho(h) e_g := \dsum_{g \in G} a_g e_{hg} \]

  Notare che questa definizione ha senso in quanto essendo $G$ un gruppo, $gh \in G$ e quindi sicuramente $e_{gh}$ è un elemento della base. Questa particolare rappresentazione di $G$ si chiama \emph{rappresentazione regolare} di $G$


  
  \label{defn:rappresentazione regolare}
\end{defn}



\subsection{Sottospazi invarianti e scomposizione delle rappresentazioni}



\begin{defn}[Sottorappresentazione]
Sia $\rho$ una rappresentazione di $G$ su $V_{\rho}$, una sottorappresentazione di $\rho$ è un sottospazio vettoriale $W\subseteq V_{\rho}$ tale che $\rho(s)(W)\subseteq W\ \forall\ s\in G$. Posso definire una rappresentazione $\sigma$ con $V_{\sigma}=W$ e $\sigma(s)=\rho(s)|_W$ (la indicherò con $\sigma\subseteq \rho$).
\end{defn}

\textbf{Notazione:} $V_{\rho}^G=\{v\in V_{\rho}: \rho(s)v=v\ \forall s\in G\}$.


\begin{defn}[Rappresentazione irriducibile]
Una rappresentazione $\rho$ di $G$ è \textit{irriducibile} se
\begin{enumerate}
	\item $\rho \neq 0$ ($\deg(\rho) \geq 1$)
	\item $\rho$ non ha sottorappresentazioni non banali (diverse da 0 e $V_{\rho}$).
\end{enumerate}

\end{defn}


\begin{defn}[Rappresentazione completamente riducibile]
Una rappresentazione si dice completamente riducibile se si può scrivere come somma di rappresentazioni irriducibili.
\end{defn}


\begin{rem}
Attenzione al gioco di parole in italiano: una rappresentazione irriducibile è completamente riducibile. Il nome della definizione può in effetti portare a confusione.
\end{rem}


\begin{rem} Normalmente la cosa che si fa più spesso in teoria della rappresentazione è cercare di scomporre la rappresentazione di un gruppo come somma di rappresentazioni irriducibili. Vedremo quindi adesso diversi teoremi che ci aiuteranno in questi problemi.

\end{rem}



\begin{exmp}[Rappresentazione regolare di $S_3$]


\end{exmp}



\begin{thm}[Le rappresentazioni di un gruppo finito sono completamente riducibili]
  Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\rho$ è completamente riducibile.
  \label{thm:gruppo finito completamente riducibile}
\end{thm}
Per dimostrare questo teorema ci servono diversi lemmi che enunciamo e andiamo a dimostrare. Finiti i lemmi seguirà la dimostrazione.


\begin{prop}[Prodotto hermitiano invariante] Sia $G$ un gruppo finito e $\rho: G \to V_\rho$ una sua rappresentazione. Allora lo spazio vettoriale $V_\rho$ ammette una forma hermitiana invariante sotto l'azione di $G$, ovvero un prodotto tale che $h(v,w) = h(\rho(g) v, \rho(g) w) \ \forall v,w\in V_\rho, \forall g \in G$
\label{thm:esistenza hermitiana}
\end{prop}

\begin{proof}

Lo spazio $V_\rho$ ammette sicuramente una forma hermitiana, che chiamiamo $h$. Ora andiamo a fare una sorta di media per trasformare questa forma in una invariante. Consideriamo quindi

\[h_G(v, w) := \dfrac{1}{|G|} \dsum_{g \in G} h(\rho(g)v,\rho(g) w) \]

\'E abbastanza facile mostrare adesso che effettivamente $h_G$ è invariante sotto l'azione di $G$. Infatti,

\[ h_G(\rho(h)v, \rho(h) w) = \dsum_{g \in G} h(\rho(gh)v , \rho(gh) w)\]

Ma dato che $G$ è un gruppo, questo vuol dire solo \emph{far partire la somma da un indice diverso.} Di conseguenza $h_G$ è $G$-invariante.

\end{proof}


\begin{lemma}
Sia $h: V_\rho \times V_\rho \to \C$ una forma hermitiana definita positiva e invariante per $\rho: G \to GL(V_\rho)$ e sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora se $W^\perp$ è l'ortogonale di $W$, $\rho|_{W^\perp}: G \to GL(W^\perp)$ è una sottorappresentazione.

\end{lemma}

\begin{proof}

Per noti teoremi di algebra lineare sappiamo che

\[ V_\rho = W \oplus W^\perp \]

Di conseguenza un generico vettore di $V_\rho$ si potrà scrivere come somma di $w_1 + w_2$, con $w_1 \in W$ e $w_2 \in W^\perp$. Inoltre sappiamo che $\rho_{|_W}$ è una sottorappresentazione. Mostriamo che anche $\rho_{|_W^\perp}$ è una sottorappresentazione: quello che dobbiamo mostrare è che $\rho(g) w_2 \in W^\perp \ \forall g \in G$. Dato che abbiamo un prodotto hermitiano la cosa più facile da verificare è che $\rho(g) w_2$ sia ortogonale a $W$. Consideriamo quindi l'espressione

\[ 0 = h(w_1, w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp\]

Ma noi sappiamo che $h$ è $G$-invariante, quindi

\[ 0 = h(w_1, w_2) = h(\rho(g) w_1, \rho(g) w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp \]

E dato che $\rho(g) W = W$ per ipotesi, abbiamo mostrato che $\rho(w_2) \in W^\perp$. 
\end{proof}




\begin{lemma}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo finito $G$. Sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora esiste una sottorappresentazione $\sigma: G \to GL(W')$ tale che

\[\rho = \rho|_W + \sigma \]
\end{lemma}


\begin{proof} La tesi segue dai due lemmi precedenti. L'ipotesi di gruppo finito si usa per l'esistenza della forma hermitiana invariante. 
\end{proof}



\begin{rem} Notare che il teorema precedente è falso per gruppi infiniti. Un esempio si può costruire prendendo $G=\Z$, $V=\C^2$, e come rappresentazione
\begin{align*}
	\rho:&\Z\to GL(\C^2)\\
	&k\to M^k
\end{align*}
dove $M=\begin{pmatrix}
        	1 & 1\\
        	0 & 1
        \end{pmatrix}$ è scritta nella base canonica.\newline
Si vede subito che una sottorappresentazine è $Span(e_1)$ essendo $e_1$ autovettore per ogni $\rho(k)$, ma non esiste un suo complementare $G$-invariante: se esistesse avrebbe dimesione 1, quindi avremmo diagonalizzato $\rho(k)\ \forall k\in \Z$, ma sappiamo che tali endomorfismi non sono diagonalizzabili. 
\end{rem}


%\textsc{Dimostrazione del teorema \ref{thm:gruppo finito completamente riducibile}:}
\begin{proof}[Dimostrazione del teorema \ref{thm:gruppo finito completamente riducibile}]
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di $G$. Se $\rho$ è irriducibile, allora è completamente riducibile e quindi segue la tesi. Se invece esiste un sottospazio invariante $W$, allora per i lemmi precedenti esiste $Z \subset V_\rho$ tale che $V_\rho = W \oplus Z$ e tale che $Z$ sia una sottorappresentazione. Per induzione si procede fino ad ottenere la tesi. %\qed
\end{proof}




\begin{thm} Se $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ sono rappresentazioni di $G$ e $f: V_\rho \to V_\sigma$ è un omomorfismo di rappresentazioni, allora $\Imm(f)$ è una sottorappresentazione di $\sigma$ e $\Ker(f)$ è una sottorappresentazione di $V_\rho$
\end{thm}
\begin{proof}
Se $v\in \Ker(f)$ allora per la definizione di omomorfismo di rappresentazioni ho che $\forall s\in G$ $f(\rho(s)v)=\sigma(s)f(v)=0$ e quindi $\rho(s)v\in \Ker(f)$. Allo stesso modo, se $w\in \Imm(f)$ allora $w=f(v)$ per qualche $v\in V_\rho$ e quindi sempre per la definizione di omomorfismo di rappresentazione $\sigma(s)w=\sigma(s)f(v)=f(\rho(s)v)\in \Imm(f)$
\end{proof}




\begin{thm}Sia $G$ un gruppo abeliano finito. Allora ogni rappresentazione di $G$ è isomorfa alla somma di rappresentazioni di grado 1.
\end{thm}
\begin{proof}
	\'E una conseguenza immediata della proposizione \eqref{prop:diagonalizzabilita rappresentazioni}: sia $\rho:G\to GL(V_{\rho})$, con $\dim V_{\rho} = n$, dato che $G$ è finito $\forall g\in G$ $\rho(g)$ è diagonalizzabile inoltre, visto che è abeliano, si sfrutta l'osservazione alla fine della proposizione per dedurre che le $\rho(g)$ sono simultaneamente diagonalizzabili. A questo punto il teorema è dimostrato: sia $\{v_1,\ldots,v_n\}$ una base comune di autovettori, per ogni $1\leq i\leq n$ $Span(v_i)$ è una sottorappresentazione di grado 1 di $G$ (perchè invariante per $G$), visto che $V_{\rho}=Span(v_1)\oplus\ldots\oplus Span(v_n)$, allora $\rho\cong\rho_1+\ldots+\rho_n$, dove $\rho_i:G\to GL(Span(v_i))$.
\end{proof}


\begin{prop} La rappresentazione regolare $\mathcal{R}$ di $C_n$ è isomorfa alla somma delle $n$ rappresentazioni irriducibili di grado 1 di $C_n$.

\end{prop}


\begin{lemma}
Date $\rho_1, \rho_2, \sigma$ rappresentazioni di $G$, allora

\[\Hom(\rho_1 + \rho_2, \sigma) \cong \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma)\]

\end{lemma}

\begin{proof}


% Consideriamo il lato sinistro dell'uguaglianza. Un generico elemento $\phi \in \Hom(\rho_1 + \rho_2, \sigma)$ è una mappa                                                                         %
%                                                                                                                                                                                                  %
% \[ \phi: V_{\rho_1} \oplus V_{\rho_2} \to V_\sigma \]                                                                                                                                            %
%                                                                                                                                                                                                  %
% che commuti con la rappresentazione, ovvero tale che                                                                                                                                             %
%                                                                                                                                                                                                  %
% \[ \phi\left( (\rho_1 + \rho_2 ) (g) v\right) = \sigma(g) \phi(v) \qquad \forall v \in V_{\rho_1} \oplus V_{\rho_2} \ , \forall g \in G\]                                                        %
%                                                                                                                                                                                                  %
%                                                                                                                                                                                                  %
% Esibiamo una mappa $\Xi: \Hom(\rho_1 + \rho_2, \sigma) \to \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma)$ nel modo più sensato e mostriamo che è un isomorfismo. In particolare, definiamo $\Xi$ %
%                                                                                                                                                                                                  %
%                                                                                                                                                                                                  %
% \[ \Xi(\phi)(v) =  \phi\left( \pi_{V_{\rho_1} } (v)\right) + \phi\left( \pi_{V_{\rho_2}} (v) \right)\]                                                                                           %


Consideriamo il lato destro dell'uguaglianza. Un elemento $\phi_i \in \Hom(\rho_i, \sigma)$ si potrà scrivere

\[
\phi_i :V_{\rho_i} \to V_\sigma | \quad \phi_i\left( \rho_i(g) v\right) = \sigma(g) \phi_i(v) \qquad \forall v \in V_{\rho_i}, \ \forall g \in G 
\]

Esibiamo ora una mappa 
\begin{align*}
\Xi :  \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma) &\to \Hom(\rho_1 + \rho_2, \sigma) \\
\phi_1 , \phi_2                                       &\to \phi_1 + \phi_2 \\
\end{align*}
 in modo ovvio e mostriamo che è un isomorfismo. 

\[ \Xi (\phi_1(v) + \phi_2(v)) = (\phi_1 + \phi_2) (v) \]


Innanzitutto la funzione $\phi_1 + \phi_2 : V_{\rho_1} \oplus V_{\rho_2}$ è effettivamente un omomorfismo di rappresentazioni e il motivo è che le due rappresentazioni in un certo senso agiscono separatamente sui due spazi senza interferire, quindi la verifica è immediata. 

$\Xi$ è inettiva in quanto per l'appunto le due rappresentazioni agiscono in modo separato sui due spazi e quindi affinché due coppie di $\phi_i$ diano lo stesso risultato, devono essere uguali le coppie.

$\Xi$ inoltre è suriettiva in quanto è un'applicazione lineare iniettiva fra spazi vettoriali che hanno la stessa dimensione.

\end{proof}


\begin{thm}[Lemma di Schur]
Siano $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ due rappresentazioni irriducibili di $G$ gruppo finito e $\phi:V_\rho \to V_\sigma$ un omomorfismo di rappresentazioni, allora $\phi$ è un isomorfismo oppure è identicamente nullo. Se poi $f:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni e lo spazio vettoriale $V_\rho$ è su $\C$ o su un campo algebricamente chiuso $\K$, allora $f$ è una moltiplicazione per scalare.
\end{thm}
\begin{proof}
Supponiamo che $\phi\neq 0$, allora sappiamo che $\Ker(\phi)\subseteq V_\rho$ è una sottorappresentazione di $\rho$, ma $\rho$ è irriducibile e quindi $\Ker(\phi)=0\Rightarrow \phi$ iniettiva. Ma anche $\Imm(\phi)\subseteq V_{\sigma}$ è una sottorappresentazione di $\sigma$ e, non essendo nulla ed essendo $\sigma$ irriducibile, coincide con tutto $V_\sigma \Rightarrow \phi$ suriettiva, da cui $\phi$ è un isomorfismo.
Consideriamo ora $f$: sia $\lambda$ un autovalore di $f$, che esiste perché $G$ è finito e stiamo lavorando su $\C$, allora $f-\lambda \Id:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni. Ma non è iniettivo, perché c'è almeno un autovettore relativo a $\lambda$, e quindi per la prima parte del lemma di Schur ho che $f-\lambda \Id$ è identicamente nullo, da cui ricaviamo che $f$ è la moltiplicazione per uno scalare ($\lambda$).
\end{proof}

Come immediato corollario, notiamo che, prese due rappresentazioni irriducibili $\rho$ e $\sigma$, si ha che $\dim\left(\Hom(\rho,\sigma)\right)$ è uguale a 1 se $\rho\simeq\sigma$, ed è uguale a 0 se $\rho\not\simeq\sigma$. Generalizzando, se $\rho$ si scompone in somma di rappresentazioni irriducibili come $\rho=\sum n_i\rho_i$, allora $\dim\left(\Hom(\rho,\rho_i)\right)=n_i$.



\begin{thm}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione e 

\[\rho = \dsum_{i=1}^N n_i \rho_i \]

una sua scomposizione come somma di rappresentazioni irriducibili a due a due non isomorfe. Allora la scomposizione è unica.
\end{thm}






\begin{thm}
Sia $\mathcal{R}$ la rappresentazione regolare di $G$, un gruppo finito, e sia 
\[ \mathcal{R} = \dsum_{i=1}^Nn_i \rho_i,\]
con $\rho_i$ irriducibili e a due a due non isomorfe. Allora ogni rappresentazione irriducibile di $G$ è isomorfa ad una $\rho_i$. Inoltre $n_i = \deg(\rho_i).$ 
\label{thm: teorema importantissimo}
\end{thm}
\begin{proof}
Per il corollario del Lemma di Schur, la tesi è equivalente al seguente fatto: se $\rho$ è una rappresentazione irriducibile di $G$, allora $\dim\left(\Hom(\mathcal{R},\rho)\right)=\deg\rho$ (in realtà non useremo l'ipotesi di irriducibilità di $\rho$). Costruiamo dunque un isomorfismo tra gli spazi vettoriali $\Hom(\mathcal{R},\rho)$ e $V_\rho$.

Chiamando $e_g$ l'elemento della base di $V_\mathcal{R}$ associato a $g\in G$, notiamo preliminarmente che, presa $\varphi\in\Hom(\mathcal{R},\rho)$, vale $\varphi(e_g)=\varphi(\mathcal{R}(g)e_{1})=\rho(g)\varphi(e_1)$, dove con $1$ si indica l'elemento neutro di $G$. Quindi, il valore di $\varphi(e_1)$ determina completamente $\varphi(e_g)$ per ogni $g$, e quindi determina completamente $\varphi$.

Sia allora $\Phi\colon\Hom(\mathcal{R},\rho)\to V_\rho$ tale che $\Phi(\varphi)=\varphi(e_{1})$. Per quanto appena visto, $\Phi$ è iniettiva. D'altra parte, lo stesso ragionamento ci permette anche di dimostrare facilmente che $\Phi$ è suriettiva: per ogni $v\in V_\rho$, la funzione $\varphi_v$ tale che $\varphi_v(e_g)=\rho(g)v$ è tale che $\Phi(\varphi_v)=\varphi_v(e_1)=\rho(1)v=v$. La linearità di $\Phi$ consente di concludere.
\end{proof}


\begin{cor}
Se $G$ è abeliano allora ha $|G|$ rappresentazioni irriducibili di grado 1 e $\mathcal{R}$ è la somma di queste.
\end{cor}


\begin{cor}
Sia $G$ un gruppo finito. $G$ ha un numero finito di rappresentazioni irriducibili, a meno di isomorfismi. Inoltre
\[|G| = \dsum n_i^2\]
\end{cor}






\newpage
\subsection{Alcuni isomorfismi notevoli}
Sia $G$ un gruppo e siano $\rho:G\to GL(V)$ e $\sigma:G\to GL(W)$ sue rappresentazioni.
Sappiamo bene che l'insieme delle applicazioni lineari da $V$ in $W$,
che denotiamo con $\Hom(V, W)$, è uno spazio vettoriale.
Quello che vogliamo fare ora è rendere tale spazio vettoriale una rappresentazione di $G$,
ovvero vogliamo definire un'azione lineare di $G$ su $\Hom(V, W)$. 
Il modo naturale di farlo è il seguente:
sia $\varphi\in\Hom(V, W)$. Definiamo $g\cdot \varphi\in\Hom(V, W)$ come la funzione che fa commutare il diagramma:
\[ \quaddiag V \varphi W {\sigma(g)} W {\rho(g)} V {g\cdot \varphi} \]
Ovvero esplicitamente: $g\cdot\varphi = \sigma(g) \circ \varphi\circ \rho(g^{-1})$

La verifica che si tratti di un'azione di gruppo è molto semplice, inoltre è evidente dalla definizione che l'azione è lineare:
dunque abbiamo davvero dotato $\Hom(V, W)$ di una struttura di $G-$rappresentazione.

Sappiamo già che $\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come spazi vettoriali.
Quello che vogliamo vedere è che sono isomorfi anche come $G-$rappresentazioni.
Ovviamente la rappresentazione che si considera su $V^*\otimes W$ è la rappresentazione $\rho^*\otimes\sigma$, che per semplicità indichiamo con $\tau$.

\begin{thm}
$\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come $G-$rappresentazioni.
\label{thm: isov*w}
\end{thm}
\begin{proof}
Conosciamo già un isomomorfismo di spazi vettoriali $\Psi:V^*\otimes W \to \Hom(V, W)$, costruito in
maniera tale che per ogni $f\in V^*, w\in W, v\in V$ si abbia $\Psi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\Psi$ è omomorfismo di rappresentazioni, ovvero che fissato $g\in G$ il seguente diagramma commuta:
\[ \quaddiag {V^*\otimes W} \Psi {\Hom(V, W)} {g} {\Hom(V, W)} {\tau(g)} {V^*\otimes W} \Psi \]
Ovvero dobbiamo controllare se
\[ g\circ \Psi = \Psi \circ \tau(g)\]
Possiamo limitarci a fare le verifiche sugli elementi decomponibili di $V^*\otimes W$, in
quanto essi generano tutto lo spazio. Prendiamo dunque $f\in V^*$, $w\in W$ e consideriamo $f\otimes w$. Vogliamo mostrare che
\[ g(\Psi(f\otimes w)) = \Psi(\tau(g)(f\otimes w))\]
Quindi prendiamo $v\in V$ e vediamo dove viene mandato dalla funzione a sinistra:
\begin{align*}
         & g(\Psi(f\otimes w))\ (v)\\
=  \qquad& \sigma(g)\ \Psi(f\otimes w)\ (\rho(g^{-1})\ v)\\
=  \qquad& \sigma(g)\ f(\rho(g^{-1})\ v)\ w\\
=  \qquad& f(\rho(g^{-1})\ v)\ \sigma(g)\ w
\end{align*}
Mentre dalla funzione a destra viene mandato in:
\begin{align*}
         & \Psi(\tau(g)(f\otimes w))\ (v)\\
=  \qquad& \Psi([f\circ\rho(g^{-1})] \otimes [\sigma(g)w])\ (v)\\
=  \qquad& f(\rho(g^{-1})v)\ \sigma(g)\ w
\end{align*}
Avendo ottenuto lo stesso risultato in entrambi i casi, si ha la tesi.
\end{proof}




\newpage
\section{Teoria dei caratteri}


\begin{defn}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo $G$. Definiamo carattere di $\rho$ la funzione che associa ad ogni elemento del gruppo $G$ la traccia della matrice associata all'elemento, ovvero

\[\chi_\rho(s) := \tr(\rho(s)) \qquad \forall s \in G \]
Notare che $\chi_{\rho}$ è una funzione che va dal gruppo in $\C$, ovvero $\chi_{\rho}: G \to \C$

\end{defn}

Vediamo delle proprietà elementari del carattere

\textsc{Osservazioni:}
\begin{enumerate}
	\item Se $\deg(\rho) = 1$ allora il carattere di $s$ è uguale a $\rho(s)$
	\item $\chi_{\rho_1} = \deg(\rho)$. \footnote{Al solito $\rho_1$ è la rappresentazione che manda ogni elemento nell'identità di $V_\rho$}\\
	Questo è vero poichè $[\rho_1]=\Id_n\Rightarrow \tr(\rho_1)=n$ ed $n=\deg(\rho)$.
	\item $\chi_{\rho + \sigma}(s) = \chi_\rho(s) + \chi_\sigma(s)$.\\ 
	Questo è dovuto al fatto che la somma di rappresentazioni si può scrivere come matrice a blocchi. Una volta scritto così è evidente il risultato.
	\item $\chi_{\rho\sigma}(s) = \chi_\rho(s)\chi_\sigma(s)$. In particolare dunque $\chi_{\rho^2}=(\chi_\rho)^2$.\\ 
	Questo deriva immediatamente dal teorema \ref{thm: tracciaprodotto}.
	\item $\chi_{\rho}(s^{-1})=\overline{\chi_{\rho}(s)}$\\
Essendo $G$ un gruppo finito, $\forall s\in G\ \rho(s)^n = id$ dove $n=|G|$: dunque tutti gli autovalori di $\rho(s)$ sono radici ennesime dell'unità e $\rho(s)$ è diagonalizzabile\footnote{Si veda la proposizione \ref{prop:diagonalizzabilita rappresentazioni}}. In tale base è evidente che:
$$\chi_{\rho}(s^{-1})=\tr(\rho (s^{-1}))=\tr(\rho (s)^{-1})=\sum_i\lambda_i^{-1}=\sum_i\overline{\lambda_i}=\overline{\tr(\rho(s))}=\overline{\chi_{\rho}(s)}$$
in quanto, avendo gli autovalori modulo 1, l'inverso coincide con il coniugio.  	
	\item $\chi_{\rho^*}(s)\footnote{Ricordiamo che $\rho^*(s) = (\rho(s)^{-1})^*$} = \overline{\chi_\rho(s)}$.\\
		Per l'osservazione precedente vale che
		$$\chi_{\rho^*}(s)=\tr(^t\rho(s^{-1}))=\tr(\rho(s^{-1}))=\overline{\tr(\rho(s))}=\overline{\chi_\rho(s)}$$
	\item $\chi_{\rho}(hsh^{-1})=\chi_{\rho}(s)$ ovvero $\chi_\rho$ è costante sulle classi di coniugio di $G$. La motivazione è semplice: se due elementi sono coniugati tra loro questo significa che le matrici corrispondenti saranno simili e la traccia è un invariante di similitudine.
	
Di conseguenza, non sarà necessario calcolare il carattere per ogni elemento del gruppo ma basterà farlo per le classi di coniugio di $G$.

Le funzioni che costanti sulle classi di coniugio di un gruppo vengono dette $funzioni\ di\ classe$. L'insieme delle funzioni di classe di un gruppo viene normalmente indicato con $Cl(G)$ e si verifica che esso è un sottospazio di $\mathbb{C}^G$.
	\item Supponiamo di avere una rappresentazione per permutazioni. Sia $I$ un insieme finito e $G$ un gruppo allora 
$$\chi_{\rho_{I}}(s)=\#\{ \text{punti fissi di } \rho_I(s) \} = |I^s|$$
dove $I^s:=\{i\in I| s\circ i=i\}$. La veridicità di questo fatto si vede scrivendo esplicitamente la matrice che rappresenta $\rho_I(s)$.
	\item Consideriamo la rappresentazione per permutazioni regolare $R$. Calcoliamone il carattere:
	\[ \chi_{\mathcal{R}}(s) = \begin{cases}
|G| \qquad \text{se } s=e \\
0 \qquad \text{se } s\neq e\\
\end{cases} \]
semplicemente perchè $s\circ g=g\Leftrightarrow s=e$.
\end{enumerate}
\begin{exmp}
$G=S_3$, $I=\{1,2,3\}$. Allora

\[ \chi_{\rho_I}(s) = \begin{cases}
3 \qquad \text{se } s=e \\
1 \qquad \text{se } s\ \text{è una trasposizione}\\
0 \qquad \text{se } s\ \text{è un treciclo}\\
\end{cases} \]
Ricordandoci che $\chi_{\rho_I}=\chi_{1+\rho}$ si ha che 
\[ \chi_{\rho}(s) = \begin{cases}
2 \qquad \ \ \text{se } s=e \\
0 \qquad \ \ \text{se } s\ \text{è una trasposizione}\\
-1\qquad \text{se } s\ \text{è un } 3\text{-ciclo}\\
\end{cases} \]
\end{exmp}

\begin{defn}[Prodotto hermitiano dei caratteri]

\[ \langle f | g \rangle = \dfrac{1}{|G|} \dsum_{s \in G} f(s)\overline{ g(s)} \]

\end{defn}


\begin{thm}[Relazioni di ortogonalità]
Se $\rho$ e $\sigma$ sono rappresentazioni irriducibili di $G$, allora vale

\[\langle \chi_{\rho}|\chi_{\sigma} \rangle = \begin{cases}
1 \qquad \text{se } \rho \cong \sigma \\
0 \qquad \text{altrimenti }\\
\end{cases} \]
\label{relazione di ortogonalita}
\end{thm}

Per dimostrare questo teorema abbiamo bisogno di un lemma che ora enunciamo e dimostriamo.




\begin{lemma}
Se $(\rho, V_\rho)$ e $(\sigma, V_\sigma)$ sono rappresentazioni \footnote{Non necessariamente irriducibili} di $G$, allora vale

\[ \langle \chi_\sigma | \chi_\rho \rangle  = \dim(\Hom (\rho, \sigma))\] 
\label{lemma relazioni ortogonalita}
\end{lemma}
\begin{proof}
L'idea principale per dimostrare questo lemma è di ridurci al caso più facile in cui una delle due rappresentazioni è quella banale. Per farlo notiamo un paio di cose
\[ \langle \chi_\sigma | \chi_\rho \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \chi_\sigma(s) \overline{\chi_\rho(s)} = \dfrac{1}{|G|} \dsum_{s\in G} {\chi_{\rho^*}(s)} \chi_{\sigma}(s) = \dfrac{1}{|G|} \dsum_{s\in G} \chi_{\rho^*\sigma}(s)  = \langle \chi_{\rho^*\sigma}  | 1 \rangle\]
Siamo passati da due rappresentazioni ad una sola. In particolare lo spazio vettoriale su cui agisce questa rappresentazione è 
\[ V_{\rho^* \sigma} = V_{\rho}^* \otimes V_\sigma \cong \Hom(V_\rho, V_\sigma)\]
dove l'isomorfismo di rappresentazioni è quello dimostrato nel teorema \ref{thm: isov*w}.
In particolare abbiamo che
\[ (V_{\rho^*\sigma})^G \cong \Hom(V_\rho, V_\sigma)^G \cong \Hom(\rho, \sigma)\]
Per cui dato che noi stiamo cercando $\dim(\Hom(\rho, \sigma))$, basterà trovare $\dim(V_{\rho^*\sigma})^G$.
\begin{lemma}
Sia $\rho$ una rappresentazione di un gruppo finito $G$. Allora
$\dim(V_{\rho})^G=\tr(T)$ dove
\[ T = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s).\]
\end{lemma}

Prima di iniziare la dimostrazione diamo una giustificazione della scelta della applicazione lineare $T$: questa è stata dettata dal fatto che
\[ \tr(T)=\tr\left(\dfrac{1}{|G|} \dsum_{s\in G} \rho(s)\right)= \dfrac{1}{|G|} \dsum_{s\in G} \tr(\rho(s))= \langle \chi_\rho| 1 \rangle \]
Si nota anzitutto che 

\[ \rho(t) T(v) = \dfrac{1}{|G|} \dsum_{s\in G} \rho(t)\rho(s) v = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s) = T(v)\]

ovvero $\Imm T\subseteq V_\rho^G$. D'altra parte se $v \in V_\rho^G$ allora è chiaro che $Tv = v$ (basta applicare la definizione). Per cui $\Imm T = V_\rho^G$. 

Inoltre 

\[T(Tv) = \ldots = Tv \qquad \text{verifica banale}\]

Per cui $T$ è un proiettore. A questo punto sappiamo dall'algebra lineare che

\[ V_\rho = \Ker T \oplus \Imm T = \Ker T \oplus V_\rho^G\]

Per cui $\tr T = \dim \Imm T = \dim V_\rho^G$. Ma dalla catena di deduzioni che abbiamo fatto

\[\dim(\Hom(V_\rho, V_\sigma)) = \dim(V_{\rho^*\sigma}^G) = \tr T = \langle \chi_{\rho^*\sigma} | 1 \rangle = \langle \chi_\sigma | \chi_\rho \rangle\]

\end{proof}

\begin{rem}
\[\langle \chi_\sigma | \chi_\rho \rangle=\overline{\langle \chi_\rho | \chi_\sigma \rangle}\]
ovvero per quanto appena dimostrato
\[ \dim(\Hom(V_\rho, V_\sigma))=\overline{\dim(\Hom(V_\sigma, V_\rho))}\]
tuttavia essendo dei numeri naturali deduciamo che le eguaglianze sussistono anche senza il coniugio.
\end{rem}











\begin{proof}[Dimostrazione del teorema \ref{relazione di ortogonalita}]

A questo punto la tesi del teorema \ref{relazione di ortogonalita} segue dal lemma precedente applicato insieme al lemma di Schur.
\end{proof}


\begin{rem}

  \begin{itemize}
  \item Ricordiamo che se $\rho$ è una rappresentazione di $G$, allora $\rho$ si può scrivere in modo unico come 
  
    \[ \rho = \dsum_i n_i \rho_i\]

    Dove le $\rho_i$ sono le rappresentazioni irriducibili di $G$ e gli $n_i$ sono numeri naturali $\geq 0$. Dall'equazione scritta sopra segue subito che
    
    \[ \chi_\rho = \dsum_i n_i \chi_{\rho_i}\]

    E possiamo ottenere un'informazione utile prendendo il prodotto scalare dell'equazione precedente con il carattere di una delle rappresentazioni $\rho_i$
    
    \[ \langle \chi_\rho | \chi_{\rho_j} \rangle = \dsum_i n_i \langle \chi_{\rho_i} | \chi_{\rho_j} \rangle \Rightarrow n_i \delta_{ij} = \langle \chi_\rho | \chi_{\rho_j} \rangle \Rightarrow n_i = \langle \chi_\rho | \chi_{\rho_i} \rangle\]

  \item Caso particolare interessante del fatto precedente riguarda la rappresentazione regolare di un gruppo. Difatti come sappiamo,
    
    \[ \chi_{\mathcal{R}}(s) = 
    \begin{cases}
      |G| \quad \text{se } s = e \\
      0 \quad \text{altrimenti}
    \end{cases}\]
    Quindi considerando una sottorappresentazione  si ha che 
    \[
    \langle \chi_{\mathcal{R}} | \chi_\rho \rangle = \frac{1}{|G|}|G|\chi_{\rho}(e)=\chi_{\rho}(e)=\deg(\rho)
    \]
    In particolare se $\rho$ è una sottorappresentazione irriducibile allora
    \[ \deg(\rho)=\dim(\Hom(\mathcal{R},\rho)) \]
    Quindi ottengo una conferma del teorema precedente 
    \[      
    \langle \chi_{\mathcal{R}} | \chi_\rho \rangle =\dim(\Hom(\mathcal{R},\rho))
    \]
    
  \item Se $\rho$ e $\sigma$ sono 2 rappresentazioni irriducibili allora $$\rho \cong \sigma \Leftrightarrow \chi_{\rho}=\chi_{\sigma}$$
    
  \item $\langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = \sum_i n_i^2$.
  \item Conseguenza dell'ultima osservazione è che una rappresentazione di un gruppo $\rho$ è irriducibile $\Leftrightarrow \langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = 1$ 
    
    
  \end{itemize}

\end{rem}


\begin{cor}[Corollario del lemma \ref{lemma relazioni ortogonalita}: Lemma di Burnside]

Sia $I$ un insieme finito e $G$ un gruppo finito che agisce su di esso. Vale allora la formula
\[ |I/G| = \dfrac{1}{|G|} \dsum_{s\in G} |I^s|\]
\end{cor}

\begin{proof}

Consideriamo la rappresentazione per permutazioni corrispondente all'azione di $G$ sull'insieme $I$, $(\rho_I, V_{\rho_I})$. Consideriamo

\[\langle \chi_{\rho_I} | 1 \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \tr \rho_I(s)\]

Ma è ovvio che 

\[\tr \rho_I(s) = |I^s| \qquad I^s = \{ i \in I | s \cdot i = i\} \]

Per cui lo spazio 
\begin{align*}
V_{\rho_I}^G &= \{v\in V_{\rho_I}:\rho_I(s)v=v\  \forall s\in G\} = \left\{\sum_i a_i e_i :\rho_I\left(\sum_i a_i e_i\right)=\sum_i a_i e_i\ \forall s\in G \right\} \\
&=\left\{\sum_i a_i e_i :\sum_i a_i e_{s\circ i}=\sum_i a_i e_i\ \forall s\in G \right\}= \left\{ \sum_i a_i e_i: \sum_i (a_{s^{-1}\circ i}-a_i)e_i=0\ \forall s\in G \right\}
\end{align*}
sarà composto da i vettori che hanno i coefficienti $a_i$ costanti su ciascuna orbita di $G$ su $I$, proprio per lasciarlo invariante. Perciò 

\[ \dim V_{\rho_I}^G = \text{numero delle orbite } = |I/G|\]

E con l'affermazione precedente si ottiene appunto il lemma di Burnside
\end{proof}



\begin{thm} Sia $G$ un gruppo finito e siano $\rho_1, \ldots , \rho_r$ le sue rappresentazioni irriducibili. Sia inoltre $\C(G)^{\#}$ lo spazio delle funzioni da $G$ in $\C$ costanti sulle classi di coniugio di $G$.

Chiaramente $\dim \C(G)^\# = $ numero di classi di coniugio di $G$ $:= s$. La tesi del teorema è che $r = s$ dove $r$ è il numero di rappresentazioni irriducibili. 
\end{thm}


\begin{rem}
 Per questo motivo la tabella dei caratteri sarà una tabella quadrata.
\end{rem}


\begin{proof}
 
Mostriamo intanto che $r \leq s$: i caratteri di $\rho_1, \ldots , \rho_r$ sono infatti ortonormali rispetto alla forma hermitiana
definita positiva $\langle \cdot | \cdot \rangle$, e quindi sono indipendenti (non posso avere più di $s$ vettori linearmente indipendenti 
in uno spazio vettoriale di dimensione $s$).

Verifichiamo ora che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = {0}$. Sia $f \in \C(G)^\#$ e $\rho$ una rappresentazione,
definiamo $T_f= \frac{1}{|G|}\dsum_s f(s)\rho(s)$ e verifichiamo che è un omomorfismo di rappresentazioni: 
\begin{align*}
 T_f \circ \rho(t) &= \frac{1}{|G|}\dsum_s f(s)\rho(s) \rho(t) = \frac{1}{|G|}\dsum_s \rho(t) \rho(t^{-1}) f(s)\rho(s) \rho(t) = \\
 &= \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}) \rho(s) \rho(t) = \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}st) =  \\
 &= \frac{1}{|G|}\rho(t) \dsum_{s'} f(s') \rho(s')= \rho(t) \circ T_f 
\end{align*}

Abbiamo usato il fatto che $f(s)\in \C$, quindi commuta con $\rho(g)$, e che $f$ è una funzione di classe nella 
sostituzione di $s$ con $s'= t^{-1}st$ (essendo il coniugio un automorfismo, cambia solo l'ordine della somma).

Se $\rho$ è irriducibile, $T_f= \alpha I$ è uno scalare per il lemma di Schur, $\alpha= \frac{\tr(T_f)}{\deg(\rho)}$.
Si ha $$\alpha =\frac{1}{\deg(\rho)}\tr(T_f)=\frac{1}{\deg(\rho)|G|}\dsum_s f(s) \chi_\rho(s)=
\frac{1}{\deg(\rho)} \langle f | \chi_{\rho^*} \rangle = 0$$ se $f \in \langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp}$.

Generalizziamo a quando $\rho$ non è irriducibile, ossia $\rho = \sigma_1+ \ldots + \sigma_n$, con $\sigma_i$ irriducibili. Allora 
si ha $V_\rho = V_{\sigma_1}  \oplus \ldots \oplus V_{\sigma_n}$, ed essendo $T_f=0 $ su ogni $V_{\sigma_i}$, è nullo anche su $V_\rho$.

Mostriamo che questo implica $f=0$: sia $\mathcal{R}$ la rappresentazione regolare di $G$; si ha:
$$ 0= |G| T_f (e_1) = \dsum_s f(s) \mathcal{R}(s)(e_1) = \dsum_s f(s) e_s $$

Di conseguenza $f(s)=0 \quad \forall s \in G$ essendo gli $e_s$ una base di $V_R$.
In questo modo abbiamo mostrato che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = \{0\}$, quindi $r=s$.
\end{proof}

















\subsection{Tabella dei caratteri}


Dato un gruppo $G$, possimo costruire la $tabella\ dei\ caratteri$ nel seguente modo:
\begin{itemize}
\item su ogni colonna mettiamo un rappresentante della classe di coniugio con sotto la cardinalità dell'orbita ovvero

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c}
\hline
$G$  & $e$ & $\Orb(g_1)$ & $\Orb(g_2)$ & \\
 & 1 & $|\Orb(g_1)|$ & $|\Orb(g_2)|$ & \\
\hline
 & &  & \\
\end{tabular}
\end{table}

\item su ogni riga mettiamo una rappresentazione irriducibile del gruppo
\item all'incrocio tra la rappresentazione $\rho_i$ e la classe di coniugio di $g_j$ inseriamo il valore di $\chi_{\rho_i(g_j)}$.


\end{itemize}



\subsection{Esempi di rappresentazioni di gruppi finiti}

\begin{exmp}[Tabella dei caratteri di $S_3$] 
La prima cosa da fare per costruire la tabella dei caratteri è vedere quanti elementi ha $S_3$, suddividerli in classi di coniugio e poi cercare le rappresentazioni irriducibili solo dopo aver fatto tutto questo. Notiamo subito che $S_3$ ha esattamente 3 classi di coniugio. La prima è ovviamente quella banale, composta solo dall'identità $e$. Poi c'è la classe delle trasposizioni $\{(1 2) ,(2 3), (1 3)\}$ che ha 3 elementi e poi ci sono i $3$cicli, ovvero $(1 2 3)$ e $(1 3 2)$. Possiamo cominciare a scrivere una tabella vuota $3\times 3$


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )  \\
 & 1 & 3 & 2 \\
\hline
 & &  & \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}



Una rappresentazione irriducibile che c'è sempre è la rappresentazione banale di grado 1, ovvero quella che manda ogni elemento nell'identità. La tabella con questa informazione diventa



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}


Un'altra rappresentazione che già conosciamo è il segno, $\epsilon$, che ricordiamo vale $(-1)^{n-1}$ dove $n$ è la lunghezza del ciclo. La tabella diventa




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

A questo punto ci sono due motivi per dire che l'ultima rappresentazione ha grado 2: il primo è che è l'unico modo di ottenere la relazione

\[ |G | = \dsum_i n_i^2 \]

Il secondo è che se fossero due rappresentazioni di grado 1 allora il gruppo avrebbe solo rappresentazioni irriducibili di grado 1 e un teorema che abbiamo fatto implicherebbe che $S_3$ sia abeliano, cosa palesemente falsa. 

Per trovare il carattere dell'ultima rappresentazione possiamo agire in più modi. Innanzitutto la tabella ora ha la forma




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 &  & \\
\hline
\end{tabular}
\end{table}


In generale ci saranno due numeri complessi $a, b$ nelle due caselle che mancano. Tuttavia noi sappiamo un sacco di teoremi che ci permettono di restringere il campo dei valori che possono avere. Per esempio noi sappiamo che 

\[\langle \rho_i | \rho_j \rangle = \delta_{ij}\]
 
Per cui imponendo che il prodotto scalare con entrambe le precedenti faccia 0 abbiamo due equazioni e due incognite, ovvero un problema risolvibile. L'altro modo è dire che

\[ \mathcal{R} = 1 + \epsilon + 2\rho\]

E dato che il carattere si comporta bene con la somma di rappresentazioni, 

\[\chi_{\mathcal{R}} = \chi_1 + \chi_\epsilon + 2 \chi_\rho  \]

Ma sappiamo anche che 

\[ \chi_{\mathcal{R}}(s) = 
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]

Per cui con agili conti riusciamo a completare la tabella








\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_3$}
\label{tabella caratteri s3}
\end{table}

L'ultimo modo è cercare di scomporre un'altra rappresentazione a caso di $S_3$, cercando di trovare la rappresentazione che ci manca. Per esempio ricordiamo l'azione di $S_3$ sui vettori di base di $\mathbb{R}^3$

\[ \tau(s) e_i = e_{s(i)}\]

Ricordiamo che il sottospazio di dimensione $1$ fatto dallo $Span$ del vettore $v = e_1 + e_2 + e_3$ è un sottospazio invariante in cui $\tau(s)$ è sostanzialmente l'identità. Il suo ortogonale è un altro sottospazio invariante su cui $\rho$ è irriducibile. Di conseguenza potremo scrivere

\[ \tau = 1 + \rho\]

E siamo sicuri che l'altra rappresentazione di grado 2 sia esattamente quella che stiamo cercando proprio grazie al teorema che ci dice che tutte le rappresentazioni irriducibili di un gruppo compaiono nella sua rappresentazione regolare. (Teorema \ref{thm: teorema importantissimo})

Dato che è facile calcolare il carattere di $\tau(s)$ in quanto è uguale a $\Fix(s)$, possiamo scrivere

\[ \Fix(s) = 1 + \chi_\rho\]

Da cui si ricava subito il carattere della rappresentazione $\rho$



\end{exmp}






\begin{exmp}[Tabella dei caratteri di $S_4$]
Facciamo la prima cosa importante: dividiamo $S_4$ in classi di coniugio. Per i soliti teoremi sugli $S_n$, le classi di coniugio saranno 
\[\{e\}, \{(a b)\}, \{(a b c)\}, \{(a b c d)\}, \{(a b)(c d)\}\]

E notiamo che sono 5. Possiamo quindi cominciare a compilare la tabella dei caratteri vuota



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


dove ho già messo la rappresentazione banale. Anche per $S_4$, essendo un gruppo simmetrico c'è la rappresentazione segno di grado 1. 




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


A questo punto bisogna fare cose a caso cercando le rappresentazioni irriducibili. Per esempio possiamo di nuovo considerare la rappresentazione per permutazioni



\[ \tau(s) e_i = e_{s(i)}\]


Che si scompone anche questa come

\[ \tau = 1 + \rho\]

Vorremmo sapere se $\rho$ è irriducibile. Potremmo invocare qualche teorema ma lo faremo con le mani calcolando il carattere di $\rho$


\[ \chi_\rho(s) = \Fix(s) - 1 = 
\begin{cases}
3 \quad \text{Se } s = e \\
1 \quad \text{Se } s = (a b) \\
0 \quad \text{Se } s = (a b c) \\
-1 \quad \text{Se } s = (a b c d ), (a b) (c d)\\
\end{cases}
\]

E andando a calcolare

\[\langle\chi_\rho |\chi_\rho\rangle = \dfrac{1}{24}\left(3^2  + 6 \cdot 1^2  + 0 + (-1)^2 \cdot (3 +6 )\right) = 1\]
 

Per cui è effettivamente irriducibile.  Aggiungiamola alla tabella.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

Abbiamo appena terminato le rappresentazioni che conoscevamo di $S_4$.\\
\textbf{Ottimo consiglio:} Quando non vengono in mente altre rappresentazioni, considera due già presenti nella tabella e fanne il prodotto. Risulta utile il seguente lemma.

\begin{lemma}
Se $\rho$ e $\sigma$ sono due rappresentazioni e $\deg(\rho)=1$ ( ovvero $\rho:G\rightarrow \mathbb{C}^*$), allora $\sigma$ è irriducibile $\Leftrightarrow$ $\rho\sigma$ lo è. Inoltre hanno lo stesso grado.
\end{lemma}

\begin{proof}
Che sia ancora a tutti gli effetti una rappresentazione si verifica esplicitamente sapendo che
\[
\forall s\in G\ \rho\sigma(s)=\rho(s)\sigma(s)
\]
Per dimostrare che è irriducibile si considera il fatto che
\[
\sigma \text{ irriducibile } \Leftrightarrow 1=\langle\chi_{\sigma}|\chi_{\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\sigma(s)}|^2
\]
Quindi
\[
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho(s)}\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)|^2|\chi_{\sigma(s)}|^2
\]
ed essendo $\rho(s)$ una radice $n-$esima dell'unità dove $n$ è l'ordine di $G$ si ha che 
\[
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}
|\chi_{\sigma(s)}|^2=\langle \chi_{\sigma}|\chi_{\sigma}\rangle
\]
Che abbiano lo stesso grado deriva dal fatto che
\[
\chi_{\rho\sigma}=\chi_{\rho}\chi_{\sigma}\Rightarrow \deg(\rho\sigma)=\chi_{\rho\sigma}(e)=\chi_{\rho}(e)\chi_{\sigma}(e)=\deg(\rho)\deg(\sigma)=\deg(\sigma).
\]
\end{proof}
Essendo $\epsilon$ di grado 1 e $\rho$ irriducibile allora anche $\rho\epsilon$ è un'altra rappresentazione irriducibile.
\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}
A questo punto dato che $|S_4|=24$ e che $1+1+3^2+3^2=20$ si possono avere due situazioni: $S_4$ potrebbe avere ancora 4 rappresentazioni irriducibili di grado 1 oppure solo più una di grado 2. Tuttavia abbiamo visto come $S_n$ ammetta solo due rappresentazioni irriducibili di grado 1 quindi siamo nel secondo caso.\\
Dato che ce ne manca solo una possiamo usare il trucco di prima (differenza dalla rappresentazione $\mathcal{R}$ ) e concludere:

\begin{table}[!ht] 
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
 $\sigma$& 2&  0 & -1& 0 & 2\\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_4$}
\label{tabella caratteri s4}
\end{table}


\begin{rem} Guardiamo la tabella, in particolare il "minore" ottenuto considerando le prime due e l'ultima riga e le prime 3 colonne. 

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 6 & 8 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\sigma$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\end{table}
Se la confrontiamo con la tabella dei caratteri di $S_3$ vediamo che sono analoghe. Intuitivamente $\rho$ in $S_3$ deriva dalla rappresentazione $\sigma$ di $S_4$ mediante un omomorfismo 
\[
S_4\rightarrow S_3
\]
che corrisponde ad una azione di $S_4$ su un insieme di 3 elementi. Tale insieme è il sottogruppo di Klein privato dell'unità ovvero
\[
\{ (12)(34),(13)(24),(14)(23)\}
\]
\end{rem}


In questo caso non è servito ma possono risultare utili i prossimi lemmi.
\begin{lemma}
$\rho^* $ è irriducibile $ \Leftrightarrow \rho$ è irriducibile.
\end{lemma}
Infatti $\chi_{\rho^*}=\overline{\chi_\rho} $ e quindi analogamente al lemma precedente si vede che
\[
1=\langle\chi_{\rho}|\chi_{\rho}\rangle \Leftrightarrow 1=\langle\chi_{\rho^*}|\chi_{\rho^*}\rangle
\]

\begin{lemma}
Se $\rho$ è una rappresentazione di grado $d$ di $G$, come sempre gruppo finito, allora:\\
$(a)$ $|\chi_{\rho}(s)|\leq d$ \\
$(b)$ Direttamente dal punto $(a)$ si decude che, 
\[
\chi_{\rho}(s)=d\Leftrightarrow \lambda_1,\ldots,\lambda_d=1\Leftrightarrow \rho(s)=\Id
\]
dove $\lambda_1,\ldots,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$.
\end{lemma}

\begin{proof}
 Se $\lambda_1,\ldots,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$ allora $\chi_{\rho}(s)=\sum_{i=1}^{d}\lambda_i$. Inoltre essendo $G$ finito $|\lambda_i|=1\forall i\in \{1,\ldots,d\}$. Se ne deduce che
\[
|\chi_{\rho}(s)|\leq \sum_{i=1}^d |\lambda_i|=d
\]
\end{proof}


\end{exmp}





\begin{exmp}[Tabella dei caratteri di $D_5$]

La prima cosa da fare è dividere $D_5$ in classi di coniugio


FINIRE

\end{exmp}



\subsubsection{I problemi della prima lezione visti con i nuovi strumenti}
\begin{exmp}[Problema 1 prima lezione]


\end{exmp}

\begin{exmp}[Problema 2 prima lezione]


\end{exmp}

\begin{exmp}[Problema 3 prima lezione]

Consideriamo un cubo. Scriviamo un numero su ciascuna delle facce e consideriamo l'operazione $T$ che per ogni faccia sostituisce al numero presente la media dei numeri presenti sulle 4 facce del cubo adiacenti. Vogliamo studiare il comportamento dei numeri del cubo quando questa iterazione viene compiuta molte volte.


Cerchiamo di formalizzare il problema usando la teoria della rappresentazione. Possiamo considerare l'insieme $F$ delle facce del cubo\footnote{Che ha quindi 6 elementi}. Una generica configurazione del cubo sarà esprimibile come 

\[ v = \dsum_{f \in F} a_f e_f \]

Dove $a_f \in \mathbb{C}$ e $e_f$ sono una base. L'operatore che sostituisce la media è lineare ma soprattuto commuta con le simmetrie del problema. Ora spiegherò meglio questo concetto.

Consideriamo il gruppo $G$ delle rotazioni del cubo, ovvero 

\[G = \{ g \in SO(3) | g(Cubo) \subset Cubo \} \]

\'E ovvio che il problema è invariante per simmetria, ovvero se $g \in G$, allora vale

\[ T v = g^{-1}T g v \]

Che è la formula di un cambio di base. Questo si può scrivere come 

\[gT = Tg \]

Ovvero ci dice che $\forall g \in G$ le due operazioni commutano. Le due frasi precedenti sono state dette un po' alla garibaldina in quanto non è $g$ ad agire sul cubo ma è una sua rappresentazione di grado $|F| = 6$. Di conseguenza è bene scrivere in modo formale che $\tau: G \to GL(V_\tau)$ è una rappresentazione del gruppo di rotazioni del cubo in $\mathbb{C}^6$ e questa rappresentazione commuta con un operatore $T$, ovvero

\[T\tau(g) = \tau(g) T \qquad \forall g \in G  \]


L'obiettivo che ci poniamo ora è quello di riuscire a scomporre $\tau$ come somma di rappresentazioni irriducibili in quanto una volta trovata una scomposizione 

\[ V_\tau = \bigoplus_{i = i}^n V_{\rho_i} \] 

Allora potremo usare il lemma di Schur per dire che su ogni $V_{\rho_i}$ l'operatore $T$ si comporta come scalare ovvero \emph{è più che diagonalizzato}. Per riuscire a capire qualcosa di come sono fatte le rappresentazioni di questo gruppo è opportuno prima cercare di dare una struttura più chiara a questo gruppo.

\'E possibile mostrare che QUALCUNO CHE HA VOGLIA DI FARLO LO FACCIA PLS $G \cong S_4$. A questo punto noi abbiamo una rappresentazione di grado 6 di $S_4$ che cerchiamo di scomporre come somma di rappresentazioni irriducibili. Tuttavia grazie al teorema \ref{thm: teorema importantissimo} sappiamo che tutte le sottorappresentazioni di $\tau$ saranno isomorfe alle sottorappresentazioni della rappresentazione regolare $\mathcal{R}(S_4)$, di cui abbiamo preventivamente calcolato la tabella dei caratteri \ref{tabella caratteri s4}. Dato che 


\[\tau = \dsum_i n_i\rho_i \Rightarrow \chi_\tau  = \dsum_i n_i\chi_{\rho_i}\]

Andiamo a calcolare i prodotti scalari dei caratteri delle rappresentazioni irriducibili di $S_4$ con il carattere di $\tau$ per trovare quali rappresentazioni compaiono. Per farlo calcoliamo prima il carattere di $\tau$


SCRIVI CHE NON HO VOGLIA

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline


\end{tabular}
\end{table} 



Per cui si ottiene

\[\tau = 1 + \epsilon\rho + \sigma \]

Ovvero

\[V_\tau = V_1 \oplus V_{\epsilon\rho} \oplus V_{\sigma} \]


Cerchiamo quindi di capire come sono fatti questi tre spazi che hanno rispettivaemente dimensione 1,3,2. 


SCRIVI PI\'U DETTAGLIATO CHE DEVO ANDARE A LEZIONE

\[V_1 =  Span(e_1 + e_2 + ... + e_6) \]
\[V_{\epsilon\rho} = \text{Le facce opposte hanno numeri opposti}\]
\[V_{\sigma} = \text{Le facce opposte hanno numeri uguali e la somma di tutti è 0}\]

Su questi spazi è facile vedere che effettivamente $T$ è scalare. In particolare

\[
\begin{cases}
T|_{V_1} = 1 \\
T|_{V_{\epsilon\rho}} = 0 \\
T|_{V_\sigma} = -\frac{1}{2}\\
\end{cases}
\]

E quindi è evidente che $T^n \to $ su ogni faccia viene la media dei numeri che c'erano all'inizio.


\end{exmp}











\newpage
\section{Rappresentazioni reali, complesse e quaternioniche}

Sono di un certo interesse le rappresentazioni su $\C$ che possono essere espresse con matrici a coefficienti reali. Diamo una definizione precisa:

\begin{defn}
Diciamo che una rappresentazione $\rho:G\to GL(V_\rho)$ del gruppo $G$ è reale se esiste una base di $V_\rho$ secondo la quale
le matrici di $\rho(g)$, al variare di $g$ nel gruppo, sono tutte a coefficienti reali.
\label{def: rappr reale}
\end{defn}

Sia $V$ un $\C-$spazio vettoriale di dimensione $n$.
Denotiamo con $V^\R$ lo spazio vettoriale su $\R$ che ha per ``insieme di vettori''
lo stesso di $V$, con la stessa somma ma chiaramente con prodotto per scalari ristretto ad $\R$.
Osserviamo che se $\{v_1, \dots, v_n\}$ è una base di $V$ allora 
$\{v_1, \imath v_1, v_2, \imath v_2,\dots, v_n, \imath v_n\}$ è una base di $V^\R$, il quale di conseguenza ha dimensione $2n$.
Se $\rho:G\to GL(V)$ è una rappresentazione del gruppo $G$, allora è definita 
in modo naturale una rappresentazione $\rho^\R:G\to GL(V^\R)$, in cui gli il gruppo $G$
agisce sullo spazio vettoriale esattamente come nella rappresentazione $\rho$ (se $G$ agisce 
in modo $\C-$lineare, a maggior ragione l'azione è $\R-$lineare).

Il fatto che $\rho$ sia una rappresentazione reale è equivalente a chiedere l'esistenza di un sottospazio reale $V_0 \subset V^\R$ che sia stabile sotto l'azione di $G$ e che abbia dimensione $n$. Cioè è possibile scomporre $V^\R$ come somma di sottorappresentazioni:
\[ V^\R = V_0 \oplus i V_0 \]

\begin{exmp}
Prendiamo come gruppo un gruppo ciclico, per esempio $\Z / 3\Z$\footnote{Che per i fisici è isomorfo a $C_3$}. Evidentemente tutte le rappresentazioni non banali di $G$ non sono reali, in quanto sono di grado 1 e sono le radici dell'unità.
\end{exmp}

Non è difficile osservare che se indichiamo con $\alpha_{i,j}$ i coefficienti della matrice $\rho(g)$
rispetto alla base $\{v_1, \dots, v_n\}$, allora la matrice di $\rho^\R(g)$ rispetto alla base
$\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$ si ottiene
ponendo al posto di $\alpha_{i,j}$ il blocchetto $2\times 2$:
\[\begin{pmatrix}
	\Re \alpha_{i,j} & -\Im \alpha_{i,j}\\
	\Im \alpha_{i,j} & \Re \alpha_{i,j}\\
\end{pmatrix}\]
dove $\Re$ e $\Im$ indicano parte reale e immaginaria.
Dunque abbiamo il seguente risultato:

\begin{prop}
	$\chi_{\rho^\R}(g) = \chi_\rho(g) + \chi_{\rho^*}(g)$
\end{prop} 
\begin{proof}
	Basta ricordare che $\chi_{\rho^*}$ è il complesso coniugato di $\chi_\rho$, dunque
	$\chi_\rho(g) + \chi_{\rho^*}(g) = 2\Re \chi_\rho(g)$ è evidentemente la traccia di $\rho^\R(g)$ per
	quanto detto sopra.
\end{proof}


\begin{prop}
	Se $\rho:G\to GL(V)$ è irriducibile ($V$ è un $\C-$spazio vettoriale), allora $\rho$ è reale se e solo se $\rho^\R$ è riducibile.
\end{prop}
\begin{proof}
	Se $\rho$ è rappresentazione reale, sia $\{v_1,\dots v_n\}$ una 
	base di $V$ tale che le matrici di $\rho(g)$ rispetto a tale base siano a coefficienti reali.
	Ora, $V^\R$ ha per base $\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$.
	Indico con $V_0$ il sottospazio di $V^\R$ generato da $\{v_1,\dots v_n\}$, che ha dimensione $n$.
	Ora, $V_0$ è $G-$invariante, dunque $\rho^\R$ è riducibile.
	
	Viceversa, supponiamo che $V^\R = U \oplus W$ sia scomposizione di rappresentazioni
	con $U$ e $W$ diversi da $\{0\}$. A meno di scambiare $U$ e $W$ posso supporre $\dim U \le n$.
	Il sottospazio di $V$ generato dai vettori di $U$ (con combinazioni lineari complesse) è $G-$invariante,
	dunque essendo $\rho$ irriducibile tale sottospazio deve coincidere con $V$.
	Tutto ciò si traduce nel fatto che $V^\R = U + \imath U$, e siccome $\dim\imath U = \dim U \le n$
	deve essere necessariamente che $U$ e $\imath U$ sono in somma diretta e hanno 
	entrambi dimensione $n$. Ora basta prendere una qualsiasi base di $U$,
	la quale è anche base di $V$ (essendo costituita da $n$ vettori che generano $V$) ed è tale che la matrice di $\rho(g)$ è 
	a coefficienti reali per ogni $g\in G$, ovvero $\rho$ è reale secondo la definizione \ref{def: rappr reale}.
\end{proof}

Ora andremo a fare una classificazione delle rappresentazioni in 3 tipi diversi:
\begin{itemize}
\item Reali
\item Complesse
\item Quaternioniche
\end{itemize}
La classificazione verrà fatta in base all'esistenza o meno di forme bilineari di un certo tipo invarianti sotto $G$.
Iniziamo con alcune proprietà soddisfatte dalle rappresentazioni reali:

\begin{thm}
Prendiamo una $\rho:G\to GL(V)$ rappresentazione $/\C$ che sia reale. Allora:
\begin{enumerate}
\item $\chi_\rho(g)\in \R$ $\forall g\in G$ 
\item $V$ possiede una forma bilineare simmetrica $G-$invariante non nulla. Ovvero lo spazio delle forme bilineari simmetriche $S^2V^*\subset V^*\otimes V^*$ è tale che $(S^2V^*)^G\neq 0$.
\end{enumerate} 
\end{thm}

\begin{proof}
Abbiamo supposto la rappresentazione reale. Quindi la matrice è reale ed in particolare lo sarà anche la sua traccia. Quindi il primo punto è vero. Ora veniamo al secondo punto: esisterà un $V_0$ spazio vettoriale reale $G-$invariante che tensorizzato con $\C$ dia $V^\R$. Consideriamo ora una forma bilineare simmetrica $B_0$ non degenere $/\R$
\[ B_0 \in S^2 V_0^*\]
Possiamo ora renderla invariante sotto l'azione di $G$ con il solito metodo del fare la media. Consideriamo quindi $\tilde B_0$ definito come
\[ \tilde B_0(v_1, v_2) = \dfrac{1}{|G|} \dsum_{g\in G} B_0(\rho(g) v_1, \rho(g) v_2)\]
Questo ha le caratteristiche precedenti ed è anche invariante sotto $G$, ovvero $\tilde B_0\in (S^2V_0^*)^G$. Possiamo a questo punto estenderla a forma bilineare su $V$ complessificandola in modo ovvio. Costruiamo quindi $B \in (S^2V^*)^G$ definendola nel seguente modo:
\[B(v_1 + i v_1', v_2 + i v_2') = \left( \tilde B_0(v_1, v_2) - \tilde B_0(v_1', v_2')\right)  + i \left( \tilde B_0(v_1', v_2) + B_0(v_1, v_2')\right)\]
\`E una banale verifica controllare che rispetta le caratteristiche richieste.
\end{proof}

\begin{rem}
Supponiamo che $\rho:G\to GL(V)$ sia una rappresentazione irriducibile. Allora il fatto che esiste una forma bilineare $G-$invariante non nulla
equivale a $(V^*\otimes V^*)^G\neq 0$. Ma ricordando che $(V^*\otimes V^*)^G \cong \Hom(V,V^*)^G$ (che è l'insieme degli omomorfismi di rappresentazioni) ciò vuol dire che $\rho$ e $\rho^*$ sono isomorfi, il che si traduce in $\chi_\rho = \chi_{\rho^*}$. Ma questo avviene esattamente quando $\chi_\rho$ assume solo valori reali. Ciò significa che la seconda condizione trovata (che prevede l'esistenza di una forma bilineare simmetrica invariante) è strettamente più forte della prima (che è soddisfatta anche nel caso in cui esista una forma bilineare invariante non necessariamente simmetrica).
\end{rem}

Vediamo ora un lemma che ci servirà per la classificazione.
\begin{lemma}
Sia $V$ una $G-$rappresentazione irriducibile $/\C$. Allora ogni forma bilineare non nulla $G-$invariante è non degenere. Inoltre, se esiste, è unica a meno di scalari, ovvero $\dim (V^*\otimes V^*)^G\le1$.
\end{lemma}
\begin{proof}
Prendiamo un elemento $B \in \left( V^* \otimes V^*\right) ^G$.

Ricordiamo che $V^* \otimes V^*$ e $\Hom(V, V^*)$ sono $G-$rappresentazioni isomorfe. In particolare:
\[\left(V^* \otimes V^*\right)^G \cong \Hom(V, V^*)^G \]
A questo punto, se esiste una forma bilineare non nulla invariante, questa corrisponde ad un omomorfismo di rappresentazioni $\phi: V\to V^*$.
Per il lemma di Schur o $\phi$ è nullo o $\phi$ è un isomorfismo. Dato che la forma è non nulla, allora $\phi = \lambda \Id$ con $\lambda \in \C\setminus\{0\}$.
\end{proof}


\begin{thm}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile $/\C$.\\
Definiamo $m_\rho = \dfrac{1}{|G|} \dsum_{g \in G} \chi_{\rho}(g^2)$ detto ``indicatore di Frobenius-Schur''. Allora
\begin{enumerate}
\item Se $B \in (V^* \otimes V^*)^G$ allora $B \in S^2V^*$ oppure $B \in \bigwedge ^2 V^*$
\item  $m_\rho \in \{-1, 0, 1 \}$
\item{ \begin{itemize}
   \item se $m_\rho = 0$ allora $(V^*\otimes V^*)^G = 0$
   \item se $m_\rho = 1$ allora $(S^2 V^*)^G \neq 0$
   \item se $m_\rho = -1$ allora $(\bigwedge^2 V^*)^G \neq 0$
\end{itemize}
}
\end{enumerate}
\end{thm}

\begin{proof}
Indichiamo $W = (V^*\otimes V^*)^G$ lo spazio delle forme bilineari $G-$invarianti, che è sottorappresentazione di $V^*\otimes V^*$.
Vogliamo dimostrare che $W\subseteq S^2V^*$ oppure $W\subseteq \bigwedge^2V^*$.
Dal lemma precedente sappiamo che $\dim W \le 1$. Se $\dim W = 0$ allora si ha automaticamente la tesi.
Supponiamo ora $\dim W = 1$.

Ricordando che $V^*\otimes V^* \cong S^2V^* \oplus \bigwedge^2V^*$ è scomposizione in somma di rappresentazioni,
consideriamo le proiezioni $p_{sym}:V^*\otimes V^* \to S^2V^*$ e $p_{alt}:V^*\otimes V^* \to \bigwedge^2V^*$
che in particolare sono omomorfismi di rappresentazioni (la verifica è facile, comunque questo accade ogni volta che si ha una somma di rappresentazioni).

Prendiamo ora $B\in W$ forma bilineare invariante non nulla. Allora almeno uno tra $p_{sym}(B)$ e $p_{alt}(B)$ è non nullo. Se fossero entrambi non nulli allora avremmo $\dim (V^*\otimes V^*)^G = \dim(S^2V^*)^G + \dim(\bigwedge^2V^*)^G \ge 2$, assurdo.
Dunque $B$ deve essere simmetrica oppure alternante, e ciò dimostra il punto 1.

Passiamo ora agli altri due punti. Ricordando il teorema \ref{thm:tracciasymalt} abbiamo che:
\begin{itemize}
\item $\dim(S^2V^*)^G = \langle 1|\chi_{S^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 + \chi_\rho(g^2)}{2}$
\item $\dim(\bigwedge^2V^*)^G = \langle 1|\chi_{\bigwedge^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 - \chi_\rho(g^2)}{2}$
\item $\dim(V^*\otimes V^*)^G = \langle 1|\chi_{\rho^*\otimes\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\chi_\rho(g)^2$
\end{itemize}
dunque possiamo scrivere
$\dim(S^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G + m_\rho}{2}$;\ \ 
$\dim(\bigwedge^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G - m_\rho}{2}$.

Ora, usando quanto ottenuto al punto 1 e distinguendo i casi in cui $V$ ammetta una forma simmetrica invariante, una forma antisimmetrica invariante, oppure non ammetta forme invarianti, si ottiene facilmente quanto affermato ai punti 2 e 3.
\end{proof}

\begin{defn}[Classificazione tramite l'indice di Frobenius]
Sia $\rho:G\rightarrow GL(V)$ una rappresentazione irriducibile $/\C$: essa è detta
\begin{itemize}
\item \textbf{reale} se $m_\rho = 1$  
\item \textbf{complessa} se $m_\rho = 0$ 
\item \textbf{quaternionica} se $m_\rho = -1$
\end{itemize}
\end{defn}

Ma la nuova definizione che abbiamo dato è compatibile con la \ref{def: rappr reale} data inizialmente?
La rassicurante risposta è data dal seguente lemma.
\begin{lemma}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile su $\C$. Allora $\rho$ è reale secondo la definizione \ref{def: rappr reale} se e solo se $m_\rho = 1$.
\end{lemma}
\begin{proof}
Sappiamo già che se $\rho$ è reale nel senso \ref{def: rappr reale} allora $m_\rho = 1$, in quanto $V_\rho$ ammette una forma simmetrica invariante non nulla. Mostriamo il viceversa.

Sia $B \in (S^2V_\rho^*)^G$, con $\dim_\C V_\rho = n$.
Noi stiamo cercando un certo $V_0 \subset V_\rho$ spazio vettoriale su $\R$ tale che $V_\rho^\R = V_0 \oplus i V_0$ .



% FRANCESCO GUARDA QUI
% SEMBRA CHE METTENDO LA DEFINIZIONE DA UNA PARTE O DALL'ALTRA CAMBI TUTTO
% HAI SPIEGAZIONI? HALP



Prendiamo ora una certa forma $h : V_\rho \times V_\rho \to \C$ hermitiana, definita positiva e $G-$invariante. Questa sicuramente esiste, è stato dimostrato nel teorema \ref{thm:esistenza hermitiana}.
Definiamo a questo punto un endomorfismo di spazi vettoriali reali $\phi: V_\rho \to V_\rho$ imponendo che soddisfi $B(x, y) = h(x, \phi(y))$.
La bontà di tale definizione è garantita dal teorema di Riesz.

Che proprietà ha $\phi$? Possiamo notare che $\phi$ è $G$-equivariante, ovvero vale $\phi(\rho(g)x) = \rho(g) \phi(x)$.
Mostriamolo rapidamente
\[ h(\phi(\rho(g)x) , y) = B(\rho(g)x, y) = B(x, \rho(g^{-1} ) y) = h (\phi(x), \rho(g^{-1} ) y) = h(\rho(g)x, y)\]
Questo non è male, in quanto se $\phi$ fosse lineare avremmo subito che $\phi$ è un omomorfismo di rappresentazioni irriducibili e potremmo usare Schur. Tuttavia
\[ \phi(z_1 x_1 + z_2 x_2) = \overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho\]
La linearità rispetto ai vettori è abbastanza evidente. L'antilinearità rispetto agli scalari deriva essenzialmente dal fatto che stiamo usando un prodotto hermitiano invece di un prodotto scalare. Infatti
\[ h( \phi(zx) , y) = B(zx, y) = B(x, zy) = h(\phi(x), zy) = \overline{z} h(\phi(x), y) \]
E quindi purtroppo $\phi$ non è davvero lineare come applicazione tra spazi vettoriali sul campo $\C$. Vediamo però che $\phi^2$ si comporta meglio:
\[\phi^2(z_1 x_1 + z_2 x_2) = \phi(\overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2))  = z_1 \phi^2(x_1) + z_2 \phi^2(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho \] 
E quindi effettivamente $\phi^2$ è un omomorfismo di rappresentazioni irriducibili. Per questo motivo possiamo applicare Schur e concludere che 
$\phi^ 2 = \lambda \Id_{V_\rho}$ per un certo $\lambda \in \C$.

Cosa possiamo dire su $\lambda$? Il claim è che sia $\lambda \in \R$ e $\lambda > 0$. Vediamo come si mostra
\[h(\phi(x), y) = B(x, y) = B(y, x) = h(\phi(y), x) = \overline{h(x, \phi(y))} \]
usando questo fatto possiamo considerare 
\[ \lambda h(x, y) = h(\phi^2(x), y) = \overline{h(\phi(x), \phi(y))} = h(x, \phi^2(y)) = \overline{\lambda} h(x, y) \qquad \forall x, y \in V_\rho\]
E questo ci dice ovviamente che $\lambda \in \R$. Per mostrare ora che $\lambda > 0$ dobbiamo sfruttare il fatto che la nostra forma hermitiana sia definita positiva. Per questo motivo andiamo a considerare
\[\lambda h(x, x) = h (\phi^2(x), x) = \overline{h(\phi(x), \phi(x))} \Rightarrow \lambda = \dfrac{\overline{h(\phi(x), \phi(x))}}{ h(x,x)} \qquad \forall x \neq 0 \in V_\rho\]
E dato che $h $ è definita positiva si ha anche $\lambda > 0$

A questo punto possiamo (a meno di riscalare) scegliere $\lambda = 1$, ovvero $\phi^2 = \Id$. A questo punto ci piacerebbe tornare a fare cose con $\phi$ e non $\phi^2$. Notiamo che se ci restringiamo a spazi vettoriali su $\R$, allora anche $\phi$ è lineare in quanto il coniugio non ci dà fastidio. Dato che quindi $\phi$ è un endomorfismo di uno spazio vettoriale reale tale che $\phi^2=1$, allora $\phi$ è diagonalizzabile e ha solo gli autovalori $\pm 1$.  Per questo motivo possiamo scomporre lo spazio di partenza $V_\rho = V_+ \oplus V_-$, con ovvia notazione per gli autospazi.

A questo punto ci manca poco. $V_+$ e $V_-$ sono sottospazi reali del nostro spazio di partenza. Se mostriamo che sono isomorfi, abbiamo trovato la nostra scomposizione dello spazio $V_\rho$ in due spazi $V_\rho = V_0 \oplus iV_0$.
Proprio per questo motivo è intelligente notare che vale $i V_+ = V_-$.
Mostriamo perché con il solito trucco della doppia inclusione. Prendiamo per esempio $x \in V_+$. Allora 
\[ \phi(ix) = - i \phi(x) = -i x \]
Ovvero il vettore $ix$ è autovettore di $\phi$ con autovalore $-1$. Applicando due volte questo ragionamento si ottiene facilmente
\[ V_+ \cong V_- \qquad (i V_+ = V_-)\]
Per cui a questo punto abbiamo finito
\end{proof}


\begin{lemma}
Sia $\rho:G\to GL(V)$ rappresentazione di un gruppo finito $G$. Allora $\dim V^G = \langle\chi_\rho|\chi_1\rangle$
\end{lemma}
\begin{proof}
Definiamo un'applicazione lineare $R:V\to V$ nel seguente modo:
\[ R = \frac{1}{|G|} \sum_{g\in G}{\rho(g)} \]
Se prendiamo $v\in V$ e $s\in G$ allora
\[ \rho(s) R(v) = \rho(s)\frac{1}{|G|} \sum_{g\in G}{\rho(g)v} = \frac{1}{|G|} \sum_{g\in G}{\rho(sg)v} = R(v)\]
ovvero $R(v)\in V^G$.
Inoltre se $w\in V^G$ allora $R(w)=w$. Di conseguenza $R^2 = R$ (cioè $R$ è una cosiddetta \emph{proiezione})
e quindi $R$ si diagonalizza con autovalori $0$ e $1$. Inoltre l'autospazio relativo a $1$ è proprio $V^G$.
Ciò implica che
\[ \dim V^G = \tr(R) = \frac{1}{|G|} \sum_{g\in G}{\chi_\rho(g)} = \langle\chi_\rho|\chi_1\rangle \]
\end{proof}

\begin{note}
Il proiettore $R$ sopra trovato si chiama \emph{proiettore di Reynolds}.
\end{note}


\begin{lemma} $\rho:G\rightarrow GL(V)$ rappresentazione irriducibile $/_\C$: allora $\chi_\rho$ è reale $\Leftrightarrow$ $\rho$ è una rappresentazione reale o quaternionica. 
\end{lemma}
\begin{proof} Abbiamo visto prima che $(V^*\otimes V^*)^G\neq 0\Rightarrow \chi_\rho=\chi_{\rho^*}\Rightarrow \chi_\rho$ è reale, e se $\rho$ è reale o quaternionica ha proprio $(V^*\otimes V^*)^G$ diverso da $0$. Viceversa $\chi_\rho$ reale implica $\chi_\rho=\chi_{\rho^*}$. D'altra parte dato che $\rho$ e $\rho^*$ sono entrambe rappresentazioni irriducibili allora $\rho\cong\rho^*\Leftrightarrow \chi_\rho =\chi_{\rho^*}$. Ciò significa che $\Hom_G(V,V^*)\neq 0\Rightarrow \exists B\in (V^*\otimes V^*)^G\Rightarrow \rho$ può essere solo reale o quaternionica.
\end{proof}


%\textbf{Esercizi:}
%\begin{enumerate}
%\item Mostrare che tutte le rappresentazioni irriducibili $/_\C$ si $S_4$ sono reali.
%\item $G$ gruppo ciclico di ordine dispari: allora le rappresentazioni non banali sono complesse.
%\end{enumerate} 
\begin{exercise}
	Mostrare che tutte le rappresentazioni irriducibili $/_\C$ si $S_4$ sono reali.
\end{exercise}
\begin{exercise}
	Sia $G$ gruppo ciclico di ordine dispari, allora le rappresentazioni non banali sono complesse.
\end{exercise}




\begin{thm}
Sia $\rho:G\rightarrow GL(V)$ rappresentazione irriducibile $/_\R$: allora $\exists \sigma:G\rightarrow GL(V_{\sigma})$ rappresentazione irriducibile $/_\C$ tale che è vera una delle seguenti affermazioni:
\begin{enumerate}
\item $\chi_\rho=\chi_\sigma$;
\item $\chi_\rho=\chi_\sigma+\chi_{\sigma^*}$;
\item $\chi_\rho= 2\chi_\sigma$.
\end{enumerate}
\end{thm}

\begin{proof} Definisco $V^\C=\C\otimes V$ la complessificazione di $V$ (è semplicemente $V$ visto però come spazio vettoriale su $\C$). Vi sono due casi:
\begin{enumerate}
\item $\rho:G\rightarrow GL(V^\C)$ (che chiamiamo $\rho^\C$ solo per ricordardi che $V$ è visto come spazio vettoriale sui complessi) è una rappresentazione irriducibile $/_\C$;
\item $\rho^\C:G\rightarrow GL(V^\C)$ è una rappresentazione riducibile $/_\C$.
\end{enumerate}
Se siamo nel primo caso $\sigma=\rho^\C$ è irriducibile $/_\C$ e ovviamente $\chi_\rho=\chi_{\rho^\C}$.\\ 
Se siamo nel secondo caso allora $V^\C=U\oplus W$ con $U$ e $W$ sottospazi $G-$invarianti. Quindi $\rho^\C=\rho_U+\rho_W$. Passiamo ora alla loro realificazione: $(\rho_\C)^\R=(\rho_U)^\R+(\rho_W)^\R$. Tuttavia sapendo che $\rho_\C$ derivava da una rappresentazione reale allora $\rho_\C^\R=\rho+\rho$ derivante dalla scomposizione $V^\R=V+iV$ dunque si ha
\[\rho+\rho=\rho_U^\R+\rho_W^\R\Rightarrow \rho=\rho_U^\R\]
 poichè le $\rho$ sono irriducibili e non si possono ulteriormente spezzare quindi devono essere contenute interamente in un sottospazio invariante. Analogamente se $\rho_U$ fosse ancora riducibile avrei una scrittura di $\rho$ come somma di due rappresentazioni più piccole, il che è assurdo. Quindi la $\sigma$ irriducibile su $\C$ cercata è $\rho_U$. Da questo deduciamo che $\dim(U)=\dim(W)=\frac{n}{2}$ e quindi deduciamo che se la dimensione di $V$ è dispari allora $\rho$ su $\C$ è irriducibile. Dato che $\sigma^\R=\rho\Rightarrow \chi_\rho=\chi_\sigma+\chi_{\sigma^*}$ per la proposizione $6.1$. Se $\sigma\cong \sigma^*$ allora si è nel terzo caso (ovvero quando il carattere di $\sigma$ è reale).

\end{proof}



\subsection{Quaternioni}
\label{sec:quaternioni}
Ovviamente le rappresentazioni quaternioniche hanno a che fare con il corpo dei quaternioni. Vediamo un po' di caratteristiche interessanti di questo oggetto.

Il corpo $\HH$ si può vedere come
\[\HH = \R \oplus i \R \oplus j \R \oplus k \R \]
Con $i,j,k$ unità immaginarie che rispettano le seguenti regole
\[ 
\begin{cases}
i^2 = j^2 = k^2 = -1 \\
ij = - ji = k \\
jk = -kj = i \\
ki = - ik = j \\
\end{cases}
\]

Vediamo un po' di proprietà interessanti. Per esempio se consideriamo 
\[ Q_8 =  \{\pm 1, \pm i, \pm j, \pm k \}\]
allora questo insieme è un gruppo se munito della moltiplicazione. Possiamo andare a vedere la tabella dei caratteri di questo gruppo. 

Per farlo possiamo utilizziamo il secondo metodo descritto nel paragrafo \eqref{par:rappr_deg_1}. Consideriamo si seguenti sottogruppi di $Q_8$
\begin{align*}
	H_1=\{ \pm1, \pm i \}\\
	H_2=\{ \pm1, \pm j \}\\
	H_3=\{ \pm1, \pm k \}
\end{align*}
\'E semplice verificare che $H_i$ è normale in $Q_8$ per $i=1,2,3$ e che $Q_8/H_i\cong \Z_2$\footnote{Visto che gli $H_i$ sono normali, $Q_8/H_i$ è un gruppo, avendo solo due elementi è necessariamente $\Z_2$.}. Di $\Z_2$ conosciamo le rappresentazioni di grado $1$: c'è la rappresentazione banale (che induce su $G$ la rappresentazione banale appunto) e $\rho:\Z_2\to \C^*$ che manda $[1]_2\to -1$. Da quest'ultima otteniamo le rappresentazioni che indicheremo con $\rho_i, \rho_j, \rho_k$ rispettivamente nel caso in cui quozientiamo per $H_1, H_2, H_3$; vediamo ora come calcolare una di queste, per esempio $\rho_i$, riportando il diagramma degli omomorfismi.
\[\tridiag {Q_8} {\pi_1} {Q_8/H_1} \rho {\C^*} {\rho_i}\]
Sia $g\in Q_8$, vogliamo capire quanto fa $\rho_i(g)$, ci sono due casi da considerare:
\begin{enumerate}
	\item se $g\in H_1$, allora $\pi(g) = eH_1$ dunque, visto che $eH_1$ è l'identità nel quoziente, $\rho_i(g)=1$
	\item se $g\not\in H_1$, allora $\pi(g) =gH_1\neq eH_1$, quindi, per la definizione di $\rho$, $\rho_i(g)=-1$
\end{enumerate}


L'ultima rappresentazione che si trova deve avere dimensione $2$ perché abbiamo già trovato $4$ rappresentazioni di grado $1$ (quella banale più le tre $\rho_i, \rho_j, \rho_k$ essendo tutte non isomorfe\footnote{La verifica è immediata}), abbiamo inoltre esaurito le rappresentazioni di grado $1$: potremmo ancora quozientare per $H=\{ 1, -1 \}$, ma si vede abbastanza facilmente che $Q_8/H\cong Z_2\times\Z_2$, metre avevamo detto che cercavamo quozienti ciclici; dalla formula $\sum n_i^2=|Q_8| = 8$ si deduce quindi che l'ultima rappresentazione deve avere grado $2$. Quest'ultima si ottiene dalla rappresentazione matriciale dei quaternioni: è noto che i quaternioni possono essere presentati come un sottoinsieme delle matrici complesse $2\times 2$:
\[
	\HH = \left\{ 
		\begin{pmatrix}
	              	z & w\\
	              	-\overline{w} & \overline{z}\\
	              \end{pmatrix}
	              \middle|\ z,w\in \C
		\right\}
\]
La rappresentazione è determinata dall'omomorfismo $\rho_{\HH}:Q_8\to GL(\C^2)$ definito sui generatori:
\[
	\rho_{\HH}(i) = \begin{pmatrix}
	          	i & 0\\
	          	0 & -i\\
	          \end{pmatrix}
	\qquad
	\rho_{\HH}(j) = \begin{pmatrix}
	          	0 & 1\\
	          	-1& 0\\
	          \end{pmatrix}
\]
Quindi vediamo che gli elementi di $Q_8$ possono essere visti come matrici $2\times2$ complesse:
\[ 
1_\HH = 
\left(
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right)
\qquad
i_\HH = 
\left(
\begin{array}{cc}
i & 0 \\
0 & -i \\
\end{array}
\right)
\qquad
j_\HH = 
\left(
\begin{array}{cc}
0 & 1 \\
-1 & 0 \\
\end{array}
\right)
\qquad
k_\HH =
\left(
\begin{array}{cc}
0 & i \\
i & 0 \\
\end{array}
\right)
\qquad
\]

Queste 4 matrici prendono il nome di matrici di spin di Pauli in quanto hanno alcune applicazioni in Fisica. Possiamo infine ricostruire quindi la tabella dei caratteri di $Q_8$

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 1 & 2 & 2 & 2 \\
$Q_8$ & 1 & -1 & $\pm i$ & $\pm j$ & $\pm k$ \\
\hline
$\rho_1$ & 1 & 1 & 1 & 1 & 1 \\
\hline
$\rho_i$ & 1 & 1 & 1 & -1 & -1 \\
\hline
$\rho_j$ & 1 & 1 & -1 & 1 & -1 \\
\hline
$\rho_k$ & 1 & 1 & -1 & -1 & 1 \\
\hline
$\rho_\HH$ & 2 & -2 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $Q_8$}
\label{tab: caratteri q8}
\end{table}


\'E interessante notare che la tabella dei caratteri di $Q_8$ è uguale a quella di $D_4$, ma i due gruppi non sono isomorfi. Questo ci ricorda che la tabella dei caratteri dice tanto di un gruppo ma non tutto.





Ora vorremmo effettivamente capire il perché dei nomi dati nella classificazione delle rappresentazioni come reali, complesse e quaternioniche. Per questo motivo ci servono un paio di concetti di algebra.


\begin{defn}[Algebra]
Un'algebra su $\R$ è uno spazio vettoriale reale $A$ dotato di una moltiplicazione $\cdot : A \times A \to A$ che sia associativa e bilineare.
Inoltre imponiamo che vi sia un elemento neutro rispetto a questa moltiplicazione. Quest'ultima richiesta non fa parte della
più generale definizione di algebra (le algebre che la soddisfano si dicono \emph{unitarie}), ma noi la inseriamo nella definizione
poiché in questo corso non tratteremo mai algebre non unitarie.
\end{defn}


\begin{defn}[Algebra di divisione]
Un algebra di divisione è un'algebra in cui ogni elemento escluso lo $0$ possiede un inverso moltiplicativo.
\end{defn}

\begin{exmp}
Gli esempi più standard di algebra di divisione su $\R$ di dimensione finita sono i campi $\R$ e $\C$ come spazi vettoriali reali.
Un esempio più sofisticato è dato dal corpo dei quaternioni $\HH$ (ovviamente visto come spazio vettoriale su $\R$).
\end{exmp}


\begin{rem}
Consideriamo una rappresentazione irriducibile $\rho:G\to GL(V_\rho)$ con $V_\rho$ spazo vettoriale su $\R$. Allora l'insieme degli endomorfismi
di $\rho$ 
\[ \End_G(V_\rho)\]
è un'algebra di divisione su $R$ se dotato della composizione.
Infatti per lemma di Schur (in particolare la prima parte dell'enunciato, che vale su ogni campo) ogni elemento di $\End_G(V_\rho)$ o è la funzione nulla oppure è un isomorfismo, quindi ammette inverso.
Se $\dim V_\rho = n$ allora $\End_G(V_\rho)$, essendo contenuto in $\End(V_\rho)$, ha dimensione finita, in particolare $\dim \End_G(V_\rho)\le n^2$.
Vedremo tuttavia che vale una limitazione molto più forte.
\end{rem}


Presentiamo ora un sorprendente teorema che afferma che non ci sono altre algebre di divisione di dimensione finita su $\R$ oltre a quelle che abbiamo elencato come esempi, ovvero $\R, \C, \HH$.
\begin{thm}[di Frobenius]
\label{thm: frobenius}
Sia $A$ un'algebra di divisione su $\R$ di dimensione finita. Allora si ha 
\[A \cong \R \quad \vee \quad  A \cong \C \quad \vee \quad A  \cong \HH \]
\end{thm}

\begin{proof}
Indichiamo con $\bm{1}\in A$ l'elemento neutro rispetto alla moltiplicazione di $A$ (occhio: è un vettore di $A$, non un numero reale).
Il sottospazio vettoriale generato da $\bm{1}$ è chiaramente isomorfo ad $\R$ come $\R-$spazio vettoriale, ma in
realtà lo è anche come sottoalgebra: infatti se $a,b\in\R$ allora
\[a\bm{1} \cdot b\bm{1} = ab (\bm{1}\cdot \bm{1}) = ab\bm{1} \]
dove nel primo passaggio abbiamo usato la bilinearità e nel secondo il fatto che $\bm{1}$ è elemento neutro.
Dunque, con un lieve abuso di notazione, possiamo scrivere $\R \subseteq A$, intendendo per $\R$ proprio il sottospazio generato da $\bm{1}$.
Se $\dim A = 1$ allora sarebbe $A=\R$. D'ora in poi supponiamo $\dim A > 1$.

Vogliamo ora mostrare che esiste una sottoalgebra di $A$ isomorfa a $\C$.
Sia $\alpha\in A \setminus\R$ e indichiamo con $A[\alpha]$ la sottoalgebra generata da $\alpha$, ovvero consideriamo l'insieme
\[A[\alpha] = \left\{ \dsum_{n = 0} ^N a_n \alpha^n |\ N\in\mathbb{N}, \ a_i \in \R,\ \alpha \in A   \right\} \]
L'algebra $A[\alpha]$, essendo contenuta in $A$, ha necessariamente dimensione finita, quindi esisterà un certo $N$ per cui gli elementi
$\alpha^0, \alpha^1, \dots \alpha^N$ sono linearmente dipendenti. Più precisamente prendiamo il minimo $N$ per cui questo avviene.
Allora esistono dei coefficienti $a_n$ reali tali che
\[ \dsum_{n=0}^N a_n \alpha^n = 0 \]
Ovvero il polinomio a coefficienti reali
\[ p(x) = \dsum_{n = 0}^N a_n x^n \] 
è annullato da $\alpha$.
Se potessimo scomporre in modo non banale $p(x) = p_1(x)p_2(x)$ allora avremmo che $0 = p_1(\alpha)p_2(\alpha)$, dunque $\alpha$ annullerebbe uno dei due fattori (qui
stiamo usando che $A$ è un'algebra unitaria), il che sarebbe assurdo vista l'ipotesi di minimalità di $N$.
Dunque $p(x)$ deve essere irriducibile. Ma su $\R$ i polinomi irriducibili possono solo avere grado $1$ o $2$.
Vediamo rapidamente perché non può avere grado $1$.
Supponiamo per assurdo che sia
\[p(x) = a_0 + a_1 x\]
Ma ciò vorrebbe dire che $a_0+a_1\cdot\alpha=0$, ovvero $\alpha=-a_0/a_1 \in \R$, ma noi avevamo assunto $\alpha\not\in\R$.
Di conseguenza $p(x)$ ha grado esattamente $2$.
Non è difficile mostrare che $A[\alpha]$ ha esattamente dimensione $2$: se $\alpha^2$ si scrive in termini di potenze inferiori lo faranno anche tutte le potenze successive,
dunque ogni elemento di $A[\alpha]$ si può scrivere nella forma $x + \alpha y$ con $x,y$ reali e $\{1,\alpha\}$ è base di $A[\alpha]$, essendo i due elementi indipendenti.
A meno di moltiplicare $p(x)$ per una costante per renderlo monico, possiamo scrivere 
\[ p(x) = (x-s)^2+t^2\]
con $s,t$ reali, $t\neq 0$. Se definiamo
\[i = \frac{\alpha-s}{t}\]
è facile osservare che $i^2 = -1$. Ora $\{1, i\}$ è base di $A[\alpha]$ (essendo $1$ e $i$ linearmente indipendenti) e il nostro $i$
gioca esattamente lo stesso ruolo dell'unità immaginaria in $\C$. A questo punto è immediato esibire un isomorfismo
\[ A[\alpha] \cong \C \]
Se $\dim A =2 $ abbiamo finito. Supponiamo d'ora in poi $\dim A > 2$ e scriviamo $\C\subset A$ identificando $\C$ con la
sottoalgebra $A[\alpha]$. Definiamo ora un'applicazione $\varphi:A\to A$ nel modo seguente:
\[ \varphi(x) = -ixi = ixi^{-1} \]
Osserviamo che $\varphi$ è $\R-$lineare, infatti presi $a,b\in\R$ e $x,y\in A$ si ha:
\[ \varphi(ax+by) = i(ax+by)i^{-1} = i(ax)i^{-1} +i(by)i^{-1} = a\varphi(x) + b\varphi(y)\]
Inoltre osserviamo che $\varphi^2$ è l'identità. Questo implica che possiamo decomporre
\[ A = A_+ \oplus A_-\] dove $A_+$ e $A_-$ sono gli autospazi relativi rispettivamente agli autovalori $1$ e $-1$.
In particolare gli elementi di $A_+$ sono esattamente quelli che commutano con $i$.
Vogliamo ora mostrare che $A_+ = \C$.
Sia $\beta\in A_+$. Imitando il ragionamento fatto prima con $\alpha$, possiamo considerare il polinomio a coefficienti complessi di minimo grado
che si annulla in $\beta$. Come fatto prima osserviamo che deve essere irriducibile (questo passaggio richiede in realtà qualche cautela,
ma tutto funziona come deve poiché $\beta$ commuta con tutti gli elementi di $\C$. Se avessimo preso $\beta\in A_-$ il ragionamento sarebbe errato).
Ma gli unici polinomi irriducibili a coefficienti in $\C$ sono quelli di grado $1$, quindi $\beta\in\C$.

Abbiamo dunque stabilito che $A_+ = \C$. Dato che $\dim A > 2$ possiamo prendere $z\in A_-$ non nullo.
Consideriamo l'applicazione $\psi_z:A\to A$ definita da $\psi_z(x) = zx$.
Osserviamo che valgono le seguenti implicazioni:
\[ x\in A_+ \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = -zx = -\psi_z(x)\]
\[ x\in A_- \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = (-z)(-x) = \psi_z(x)\]
ovvero $\psi_z$ scambia $A_+$ e $A_-$. Inoltre $\psi_z$ è bigettiva e lineare, dunque concludiamo che $A_+\cong A_-$ come $\R-$spazi vettoriali e in particolare $\dim A = 4$.

Con un ragionamento simile a quello che avevamo fatto per $\alpha$, osserviamo che $z^2$ è nel sottospazio generato da $1$ e $z$. Inoltre $z^2=\psi_z(z)\in A_+$.
Visto che $Span(1,z) \cap A_+ = \R$ deve essere per forza $z^2\in\R$.

Se fosse $z^2 \ge 0$ allora potremmo scrivere $z^2=r^2$ per qualche $r\in\R$.
Visto che $r$ e $z$ commutano sarebbe allora $(z-r)(z+r)=0$, dunque uno dei due fattori sarebbe $0$, assurdo perché $z\not\in\R$. Pertanto $z^2 < 0$ e possiamo definire
\[j = \frac{z}{\sqrt{-z^2}}\]
così $j^2 = -1$. Infine definiamo $k = ij$.

Ora $k$ e $j$ sono indipendenti e sono in $A_-$, dunque formano una base di $A_-$. Allora $\{1,i,j,k\}$ è base per $A$ ed è facile
verificare che questi quattro elementi rispettano le stesse regole di moltiplicazione dei quaternioni, pertanto si conclude che $A\cong\HH$ e la tesi è dimostrata.


\end{proof}




Dal teorema precedente si ha immediatamente che se $G\to GL(V)$ è una rappresentazione irriducibile su $\R$ allora $\End_G(V)$ è isomorfo a uno
tra $\R, \C, \HH$.

\begin{prop}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora:
\begin{itemize}
\item $\End_G(V) \cong \R$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 1$
\item $\End_G(V) \cong \C$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 2$
\item $\End_G(V) \cong \HH$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 4$
\end{itemize}
\end{prop}
\begin{proof}
Basta ricordare che $\dim \End_G(V) = \langle\chi_\rho|\chi_\rho\rangle$ e la tesi segue subito
dal teorema di Frobenius.
\end{proof}










\newpage
\section{Rappresentazioni indotte}
Supponiamo di avere $\rho:H\rightarrow GL(V)$ una rappresentazione di $H<G$ con $G$ gruppo. Il nostro obiettivo è quello di definire una $\widetilde{\rho}:G\rightarrow GL(V)$ rappresentazione di $G$ che estende $\rho$.

\begin{defn}Indichiamo con \emph{induzione da $H$ in $G$ di $W$} il seguente spazio vettoriale:
\[\Ind_H^G(W)=\{f:G\rightarrow W: \rho(h)\circ f(gh)=f(g)\ \forall (g,h)\in G\times H\}   \]
\end{defn}

\begin{rem} $\Ind_H^G(W)$ è lo spazio vettoriale di una rappresentazione di $G$.
Si consideri $N=\{f:G\rightarrow W\}$: definisco
\[\rho_N:G\rightarrow GL(N) \textup{ tale che }(\rho_N(g)f)(g')=f(g^{-1}g')\]
Si verifica che $\rho_N$ è una rappresentazione di $G$ su $N$. Notiamo innanzitutto che $\Ind_H^G(W)\subseteq N$ e che tale sottospazio è $\rho_N-$invariante poichè detta $f\in \Ind_H^G(W)$ si ha che
\[ \rho_N(g)(f)(g')=f(g^{-1}g')=\rho(h)f(g^{-1}g'h)=\rho(h)\rho_N(g)(f)(g'h)\]
ovvero $\rho_N(g)f\in \Ind_H^G(W)$. Questo ci dice che $\Ind_H^G(W)$ è una sottorappresentazione di $\rho_N$ che chiamiamo $\rho_{ind}:G\rightarrow GL(\Ind_H^G(W))$.
\end{rem}

\begin{exmp}
$H=\{e\}$ e $W=\C$ con $\rho:H\rightarrow GL(\C)$: allora
\[\Ind_H^G(W)=\{f:G\rightarrow \C:\rho(e)f(ge)=f(g)\}=\{f:G\rightarrow \C\}\]
\end{exmp}

\textbf{Notazione:} Spesso indicheremo lo spazio vettoriale $\{f:G\rightarrow \C\}$ con $\C[G]=\bigoplus_{g\in G}\C e_g$ dove gli $\{e_g\}_{g\in G}$ sono una base di $\{f:G\rightarrow \C\}$. 
In tal modo risulta che 
\[\{f:G\rightarrow W\}=\C[G]\otimes W\]
sempre nel senso che se prendo $\{e_g\}_{g\in G}$ una base di $\C[G]$ e $\{w_i\}\in W$ una base di $W$ allora 
\[e_g\otimes w_i:G\rightarrow W \textup{ tale che } g\mapsto w_i \textup{ e } g'\mapsto 0\ \forall g'\neq g\]

Torniamo a $\rho_{ind}$: ricapitolando $\rho_{ind}:G\mapsto GL(\Ind_H^G)$ tale che $g\mapsto \rho_{ind}(g)$ dove $\rho_{ind}(g)f(g')=f(g^{-1}g')$. Vediamo ora un po' delle sue caratteristiche. Poniamo per snellire la scrittura $V=\Ind_H^G(W)$. Ricordandoci il nostro aim, di certo vogliamo che $W$ sia un sottospazio di $V$.

\begin{defn}
Data $f\in \{f:G\rightarrow W\}$ (che contiene $V$) definiamo il supporto di $f$ 

\[\Supp(f)=\{g\in G:f(g)\neq 0\}.\]
\end{defn}

\begin{defn}
Dato $g\in G$ definiamo $V_g=\{f\in V : \Supp(f)\subseteq gH \}$.
\end{defn}

\begin{rem} se $g'\in gH$, allora $g'H=gH\Rightarrow V_g=V_{g'}$. Quindi più che dipendere dagli elementi di $G$, gli insiemi sopra definiti dipendono dalle classi laterali di $H$ in $G$. Perciò ora parleremo non più di $V_g$ bensì di $V_{gH}$.
\end{rem}

\begin{rem} dato che le classi laterali di $H$ sono una partizione di $G$, dette $\{g_iH\}_{i\in |G/H|}$ i rappresentanti delle classi laterali $gH$ al variare di $g$ in $G$ allora 
\[G=\bigsqcup_{i=1}^{|G/H|} g_iH\] 
e quindi ogni funzione la posso scrivere come combinazione lineare di $f_i\in V_{g_iH}$.
\[\Rightarrow V=\bigoplus_{i=1}^{|G/H|} V_{g_iH}\]
\end{rem}
Voglio ora rintracciare $W$, in quanto sono interessato a trovare un sottospazio isomorfo a $W$ dentro $V$. Notiamo innanzitutto che la scrittura precedente non è detto che sia una decomposizione di rappresentazioni, ma possiamo vedere subito che si comporta bene sotto l'azione di $\rho_{ind}$. 

\begin{lemma}
$\rho_{ind}$ permuta le varie $V_{gH}$ nel senso che $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. 
\end{lemma}

\begin{proof} Data $f\in V$, definisco
  \[f_i(g)=\begin{cases}
    f(g)& \textup{se } g\in g_iH\\
    0& \textup{altrimenti}
  \end{cases}\Rightarrow f=\sum_{i=1}^{|G/H|} f_i\ \textup{ e per come le ho definite } f_i\in V_{g_iH}\]
  Sia ora $f\in V_{g'H}$ e fissiamo un $g\in G$ allora
  \[\rho_{ind}(g)(f)(x)=f(g^{-1}x)\Rightarrow \rho_{ind}(g)(f)(x)\neq 0\leftrightarrow g^{-1}x\in \Supp(f)\subseteq g'H\leftrightarrow x\in g\Supp(f)\subseteq gg'H\]
  Ovvero $x\in \Supp(\rho_{ind}(g)(f))\Leftrightarrow x\in g\Supp(f)$. Ciò implica che
  \[ V_{g'H} \begin{matrix}
    \overset{\rho_{ind}(g)}{\longrightarrow}\\ 
    \underset{\rho_{ind}(g^{-1})}{\longleftarrow}
  \end{matrix} V_{gg'H} \]
  Essendo $\rho_{ind}(g),\rho_{ind}(g^{-1})\in GL(V)$ allora seguono iseguenti fatti:
  \begin{enumerate}
  \item $\rho_{ind}(g)V_{g'H}\subseteq V_{gg'H}$;
  \item $\rho_{ind}(g^{-1})V_{gg'H}\subseteq V_{g'H}$;
  \item $V_{g'H}\cong V_{gg'H}$, ovvero sono due spazi vettoriali isomorfi;
  \item $\rho_{ind}(g)^{-1}=\rho_{ind}(g^{-1})$;
  \end{enumerate}
  Quindi $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. 
\end{proof}

\begin{cor}
\[\forall i,j\in |G/H|\ \ \ V_{g_iH}\cong V_{g_jH}.\]
In particolare hanno tutti la stessa dimensione e quindi
\[\dim(V)=|G/H|\dim(V_{eH})=\dim(\Ind_H^G(W))\]
\end{cor}
Concentriamoci ora su $V_{eH}=\{f\in V:supp(f)\subseteq H\}$. Sia $h\in H$: allora $\rho_{ind}(h):V_{eH}\rightarrow V_{eH}$ in quanto $eH=hH$. Ciò significa che $\rho_{ind}|_{H}$ definisce una rappresentazione di $H$ in $V_{eH}$.
\begin{lemma}
La rappresentazione appena trovata di $H$ su $V_{eH}$ è naturalmente isomorfa a\\ $\rho:H\rightarrow GL(W)$.
\end{lemma}

\begin{proof} Considero la seguente funzione: 
\[\Phi:V_{eH}\rightarrow W\ \textup{tale che } f\mapsto f(e)\]
ovvero la funzione che associa ad un $f$ la valutazione della stessa nell'elemento neutro. Verifichiamo innanzitutto che si tratta di un omomorfismo di rappresentazione. Ricordiamo chi è $V_{eH}$:
\[V_{eH}=\{f:G\rightarrow W: \rho(h)f(gh)=f(g)\ \forall h,g\ \ \textup{e } \Supp(f)\subseteq H\}\]
$\Phi$ è un omomorfismo di rappresentazioni se e solo se $\Phi(\rho_{ind}(h)f)=\rho(h)\Phi(f)\ \forall h\in H$.
\[\Phi(\rho_{ind}(h)f))=\rho(h)\Phi(f) \Leftrightarrow (\rho_{ind}(h)f)(e)=\rho(h)(f(e))\Leftrightarrow f(h^{-1}e)=f(h^{-1})=\rho(h)f(e)\]
ma ciò è vero in quanto le $f\in \Ind_H^G(W)$. \\
Verifichiamo che $\Phi$ è iniettiva. $\Supp(f)\subseteq H$ e $\forall h\in H\ f(h)=\rho(h^{-1}f(e))$: questo mi dice che una volta che si è fissato il valore di $f(e)$ allora la funzione è univocamente determinata su $H$. D'altra parte so che in $G\setminus H$ la funzione è identicamente nulla $\Rightarrow$ quindi è univacamente determinata su tutto $G$. Allora detti $\Phi$ è iniettiva.\\
Vediamo ora che è anche surgettiva. Sia $w\in W$ vogliamo $f\in V_{eH}$ che valutata in $e$ dia $w$. Definiamo $f_w:G\rightarrow W$ nel seguente modo: 
\[f_w:=\begin{cases}
\rho(h^{-1})(w)& \textup{ se } h\in H\\
0& \forall g\in G\setminus H
\end{cases}\]
Si verifica che tale funzione soddisfa le proprietà richieste. 
\end{proof}


Dunque possiamo ora concludere il seguente teorema
\begin{thm} Dette $g_1H,...,g_{|G/H|}H$ i rappresentanti delle classi laterali allora 
\[\Ind_H^G(W)=\bigoplus_{i=1}^{|G/H|}\rho_{ind}(g_i)V_{eH}\bigl( =\bigoplus_{i=1}^{|G/H|} V_{g_iH}\bigr) \]
Inoltre $\rho_{ind}|_H$ definisce una rappresentazione di $H$ su $V_{eH}$ isomorfa a $\rho$. In particolare quindi 
\[\dim(\Ind_H^G(W))=|G/H|\dim(W)\] 
\end{thm}
Vediamo ora degli esempi.
\begin{enumerate}
\item Abbiamo visto prima $H=\{e\}$ $W=\C$ allora $\Ind_H^G(W)=\C[G]$ è la rappresentazione regolare di $G$. 
\item $H\neq \{e\},\subseteq G$ e $\rho:H\rightarrow GL(W)$ la rappresentazione banale con $W=\C$. Allora
\[\Ind_H^G(\C)=\{f:G\rightarrow \C:\rho(h)f(gh)=f(gh)=f(g)\ \forall g,h\} \]
\[\text{allora } f \in \Ind_H^G(\C)\text{ se e solo se è costante sulle classi laterali di } H\]
\[\text{quindi } \Ind_H^G(\C)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di }\ H\}=\C[G/H]\]


%% \begin{tikzpicture}
%% \centering
%% \matrix(m)[matrix of math nodes,row sep=3em,column sep=3em,minimum width=3em]
%% {
%% f:G& \C\\
%% G/H& \\};
%% \path[-stealth]
%% (m-1-1) edge (m-1-2)
%% (m-1-1) edge node [left] {$\pi$} (m-2-1)
%% (m-2-1) edge node [right] {$\ f$} (m-1-2);
%% \end{tikzpicture}\\

\[
\begin{diagram}
G          & \rTo^{f}            & \C \\
\dTo<{\pi} & \ruTo>{f \circ \pi} &    \\
G/H        &                     &    \\
\end{diagram}
\]


$G/H$ è un insieme finito dove $G$ agisce per moltiplicazione a sinistra ($g(g'H)=gg'H$). Quindi $\Ind_H^G(W)$ è la rappresentazione per permutazione associata all'azione di $G$ in $G/H$. \\
\textbf{Recall:} $X$ è un $G-$insieme, sia $X= \bigsqcup X_i$ la decomposizione in $G-$orbite: allora 
\[\C[X]=\bigoplus_{i=1}^{\#-orbite} \C[X_i]\] 
\item $H\subseteq G$ e $\rho:H\rightarrow GL(W): \rho(h)=id\ \forall h\in H$. Nel caso in cui $W$ avesse dimensione 1 si torna all'esempio 1. Tuttavia anche se $\dim(W)>1$, $\Ind_H^G(W)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di } H\}=\C[G/H]\otimes W$ 
\end{enumerate}
\begin{exercise} Supponiamo che $W=W_1\bigoplus W_2$ una rappresentazione di H non riducibile: allora 
\[\Ind_H^G(W)=\Ind_H^G(W_1)\bigoplus \Ind_H^G(W_2)\]
\end{exercise}

\subsection{Formula di aggiunzione}
Vediamo ora una proprietà universale di queste rappresentazioni indotte. Prima abbiamo definito passando da $\rho$ a $\rho_{ind}$ sostanzialmente una applicazione
\[\Ind:Rappr(H)\rightarrow Rappr(G)\]
e abbiamo anche "l'inversa" ovvero la restrizione della rappresentazione ad H
\[\Res:Rappr(G)\rightarrow Rappr(H)\]
e risulta che $\Res_H^G(V)=V$ visto però come una rappresentazione di $H$ ($\rho_{res}=\rho|_H$). Che legame c'è tra queste due applicazioni? 
\begin{thm}
Sia $\rho:H\rightarrow GL(W)$ una rappresentazione di $H$ e sia $\sigma:G\rightarrow GL(U)$ una rappresentazione di $G$. Allora
\[\Hom_H(W,\Res_H^G(U))\cong \Hom_G(\Ind_H^G(W),U)\]
ovvero che ogni $\phi:W\rightarrow \Res_H^G(U)$ omomorfismo di $H-$rappresentazione si estende in modo unico a un $\hat{\phi}:\Ind_H^G(W)\rightarrow U$ omomorfismo di $G-$rappresentazione
\end{thm}
\begin{proof} sia $\phi:W\rightarrow \Res_H^G(U)$ un omomorfismo di $H-$rappresentazione e sia $V:=\Ind_H^G(W)=\bigoplus_{i=1}^{\#-orbite} \rho_{ind}(g_i)V_{eH}$. Essendo $V_{eH}\cong W$ come $H-$rappresentazioni, allora $\phi:V_{eH}\rightarrow \Res_H^G(U)$: voglio estenderlo a $V$. Essendo $V$ definito come una somma diretta è sufficiente definire l'omomorfismo sui blocchi e per questo considero $V_{g_iH}$: ho che
\[V_{g_iH}\overset{\rho_{g_i^{-1}}}{\rightarrow} V_{eH}\overset{\phi}{\rightarrow} \Res_H^G(U)\overset{\sigma(g)}{\rightarrow} U\Rightarrow \sigma(g)\phi\rho_{ind}(g_i^{-1}):V_{g_iH}\rightarrow U\] 
Incollando tutte queste applicazioni ottengo $\hat{\phi}:\Ind_H^G(W)\rightarrow U$ dove
$\hat{\phi}(v)=\sigma_{g}\circ \phi\circ \rho_{ind}(g^{-1})(v)$ con $v\in V_{gH}$. Si verifica che $\hat{\phi}\in \Hom_G(\Ind_H^G(W),U)$. \\
Vediamo ora che questa estensione è unica. Siano $\hat{\phi_1}$ e $\hat{\phi_2}$ due estensioni di $\phi$: valutiamole nello stesso elemento $v\in V_{gH}$.
\[\hat{\phi_1}(\rho_{ind}(g^{-1})v)=\phi(\rho_{ind}(g^{-1})(v))=\hat{\phi_2}(\rho_{ind}(g^{-1})v)\]
dove l'uguaglianza è vera perchè $\rho_{ind}(g^{-1})(v)\in V_{eH}$. Quindi coincidendo sui singoli blocchi le due estensioni coincidono anche su $V$. E quindi l'estensione è unica. 


\end{proof}






\subsubsection{Le rappresentazioni dei gruppi diedrali $D_{n}$}
Per fare un esempio di come si possano usare le rappresentazioni indotte, studiamo le rappresentazioni dei gruppi diedrali. Ricordiamo che i gruppi diedrali hanno due generatori, che in accordo con quello che è stato fatto all'inizio del corso chiameremo $\sigma$ e $\tau$. Il gruppo $G = D_{n}$ si scrive quindi

\[ D_{n} = \{ 1, \sigma, \sigma^2, ... \sigma^{n-1}, \tau, \tau\sigma, ... \tau\sigma^{n-1} | \ \tau^2 = \sigma^n = 1, \quad, \sigma\tau = \tau\sigma^{n-1} \}\]


Consideriamo l'ovvio sottogruppo $H$ generato da $\sigma$, che è evidentemente un gruppo ciclico, isomorfo a $\Z / n \Z$. Le rappresentazioni di questo gruppo, dato che è abeliano, sono decisamente banali, in quanto sono di grado 1 e sono le radici ennesime dell'unità. In particolare la rappresentazione $\rho_k$ è univocamente determinata dal suo valore sul generatore. Indicheremo in modo naturale

\[\rho_k(\sigma) = \omega^k  \qquad \omega = e^{\frac{2\pi i}{n}}\]


Studiamo ora la rappresentazione che induce $\rho_k$, ovvero studiamo $\Ind_G^H \rho_k$, che per amore di brevità indicheremo con $\Ind \rho_k$. \'E abbastanza utile notare che noi conosciamo la dimensione della rappresentazione indotta, in quanto conosciamo la formula

\[ \dim \Ind^H_G \rho_k = |G/H| \deg \rho_k = 2 \cdot 1 = 2\]

Per andare avanti è intelligente ricordare la definizione di questo spazio. Indicheremo con $W$ lo spazio su cui agisce $\Ind \rho_k$ (che è ovviamente isomorfo a $\C^2$)


\[ \Ind \rho_k = \{ f : G \to W\ |\ f(gh) = \rho(h)^{-1} f(g) \quad \forall g \in G, \forall h \in H \}\]

Possiamo prendere una base di questo spazio per scrivere la matrice che le rappresenta e calcolarne poi il carattere, per esempio. Scegliamo questa base: prendiamo le funzioni $f_H, f_{\tau H}$ definite da 

\[ 
\begin{cases}
f_H (e) = 1 \\
f_H (\tau) = 0 \\
\end{cases}
\qquad
\begin{cases}
f_{\tau H} (e) = 0 \\
f_{\tau H } (\tau) = 1 \\
\end{cases}
\]



Evidentemente questa è una base e, data la definizione dello spazio, è anche sufficiente a determinare il comportamento delle funzioni su tutto il gruppo $G$.

In particolare una generica funzione $f \in \Ind \rho_k$ si potrà scrivere come $\C$ combinazione lineare delle due funzioni appena definite, ovvero

\[ f = a f_H + b f_{\tau H} \qquad a,b \in \C\]


Possiamo ovviamente esprimere la funzione in modo diretto come

\[ f(g) = f(1) f_H(g) + f(\tau) f_{\tau H} (g) \]

Ora che abbiamo lo spazio dove agiscono le rappresentazioni indotte, dobbiamo trovare esplicitamente le rappresentazioni. Per ogni $\rho_k$, indicheremo con $\hat \rho_k$ la sua indotta. Per vedere come è fatta, facciamola agire sulla base dello spazio. In questo modo potremo scriverla come matrice.

Dobbiamo quindi calcolare

\[ 
\begin{cases}
\hat \rho_k (\sigma) f_H \\ 
\hat \rho_k (\sigma) f_{\tau H} \\
\end{cases}
\qquad
\begin{cases}
\hat \rho_k (\tau) f_H \\
\hat \rho_k (\tau) f_{\tau H}\\
\end{cases}
\]

Facciamo questa cosa in quanto in realtà dovremmo calcolare su ogni elemento del gruppo la matrice, ma sappiamo che $\sigma$ e $\tau$ generano il gruppo, ed è più facile moltiplicare matrici piuttosto che fare conti di questo tipo.  Per esempio


\[ 
\hat \rho_k (\sigma) f_H(1) = f_H(\sigma^{-1}) = \rho_k(\sigma) f_H(1) = \omega^k f_H(1)
\]

Con conti analoghi si ottiene


\[ 
\hat \rho_k (\sigma) = 
\left(
\begin{array}{cc}
\omega^k & 0 \\
0 & \omega^{-k} \\
\end{array}
\right)
\qquad
\hat \rho_k (\tau) = 
\left(
\begin{array}{cc}
0 & 1 \\
1 & 0 \\
\end{array}
\right)
\]


A questo punto abbiamo un sacco di rappresentazioni di $D_{n}$. Se vogliamo costruire tutte le rappresentazioni del gruppo dobbiamo domandarci se sono irriducibili o meno e in caso decomporle. Alcuni casi particolari si vedono abbastanza facilmente. Per esempio, per $k=0$, $\hat \rho_k(\sigma) = \Id$. Di conseguenza, tutte le $\hat \rho_0$ sono simultaneamente diagonalizzabili e quindi in realtà la rappresentazione non sarà irriducibile in quanto il gruppo diventa abeliano.



BISOGNA FINIRE CHE DEVO ANDARE A LEZIONE.






















\newpage
\section{Rappresentazioni di gruppi compatti}

Ci piacerebbe in qualche modo estendere quello che abbiamo fatto per i gruppi finiti ad una categoria particolare di gruppi infiniti, in particolare i gruppi compatti. Per farlo, abbiamo bisogno di un po' di definizioni che generalizzino quello che abbiamo fatto. 


\begin{defn}[Gruppo topologico] Dato un insieme $G$, una funzione $\cdot: G\times G \to G$ e un sottoinsieme delle parti di $G$, $\tau$, la terna $(G, \cdot, \tau)$ si dice gruppo topologico se valgono le seguenti proprietà:

\begin{itemize}
\item $(G, \cdot)$ è un gruppo.
\item $(G, \tau)$ è uno spazio topologico.
\item Le due operazioni $\cdot : G \times G \to G$ e $i: G \to G$, la seconda definita come la mappa che manda $g $ in $g^{-1}$,  sono continue secondo la topologia indotta da $\tau$.
\end{itemize}

\end{defn}


La definizione che abbiamo dato sopra è in un certo senso l'unica che si poteva dare per mettere insieme topologia e gruppi, in quanto le prime due sono obbligate e la terza di dice che in qualche modo vogliamo che le due operazioni di gruppo e di topologia si parlino fra di loro e che non siano indipendenti. Si possono fare diversi esempi, come


\begin{itemize}
\item $G$ finito, con la topologia discreta
\item $G$ qualsiasi, con la topologia discreta
\item Il gruppo additivo dei numeri reali $(\R, +)$ con la topologia euclidea
\item Il gruppo $GL_n (\R)$ (o anche su $\C$), con la topologia indotta da quella di $M_n(\R)$

\end{itemize}


In questo corso non parleremo di rappresentazioni di gruppi topologici in generale, ma solo di alcuni, ovvero quelli compatti. Per cui diamo la definizione

\begin{defn}
$(G, \cdot, \tau)$ si dice compatto se $(G, \tau)$ è compatto.
\end{defn}


Facciamo un po' di esempi di gruppi compatti che andremo a trattare
\begin{itemize}
\item I gruppi finiti, con la topologia discreta.
\item I gruppi $O_n, SO_n, U_n, SU_n$: dimostriamo che effettivamente lo sono (lo dimostriamo per $O_n$ perchè per $U_n$ la dimostrazione è analoga e gli altri sono sottogruppi).
Se $G\subseteq \R^m$ dotato della topologia euclidea indotta allora
\[G\ \text{compatto}\ \Leftrightarrow G\ \text{è chiuso in}\ \R^m\ \text{e limitato}\]
Considero $O_n(\R)\subseteq \R^{n^2}$: esso è così descritto
\[O_n(\R)=\{A\in M_n(\R):\ A^tA=I\}=\{A\in M_n(\R):\ \sum_{h=1}^{n}a_{ih}a_{jh}=\delta_{ij}\ \forall i,j=1,..,n\}\]
$\Rightarrow O_n$ è chiuso poichè descritto da equazioni (risulta essere l'intersezione finita di cotroimmagini di chiusi $\{1\},\{0\}$). Inoltre per $i=j\Rightarrow \sum_{h=1}^n a_{ih}^2=1\Rightarrow |a_{ij}|\leq 1\ \forall i,j\Rightarrow O_n$ è limitato.
\end{itemize}

In particolare può essere utile notare alcuni fatti noti che possono aiutare, cioé
\begin{align*}
SO_2(\R) \cong \mathbb{S}^1 \\
SU_2(\C) \cong \mathbb{S}^3
\end{align*}

A questo punto sarà opportuno dare una nozione di rappresentazione in cui entra in gioco la topologia di $G$, oltre alla sua struttura di gruppo. 
\begin{defn}[Rappresentazione continua]
Consideriamo una rappresentazione del gruppo topologico $G$, $\rho: G \to GL(V_\rho)$. Diciamo che la rappresentazione è continua se la mappa
\[ \rho: G \to GL(V_\rho)\]
è continua. Per il primo termine bisogna considerare la topologia su $G$, che è definita in quanto il gruppo è topologico, mentre su $GL(V_\rho)$ si usa la topologia euclidea. 
\end{defn}

Facciamo alcuni esempi
\begin{itemize}
\item Se $G$ è finito, ovviamente con la topologia discreta ogni rappresentazione è continua. In un certo senso questo non è così banale, in quanto ci dice che quello che andremo a fare sarà una generalizzazione di quello che abbiamo fatto per gruppi finiti.
\item Un'altra rappresentazione del tutto ovvia: la rappresentazione del gruppo $GL(V)$ in $GL(V)$ tramite l'identità, che è evidentemente continua
\item La mappa esponenziale $e^x: (\R, +) \to (\R^+, \cdot )$ che associa $x$ a $e^x$
\end{itemize}



\subsection{Le rappresentazioni di $\mathbb{S}^1$}
Per chiarirci le idee, andremo a studiare nel dettaglio un gruppo particolare, ovvero $\mathbb{S}^1$, che è ovviamente isomorfo a $SO_2(\R)$. Cerchiamo quindi innanzitutto le sue rappresentazioni irriducibili su spazi di dimensione finita sui complessi. 

\begin{lemma}
Tutte le rappresentazioni irriducibili $/_\C$ di un gruppo compatto abeliano $G$ hanno dimensione 1. Di conseguenza, essendo $\mathbb{S}^1$ un gruppo compatto abeliano, tutte le rappresentazioni di $\mathbb{S}^1$ irriducibili su uno spazio vettoriale complesso hanno dimensione 1.
\end{lemma}
\begin{proof}
  $G$ è un gruppo compatto abeliano e sia $\rho:G\rightarrow GL(V)$ con $V$ $\C-$sp. vettoriale: quindi
  \[ gh = hg \qquad \forall g,h \in G \ \Rightarrow\ \rho(g)\rho(h) = \rho(h)\rho(g) \qquad \forall g, h \in G\]
  Il che vuol dire che, fissato $h$, $\rho(h)$ è un'omomorfismo di rappresentazioni irriducibili su $\C$, ovvero appartiene a $\End_{\mathbb{S}^1}(V)$ proprio perchè commuta con $\rho(g)$ per tutte le $g$. Sia $\lambda$ un autovalore di $\rho(h)$ (esiste perchè siamo su $\C$): allora $\rho(g)-\lambda \Id\in \End_{\mathbb{S}^1}(V)$ e ha un nucleo $\Rightarrow$ per Schur il nucleo coincide con $V$ essendo $V$ irriducibile. Allora $\rho(h) \equiv \lambda(h) \Id$. Quello che abbiamo fatto vale per ogni $h \in G$, per cui tutti i $\rho(g) $ sono in realtà scalari. Questo ci dice che ogni base di $V$ è una base di autovettori per $\rho(g)\ \forall g\in G$ e quindi ogni autospazio è $G-$invariante. Dunque le rappresentazioni irriducibili di $G$ sono tutte di grado 1.
\end{proof}


\begin{prop} 
  Le rappresentazioni irriducibili di $\mathbb{S}^1$ sono tutte e sole le $\rho_n:\mathbb{S}^1\to\C^*$ definite, al variare di $n$ in $\Z$, nel seguente modo:
  \[ \rho_n(z) = z^n \]
\end{prop}


\begin{proof}
  Innanzitutto notiamo che le $\rho_n$ definite nell'enunciato sono effettivamente delle rappresentazioni irriducibili di $\mathbb{S}^1$, quindi bisogna solo verificare che non ce ne sono altre.

  Sappiamo che le rappresentazioni irriducibili dovranno avere dimensione $1$, per cui saranno omomorfismi $\rho:\mathbb{S}^1\to\C^*$. Inoltre dovrà per forza essere $|\rho(z)| = 1 \quad \forall z \in \mathbb{S}^1$. Questo si dimostra abbastanza facilmente. Supponiamo per assurdo infatti che si abbia
  \[ |\rho(x)| > 1\]
  per qualche $x$. Allora anche $\rho(x)^n$ farebbe parte di $\rho(G)$. Tuttavia qui c'è un assurdo in quanto $\rho(G)$ è compatto perché la rappresentazione è continua (compatti vanno in compatti), mentre $\rho(x)^n$ non è limitato. Se invece per assurdo fosse $|\rho(x)| < 1$ si potrebbe usare lo stesso argomento notando che $|\rho(x^{-1})| > 1$.
  Di conseguenza sappiamo che $\rho(\mathbb{S}^1) \subseteq \mathbb{S}^1$.
  Consideriamo ora la mappa
  \begin{align*} 
    \phi : (\R, +) &\to \mathbb{S}^1 \\
    x &\to  e^{ix}
  \end{align*}
  che è evidentemente continua ed è omomorfismo di gruppi (è un rivestimento).
  Sia $\hat\rho$ l'applicazione che fa commutare il seguente diagramma:
  \[\tridiag \R \phi {\mathbb{S}^1} \rho {\mathbb{S}^1} {\hat\rho}\]
  Essendo composizione di omomorfismi continui, anche $\hat\rho$ è omomorfismo continuo. Si può dimostrare (ma non lo facciamo in questa sede) che esiste un'applicazione continua $\theta:\R\to\R$ che fa commutare il diagramma:
  % ~ Si può dimostrare che questa mappa è l'unica che soddisfi le proprietà del rivestimento universale da $\R$ a $\mathbb{S}^1$. Ci è utile perché in qualche modo scarica il problema di trovare le rappresentazioni da e verso $\mathbb{S}^1$ a cercare delle rappresentazioni da e verso $(\R, +)$. Per l'unicità del rivestimento concluderemo che sono anche tutte. 
  \[
  \begin{diagram}
    \R          & \rTo^{ \phi }         & \mathbb{S}^1           \\
    \dTo<{\theta}   & \rdTo^{\hat \rho}     & \dTo>{\rho}            \\
    \R          &  \rTo_{\phi}          & \mathbb{S}^1           \\
  \end{diagram}
  \]
Ovvero che fattorizza $\hat\rho=\phi\theta$. A questo punto $\theta(0)$ deve essere un multiplo intero di $2\pi$, visto che deve risultare $e^{i\theta(0)} = \hat\rho(0) = 1$.
Quindi, a meno di traslare $\theta$ di un multiplo intero di $2\pi$ (il che non interferisce con la commutatività del diagramma), possiamo assumere $\theta(0) = 0$. Dati $x,y$ reali deve valere
\[ e^{i\theta(x+y)} = \hat \rho(x+y) = \hat \rho(x )\cdot \hat \rho(y) = e^{i\theta(x)+i \theta(y)}\]
per cui deve essere
\[ \theta(x+y) - \theta(x) - \theta(y) \in 2\pi \Z \qquad \forall x, y \in \R \]
Ma visto che $\theta$ è continua l'espressione sopra deve essere necessariamente costante al variare di $x$ e $y$. Ponendo $x=y=0$ ricaviamo che tale costante è $-\theta(0)=0$, dunque
\[ \theta(x + y) = \theta(x ) + \theta (y) \qquad \forall x,y\in \R\]
A questo punto si osserva (sfruttando la continuità di $\theta$) che deve essere $\theta(x) = \theta(1) x$, con $\alpha \in \R$. Non solo, possiamo dare delle informazioni a riguardo di $\theta(1)$. Infatti, dato che $\hat\rho$ è ``periodico'', deve essere
\[ 1 = \hat\rho(0) = \hat\rho(2\pi) = e^{i\theta(1) 2\pi}\]
per cui in realtà si ha $\theta(1) \in \Z$. 
Ricapitolando, abbiamo mostrato che per ogni $x\in\R$ vale che
\[e^{i \theta(1)x} = \rho(e^{ix}) \]
dove $\theta(1)$ è un intero fissato. Ovvero per ogni $z\in\mathbb{S}^1$ vale
\[ \rho_{\theta(1)}(z) = z^{\theta(1)} = \rho(z) \]
che è quello che volevamo mostrare.

\end{proof}

A questo punto il nostro obiettivo sarebbe quello di cercare la stessa cosa che abbiamo fatto per i gruppi finiti, ovvero scomporre una rappresentazione generica come somme di rappresentazioni irriducibili. Per farlo in sostanza abbiamo inventato quel prodotto hermitiano invariante che da solo ci ha permesso praticamente di fare tutto. Sarebbe molto bello avere una cosa simile anche per questi gruppi topologici. In particolare per i gruppi compatti possiamo andare a cercare di definire qualcosa di molto simile.


\begin{defn}[Integrazione su un gruppo]
Sia $G$ un gruppo topologico. Consideriamo un'applicazione $I: C_{\R}(G) \to \R$
dove con $C_\R$ si intendono le funzioni continue da $G$ in $\R$\footnote{Tutto funziona allo stesso modo se al posto di $\R$ mettiamo $\C$. In futuro non si baderà a questa distinzione.}. La mappa $I$ si dice integrazione se rispetta le seguenti proprietà.
\begin{itemize}
\item \`E lineare: $I(a f + b g) = aI(f) + bI(g)$.
\item Se la funzione $f$ è positiva ($ > 0$), allora anche l'integrale deve essere positivo ($> 0$).
\end{itemize}
\end{defn}


A questo punto, in analogia con quello che abbiamo fatto con i prodotti finiti, cerchiamo di trovare quello invariante. 

%\begin{defn} Sia $G$ un gruppo compatto e $\rho: G \to GL(V_\rho)$ una sua rappresentazione continua. Sia $I$ una %ntegrazione su $G$. Si dice che $I$ è invariante sotto $\rho$ se 
%\[ I(f(x)) = I(f(\rho(g) x)) \qquad \forall g \in G\]
%\end{defn}
\begin{defn} Sia $G$ un gruppo compatto e $I$ una integrazione su $G$. Si dice che $I$ è $G$-\textit{invariante} se $\forall f\in C_{\R}(G), \forall g\in G$ vale che
\[ I(f) = I(L_g(f)) = I(R_g(f))\]
dove $(L_g(f))(x) = f(g^{-1}x)$ e $(R_g(f))(x) = f(xg)$.
\end{defn}
Se questa integrazione esiste, allora in analogia a quanto fatto per i gruppi finiti possiamo sperare che le rappresentazioni di un gruppo compatto siano completamente irriducibili.


\begin{exmp}Alcuni esempi di integrazioni invarianti sono:
	\begin{enumerate}
		\item Nel caso dei gruppi finiti $I(f)=\frac{1}{|G|}\sum_{g\in G}f(g)$ è una integrazione invariante.
		\item Definiamone esplicitamente una su $\mathbb{S}^1$ 
		\[\dint_{\mathbb{S}^1}f(g)\ dg:=\frac{1}{2\pi}\dint_0^{2\pi}f(e^{i\theta})\ d\theta\]
		Le verifiche che sia una integrazione a tutti gli effetti sono banali: infatti che sia lineare e la positività derivano dalle proprietà dell'integrazione su $\R$ e l'invarianza deriva dal fatto che far agire $\mathbb{S}^1$ significa solo ruotare di un angolo ovvero riparametrizzare la circonferenza.
		Il diviso $2\pi$ è dovuto al fatto che vogliamo in generale avere una integrazione invariante normalizzata ovvero
		\[\dint_{\mathbb{S}^1}1\ dg=1\]
		Nel caso dei gruppi finiti dividevamo per la cardinalità del gruppo: ora che si parla di gruppi compatti possiamo immaginare che si divida per la "misura" del gruppo. 
	\end{enumerate}
\end{exmp}

\begin{lemma}
  Se esiste l'integrazione invariante su un gruppo, allora esiste una forma hermitiana definita positiva invariante sotto l'azione del gruppo.
\end{lemma}
\begin{proof}
  Sia $h$ una forma hermitiana definita positiva su $V_\rho$. Consideriamo la quantità
  \[ h_G(v,w) = I( h(\rho(g) v, \rho(g) w)) \]

  Evidentemente è di nuovo una forma hermitiana definita positiva ed è invariante sotto $G$.


\end{proof}




\begin{thm} Sia $G$ un gruppo compatto e $\rho : G \to GL(V_\rho)$ una sua rappresentazione continua, con $V_\rho$ di dimensione finita. Se esiste una integrazione invariante $I$, allora la rappresentazione è completamente riducibile.

\end{thm}

\begin{proof}
  La dimostrazione è identica alla dimostrazione \ref{thm:gruppo finito completamente riducibile}, usando la forma hermitiana invariante appena definita. 
\end{proof}


\begin{thm}[Teorema di Haar]
Per ogni gruppo compatto esiste l'integrazione invariante. 
\end{thm}

Di questo teorema non daremo la dimostrazione in questo corso. 

Definiamo ora in $\C(\mathbb{S}^1)$ un prodotto hermitiano come segue
\[(f_1,f_2):=\dint_{\mathbb{S}^1}f_1(g)\overline{f_2(g)}\ dg\]
\'E una buona definizione ed è $\mathbb{S}^1$ invariante.
\begin{rem}
\[(\rho_n,\rho_m)=\dint_{\mathbb{S}^1}\rho_n(g)\overline{\rho_m(g)}\ dg=\frac{1}{2\pi}\dint_0^{2\pi}e^{in\theta}e^{-im\theta}\ d\theta=\frac{1}{2\pi}\dint_0^{2\pi}e^{i\theta (n-m)}\ d\theta \]
\[\Rightarrow (\rho_n,\rho_m)=\left\{\begin{matrix}
1 & n=m\\ 
0 & n\neq m
\end{matrix}\right.\Rightarrow (\rho_n,\rho_m)=\left\{\begin{matrix}
1 & \rho_n\cong \rho_m\\ 
0 & \rho_n \ncong \rho_m
\end{matrix}\right.\]
ovvero $\{\rho_n\}_{n\in \N}$ formano un sistema ortonormale.
\end{rem}
\begin{cor} Sia $\rho: \mathbb{S}^1\rightarrow GL(V)$ una rappresentazione: allora
\[\rho\cong \sum_n a_n\rho_n\ \ \text{con}\ n\in \N\ \text{ed}\ a_n\in \Z\]
Più precisamente $a_n=(\rho,\rho_n)$.
\end{cor}

MANCA LA PARTE IN CUI PARLAVA DI FOURIER









\newpage
\subsection{Le rappresentazioni di $SU(2)$}



Dopo aver studiato in dettaglio $\mathbb{S}^1$, vediamo come si comporta un suo parente stretto, $SU(2)$. Ricordiamo che $SU(2)$ è definito come un sottoinsieme di $GL(\C^2)$. In particolare ogni matrice di $SU(2)$ si può scrivere come 

 \[ SU(2) = \left\{  \left(\begin{array}{cc} z_1 & z_2 \\ -\overline{z_2} & \overline z_1 \end{array}\right) \quad z_1, z_2 \in \C  , \qquad |z_1|^2 + |z_2|^2 = 1\right\} \]

 È abbastanza naturale considerare $\C^2 \cong \R^4$ e accorgersi che in sostanza $SU(2) \cong \mathbb{S}^3$, in quanto tutti gli elementi rispettano l'equazione

 \[ x_1 ^2 + x_2^2 + x_3^2 + x_4^2 = 1 \]
E scrivendo in questo modo ci accorgiamo subito che quindi $SU(2)$ è un gruppo compatto e connesso.
\begin{cor} 
Ogni rappresentazione di $SU(2)$ è scomponibile in una somma di rappresentazioni irriducibili.
\end{cor}
Nella sezione \ref{sec:quaternioni} abbiamo visto come i quaternioni possono essere visti come un sottoinsieme di $M_2(\C)$
\[
\mathbb{H}= \left\{\begin{pmatrix}
 z_1& z_2\\ 
 -\overline{z_2}& \overline{z_1} 
\end{pmatrix}\in M_2(\C) \right\}
\]
Quindi $SU(2)$ può essere immerso in $\HH$ avendo le matrici la stessa struttura.\\
 MANCA LA PARTE SU $\rho_H$ MA NON SO QUANTO SERVA.


 
Come abbiamo fatto per $\mathbb{S}^1$, vorremmo andare a studiare tutte le sue rappresentazioni irriducibili. Per farlo, innanzitutto sarebbe intelligente farsi un'idea di come può essere fatta l'integrazione invariante su questo gruppo. Se lo immergiamo in $\R^4$, possiamo sfruttare l'integrazione lì definita e quindi fare un integrale triplo

 \[ I(f) = \dint_{\mathbb{S}^3} f(g) dg = \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi\]


L'invarianza dell'integrazione sotto l'azione di $G$ deriva sostanzialmente dal fatto che quando si cambia variabili bisogna moltiplicare l'integrando per il determinante dello Jacobiano. Tuttavia $det(SU(2)) = 1$ e quindi non ci sono problemi.

\paragraph{Rappresentazioni irriducibili}
Vediamo ora di classificare tutte le rappresentazioni irriducibili di $SU(2)$. 
Consideriamo l'azione di $SU(2)$ sullo spazio $V_m = \C[x,y]_m$ dei polinomi omogenei su $\C$ di grado $m$ definita in questo modo 

 \[
 \rho_m(A) f(x, y) = f(a_{11} x + a_{21} y , a_{12} x + a_{22} y) \qquad \forall f \in V_m, \forall A \in SU(2)
 \]

\begin{prop} $\rho_m$ è una rappresentazione irriducibile di $SU(2)$ $\forall m\in \N$.
\label{prop:irrid su2}
\end{prop}
\begin{proof} Cerchiamo di trovare un sottogruppo di $SU(2)$ di cui conosciamo già le rappresentazioni irriducibili e studiamo le $\rho_n$ ristrette a tale sottogruppo. Consideriamo il seguente insieme delle matrici speciali unitarie diagonali
\[T=
\left\{\begin{pmatrix}
z & 0\\ 
 0& \overline{z}
\end{pmatrix}\in M_2(\C):\ |z|^2=1 \right\}\subseteq SU(2)\Rightarrow T= \left\{\begin{pmatrix}
e^{i\theta}& 0\\ 
 0& e^{-i\theta}
\end{pmatrix}: 0\leq \theta \leq 2\pi \right\}\]

\'E evidente che $T$ sia isomorfo a $\mathbb{S}^1$. Dunque $\rho_m|_{\mathbb{S}^1}$ è una rappresentazine di $\mathbb{S}^1$ su $V_m$. Come agiscono le matrici in $T$? Per vederlo, consideriamo la base "canonica" di $V_m$, formata dai monomi $\{ x^ky^{m-k} \}_{k \in \{0, ..., m\}}$ e scriviamo
\[V_m=\bigoplus_{k=0}^m \C (x^k y^{m-k})\]
Dato un $\theta\in [0,2\pi]$ allora
\[\rho_m\begin{pmatrix}
e^{i\theta} & 0\\ 
 0& e^{i\theta}
\end{pmatrix} (x^ky^{m-k}) = (xe^{i\theta})^k(ye^{i\theta})^{m-k}=e^{i(m-2k)} x^k y^{m-k} \]

Ovvero, quello che abbiamo appena scoperto è che la base considerata è formata da autovettori per il sottogruppo $T$. Quindi, denotando con $\rho_n^{\mathbb{S}^1}$ le rappresentazioni irriducibili di $\mathbb{S}^1$, allora $\rho_m|_{\mathbb{S}^1}=\sum_{k=0}^{m}\rho_{m-2k}^{\mathbb{S}^1}$.

Quindi ora di $V_m$ abbiamo una duplice scrittura:
\[V_m=\bigoplus_{k=0}^m \C (x^k y^{m-k})=\bigoplus_{k=0}^m V_{\rho_{m-2k}^{\mathbb{S}^1}}\]
dove la seconda scrittura è una somma diretta di rappresentazioni di $T$ irriducibili e a 2 a 2 non isomorfe. Consideriamo $W\neq \emptyset \subseteq V_m$ sottospazio stabile per $SU(2)$: iniziamo con il dimostrare il seguente fatto:

\textbf{Fatto:} Sia $\pi_k:V_m\rightarrow \C [x^{m-k}y^k]$ la proiezione di un vettore sulla sua componente $(m-k)-$esima. Allora se $\pi_k|_W\neq 0$ allora $V_{m,k}:=\C[ x^{m-k}y^k]\subseteq W$.

Dato che  $\{ x^ky^{m-k} \}_{k \in \{0, ..., m\}}$ sono una base di autovettori per le matrici in $T$ allora $\forall k,\ \pi_k\in \Hom_T(V_m,V_{m,k} )\Rightarrow \pi_k|_W\in \Hom_T(W,V_{m,k})$. Se $\pi_k|_W\neq 0\Rightarrow W=\Ker(\pi_k|_W)\bigoplus W_k$ dove la $\dim(W_k)$ è forzata ad essere 1. Quindi $W_k\cong V_{m,k}$ come rappresentazioni. Tuttavia $V_{m,k}$ è l'unico sottospazio invariante per $\rho_{m-2k}^{\mathbb{S}^1}\Rightarrow V_{m,k}=W_k$.


Quindi dato che abbiamo supposto il nostro $W$ diverso dal vuoto, esiste un $k$ tale che $x^{m-k}y^k\in W$. Se mostriamo che $x^m\in W$ allora abbiamo concluso poichè data la generalità di $W$ avrei dimostrato che $x^m\in$ a tutti i sottospazi di $V_m$ non vuoti e invarianti per $SU(2)$. Quindi se $W$ fosse diverso da $V_m$ avrei $V_m=W\bigoplus U$ ma con $U\neq \emptyset \Rightarrow x^m\in U\cap W$ il che è assurdo poichè devono essere in somma diretta. Per mostrare che $x^m\in W$ riusiamo il fatto e ci riduciamo a dover dimostrare che $\pi_0|_W\neq 0$. Considero al variare di $g\in SU(2)$ il vettore $\pi_0(\rho_m(g)(x^{m-k}y^k))$ dove $x^{m-k}y^k$ è il monomio che siamo sicuri appartenga a $W$. Bene, ma allora mi basta scegliere
\[g=\frac{1}{\sqrt{2}}\begin{pmatrix}
1& -1\\ 
 1& 1
\end{pmatrix}\Rightarrow \rho_m(g)(x^{m-k}y^k)=x^m\]
Quindi $W=V_m$. Le $\rho_m$ sono irriducibili.
\end{proof}




 \paragraph{Le classi di coniugio di $SU(2)$}
 Dato che abbiamo dato tanto peso alla teoria del carattere per i gruppi finiti, probabilmente anche per i gruppi compatti ci saranno applicazioni interessanti. Dato che il carattere è una funzione di classe, è intelligente andare a studiare nel dettaglio le classi di coniugio di $SU(2)$.


 Abbiamo già visto che possiamo immergere $\mathbb{S}^1$ in $SU(2)$. Vediamo come questo abbia a che fare con le classi di coniugio.


 Ricordiamo che possiamo scrivere un generico punto di $\mathbb{S}^3$ come

 \[
 \begin{cases}
 x_1 = \cos\theta \\
 x_2 = \sin\theta\cos\phi \\
 x_3 = \sin\theta\sin\phi\cos\psi \\
 x_4 = \sin\theta\sin\phi\sin\psi \\
 \end{cases}
 \]

 Per cui possiamo scrivere il generico elemento di $SU(2)$ come

 \[
 g(\theta, \phi, \psi) = 
 \left(
 \begin{array}{cc}
   \cos\theta + i \sin\theta \cos\phi & \sin\theta\sin\phi( \cos\psi + i \sin\psi) \\
   -\sin\theta\sin\phi(\cos\psi - i \sin\psi) & \cos\theta - i \sin\theta\cos\phi  \\

 \end{array}
 \right)
 \]
 

 Dato che le matrici di $SU(2)$ sono normali, possiamo usare il teorema spettrale normale per dire che sono tutte unitariamente diagonalizzabili, ovvero $\forall g \in SU(2) \exists h \in U(2) \  | \ hgh^{-1} = m$, con $m$ diagonale. Tuttavia possiamo restringerci ad avere anche $h$ in $SU(2)$ scegliendo $h' = \frac{1}{\sqrt{det(h)}} h$. Per questo motivo le classi di coniugio saranno tutte matrici di Jordan diagonali, senza 1 sulla sopradiagonale. Per questo motivo la classe di coniugio sarà univocamente determinata dagli autovalori della matrice. Il polinomio caratteristico di una matrice $2\times 2$ è

 \[ p(t) = t^2 - \tr(M) t + det(M) \]

 Ma dato che siamo in $SU(2)$ il determinante è 1. La traccia è invece con una banale somma $2\cos\theta$. I due autovalori hanno modulo 1 e quindi in sostanza sono $e^{i\theta}$ e $e^{-i\theta}$. Per questo motivo un generico elemento di $SU(2)$ è


 \[
 g(\theta, \phi, \psi) \sim \left(
 \begin{array}{cc}
   e^{i\theta} & 0 \\
   0 & e^{-i\theta} \\
 \end{array}
 \right) = g(\theta, 0 , 0)
 \]




 Per questo motivo la funzione $\frac{1}{2} \tr(g) : SU(2) \to \R$ è una funzione suriettiva in $[-1,1]$ e le sue fibre sono le classi di coniugio.

 \paragraph{Il carattere delle rappresentazioni irriducibili di $SU(2)$}


 Prendiamo una funzione $f$

 \[ f \in \C(SU(2)) ^\# = \{ \text{Funzioni di classe } f: SU(2) \to \C  \} \]

 Sappiamo che se questa funzione è di classe, allora non dipende dal rappresentante della classe di coniugio. Per questo motivo possiamo quindi dire che

 \[ f(g(\theta, \phi, \psi)) = f(g(\theta, 0 , 0)) \]
 Vorremmo andare a studiare come si comporta l'integrazione invariante su questa funzione e poi considerare il caso particolare in cui questa funzione sia per l'appunto il carattere di $\rho$, una generica rappresentazione di $SU(2)$.

 \begin{align*}
 \dint_{\mathbb{S}^3} f(g) dg &= \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &=\dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, 0, 0) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &= \dfrac{2}{\pi} \dint_0^\pi f(\theta, 0, 0) \sin^2\theta \ d\theta = \\
  &= \dfrac{1}{\pi} \dint_0^{2\pi} f(\theta, 0, 0) \sin^2\theta \ d\theta 
 \end{align*}
 
 Quindi l'espressione si semplifica parecchio. Prendiamo ora per l'appunto una rappresentazione di $\rho$ e vediamo come si comporta il carattere. In particolare prendiamo una delle $\rho_m$ irriducibili che abbiamo trovato prima. Sappiamo che

 \[ \rho_m: SU(2) \to GL(V_m) \qquad V_m = \bigoplus_{k=0}^m V_{m,k}\]
 
 In particolare siamo già riusciti a diagonalizzare $\rho_m$. Avremo che

 \begin{align*}
   \chi_{\rho_m} (\theta, \phi, \psi) &= \chi_{\rho_m}(\theta) = \chi_{\rho_m}(e^{i\theta}) = \\
   &= \dsum_{k=0}^m \rho_{m-2k}^{\mathbb{S}^1}(e^{i\theta}) = \dsum_{k=0}^m e^{i(m-2k)} \\
   &= \dfrac{e^{i(m+i)\theta} - e^{-i(m+1)\theta}}{e^{i\theta} - e^{-i\theta}} = \dfrac{\sin((m+1)\theta)}{\sin\theta}
 \end{align*} 
 


 Per cui, in analogia a quanto fatto per i gruppi finiti, diamo la definizione di

 \begin{defn}[Prodotto hermitiano di caratteri]

   \[
   \langle \chi_\rho | \chi_\sigma \rangle = \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg
   \]

 \end{defn}

 Che andiamo prontamente a calcolare in questo caso

 \[
 \langle \chi_{\rho_n} | \chi_{\rho_m} \rangle = \dfrac{1}{\pi}\dint_0^{2\pi} \dfrac{\sin((n+1)\theta)}{\sin\theta} \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta \ d\theta = \delta_{mn}
 \]


 Cosa molto simile a quanto era già successo per i gruppi finiti. A questo punto viene il sospetto che possa esserci qualcosa di più profondo sotto. Vedremo fra poco dei teoremi che generalizzano questi risultati. 








 \begin{thm}
   \label{thm:ortogonalita compatto}
   Sia $G$ un gruppo compatto  e siano $\rho$ e $\sigma$ due sue rappresentazioni irriducibili. Allora vale
   \[
   \langle \chi_\rho | \chi_\sigma \rangle =
   \begin{cases}
     1 \qquad \text{se } \sigma \cong \rho \\
     0 \qquad \text{altrimenti} \\
   \end{cases}
   \]
 \end{thm}

 Per dimostrare il teorema appena enunciato ci servono un paio di lemmi:

 \begin{lemma}
   Siano $\rho: G \to GL(V)$ e $\sigma: G \to GL(W)$ due rappresentazioni\footnote{Non necessariamente irriducibili} di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$ un omomorfismo di spazi vettoriali\footnote{Per gli amici, un'applicazione lineare.}. Allora possiamo definire un $\phi^G \in \Hom_G(V, W) $ nel seguente modo

   \[ \langle \phi^G(v) | w \rangle = \dint_G \langle \sigma(g) \phi (\rho(g) v) | w\rangle dg\]

   Dove $\langle \cdot | \cdot \rangle : W \times W \to \C$ è una forma hermitiana definita positiva.
   
 \end{lemma}

 \begin{proof}
   Dobbiamo in sostanza dimostrare che l'omomorfismo che abbiamo definito è anche un omomorfismo di rappresentazioni, ovvero che è $G-$equivariante, in quanto la linearità è assicurata dal prodotto hermitiano. In formule, dobbiamo mostrare che
   \[ \phi^G (\rho(g) v) = \sigma(g) \phi^G(v) \qquad \forall g \in G, \forall v \in V \Leftrightarrow \phi^G(v) = \sigma(g) ^{-1} \phi^G(\rho(g) v) \qquad \forall g \in G, \forall v \in V \]

   In sostanza la tesi segue dalla definizione sfruttando il fatto che l'integrazione su $G$ sia invariante


   \[ \langle \sigma(g) ^{-1} \phi^G(\rho(g) v) | w \rangle  = \dint_G \langle \sigma(hg) ^{-1} \phi^G(\rho(hg) v) | w \rangle dh = \langle \phi(v) | w \rangle\]

   Dove ho saltato un paio di passaggi ma il succo è che essendo l'integrazione invariante per traslazioni di $G$, è evidente che l'oggetto definito in questo modo diventa invariante sotto $G$
 \end{proof}

 \begin{lemma}
   Siano $\rho: G \to V$ e $\sigma: G \to W$ rappresentazioni irriducibili di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$. Allora

   \[
   \begin{cases}
     \phi^G = 0 \qquad \text{se } \rho \ncong \sigma \\
     \phi^G = \frac{\tr \phi}{\dim V}\Id_V \qquad \text{altrimenti} \\
   \end{cases}
   \]

 \end{lemma}

 \begin{proof}
   Nel caso in cui le due rappresentazioni non siano isomorfe, la tesi segue banalmente dal lemma di Schur. Nell'altro caso, possiamo di nuovo usare Schur e dire che $V \cong W$ come spazi vettoriali, ma anche che in un certo senso sono lo stesso spazio, in quanto $\phi = \lambda \Id$, sempre per Schur. L'unica cosa che rimane da mostrare è che vale proprio $\tr \phi^G = \tr \phi$. Per mostrarlo, possiamo fissare una base ortonormale di $V$, $\{ e_n\}_{n\in I}$ e notare che in questa base

   \[ \tr(\psi) = \dsum_{n \in I}\langle \psi e_n | e_n \rangle \qquad \forall \psi \in \End(V)\]
   Per cui,

   \begin{align*} \tr \phi^G &= \dsum_{n \in I}\langle \phi^G e_n | e_n \rangle = \dsum_{n \in I} \dint_G \langle \rho(g)^{-1} \phi(\rho(g) e_n) | e_n \rangle dg = \\
     &= \dsum_{n \in I}\dint_G \langle \phi(\rho(g) e_n) | \rho(g) e_n \rangle  dg = \dsum_{n \in I} \dint_G \langle \phi e_n |  e_n \rangle dg = \\
     &= \dsum_{n\in I} \langle \phi e_n | e_n \rangle = \tr \phi
   \end{align*}

   
 \end{proof}


 \begin{proof}[Dimostrazione del teorema \ref{thm:ortogonalita compatto}]
 Fissiamo una base ortonormale di $V$, $\{v_n\}$, e una base ortonormale di $W$, $\{w_n\}$. In base a quanto detto poco fa, avremo che
 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle &= \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg = \dint_G \tr(\rho(g)) \tr(\sigma(g)^{-1}) dg = \\
   &= \dint_G \dsum_{i \in I} \langle \rho(g) v_i | v_i \rangle \dsum_{j \in J} \langle \sigma(g)^{-1} w_j | w_j \rangle dg = \\
   &= \dsum_{i,j} \dint_G \left\langle \rho(g) \langle \sigma(g)^{-1} w_j | w_j \rangle v_i | v_i \right\rangle dg 
 \end{align*} 

 Se definiamo
 \[ \phi_{ij} \in \Hom(W, V), \quad \phi(w) = \langle w | w_j \rangle v_i \]

 e con quello che abbiamo fatto nel lemma poco sopra definiamo anche $\phi^G_{ij}$, possiamo notare che in effetti l'espressione poco sopra è proprio $\phi^G_{ij}$, ovvero

 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | w_j \rangle
 \end{align*}

 Se $\rho$ e $\sigma$ non sono isomorfe, allora si ha identicamente $\phi_{ij}^G = 0$ per ogni $i$ e $j$. Di conseguenza $\langle \chi_\rho | \chi_\sigma \rangle = 0$. Altrimenti, al solito posso identificare $V \cong W$ e possiamo usare l'altra parte del lemma precedente per dire che

\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | v_j \rangle = \dsum_{i,j} \dfrac{\tr \phi_{ij}}{\dim V} \langle v_i | v_j \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{\dim V} \delta_{ij} 
 \end{align*}

Tuttavia in sostanza la matriche che rappresenta $\phi_{ij}$ nelle base $v_i$ è evidentemente una matrice che ha tutti zeri tranne uno che è uno, esattamente nella posizione $(i,j)$, per cui


\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{\dim V} \delta_{ij} = \dsum_{i=j} \dfrac{1}{\dim V} = 1
 \end{align*}

 \end{proof}

 \begin{cor}
   Sia $G$ un gruppo compatto e $\rho$ una sua rappresentazione. Allora $\rho$ è irriducibile se e solo se $\langle \chi_\rho | \chi_\rho \rangle = 1$
 \end{cor}

 
 A questo punto possiamo concludere la dimostrazione del fatto che le rappresentazioni che abbiamo trovato prima di $SU(2)$ sono tutte e sole quelle irriducibili.

 
 \begin{thm}
   Sia $\rho$ una rappresentazione irriducibile di $SU(2)$. Allora $\exists m \in \N$ tale che $\rho \cong \rho_m$. Chiaramente le $\rho_m$ sono quelle definite nella proposizione \ref{prop:irrid su2}
 \end{thm}

 \begin{proof}
   Supponiamo per assurdo che sia $\langle \chi_\rho | \chi_{\rho_m} \rangle = 0 \quad \forall m \in \N$. Vogliamo mostrare che deve essere $\rho = 0$. 


   \begin{align*}
     0 &= \langle \chi_\rho | \chi_{\rho_m} \rangle = \dint_G \chi_\rho(g) \overline{\chi_{\rho_m} (g)} dg = \\
     &= \dfrac{1}{\pi} \dint_0^{2\pi} \chi_\rho(\theta) \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta  \ d\theta \\
     &= \dfrac{1}{\pi} \dint_{0}^{2\pi} \left(\chi_\rho (\theta) \sin\theta \right) \sin(  (m+1) \theta) \ d\theta
   \end{align*}

   A questo punto possiamo ricordare che in realtà il carattere è una funzione di classe e in particolare si ha $\chi_\rho(\theta) = \chi_\rho(-\theta)$. La funzione $f(\theta) = \chi_\rho(\theta) \sin\theta \in C_\R[0,2\pi]$ è quindi dispari. Inoltre, è evidentemente anche $\mathbb{L}^2 [0,2\pi]$. Ma noi sappiamo dall'analisi che $\{\sin(nx)\}_{n\in \N}$ è una base delle funzioni dispari di $\mathbb{L}^2[0,2\pi]$. Di conseguenza si ha $f(\theta) = 0 \quad q.o.$, ma dato che è continua, $f(\theta) = 0$ identicamente. Per lo stesso motivo, si deve avere $\chi_\rho(\theta) = 0$
   
 \end{proof}


 \newpage
 \subsubsection{Le rappresentazioni irriducibili di $SO(3)$}

 Abbiamo studiato in dettaglio due gruppi importanti, $\mathbb{S}^1$ e $SU(2)$. Sarebbe bello a questo punto sfruttare quello che abbiamo fatto per trovare le rappresentazioni di $SO(3)$, che ha notevole rilevanza fisica. In particolare, se riuscissimo a trovare un omomorfismo continuo $\phi$ da $SO(3)$ a $SU(2)$ potremmo notare sul seguente diagramma

 \[
 \begin{diagram}
   SU(2)       & \rTo^{\rho_m}        & GL(V_m) \\
   \uTo<{\phi} & \ruTo>{\hat{\rho}_m} & \\
   SO(3)       &                      & \\
 \end{diagram}
 \]

 \noindent che otteniamo gratis un sacco di rappresentazioni continue anche per $SO(3)$. Andiamo quindi a costruire questo omomorfismo.

 \begin{defn}
   Definiamo lo spazio vettoriale su $\R$, $\mathbb{E}$ in questo modo

   \[ \mathbb{E } = \{ \text{Le matrici autoaggiunte a traccia nulla }\} = \left\{
   \left(
   \begin{array}{cc}
     x_1 & x_2 + i x_3 \\
     x_2 - i x_3 & -x_1 \\
   \end{array}
   \right)
   \  \  x_i \in \R
   \right\}\]

 \end{defn}

 Ci piacerebbe far agire in modo sensato $SU(2)$ su $\mathbb{E}$, sperando di vedere la struttura di $SO(3)$. Il primo tentativo può essere quello di definire

 \[ \phi(g) x = gx \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 Tuttavia questa azione non funziona in quanto non è per niente detto che la matrice in arrivo abbia di nuovo traccia nulla. Per mantenere la traccia invariata, la cosa più sensata da fare è agire per coniugio

 \[ \phi(g) x = gxg^{-1} \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 In questo caso è evidente che la matrice in arrivo ha traccia nulla. Controlliamo che sia anche autoaggiunta

 \[ \left( \phi(g) x \right)^\dag = \left( g x g^{-1}\right)^\dag = (g^{-1})^\dag x^\dag g ^\dag = g x g^{-1} = \phi(g) x\]

 In quanto $x$ è autoaggiunta e vale $g^{-1} = g^\dag \forall g \in SU(2)$. A questo punto se riuscissimo a trovare un prodotto scalare invariante sotto l'azione di $SU(2)$ potremmo dire di aver in qualche modo mappato $SU(2)$ in $O(3)$ e ci staremmo avvicinando all'obiettivo. In particolare notiamo che la funzione
 \[ f(x) = -det (x) = x_1 ^2 + x_2^2 + x_3^2 \qquad \forall x \in \mathbb{E}\]
 è in effetti una forma quadratica definita positiva su $\mathbb{E} \cong \R^3$ ed è anche invariante sotto $SU(2)$ in sostanza per la formula di Binet. Abbiamo quindi definito $\phi: SU(2) \to O(3)$ che è in sostanza un omomorfismo continuo di gruppi compatti. Inoltre, dato che $SU(2)$ è connesso, anche $\phi(SU(2))$ sarà connesso e conterrà l'identità. Per questo motivo possiamo quindi dire che

 \[ \phi(SU(2)) \subseteq SO(3) \] 

 A questo punto, è decisamente il caso di capire meglio come $\phi$ mappi un gruppo nell'altro in modo da sfruttare quello che sappiamo per uno anche per l'altro.



 \begin{prop}
   \label{prop:omo su2 so3}
  \begin{enumerate}
    \item L'omomorfismo $\phi : SU(2) \to SO(3)$ sopra definito è surgettivo. 
    \item $\ker \phi = \{\Id, -\Id\}\subseteq SU(2)$
    \item $SU(2) / \Ker(\phi) \cong SO(3)$ come gruppi.
  \end{enumerate}
  \end{prop}

Per dimostrare questa proposizione abbiamo bisogno di un lemma.


\begin{lemma}
  Sia $H \subseteq SO(3)$ un sottogruppo. Supponiamo che valgano le due seguenti affermazioni:

  \begin{enumerate}
  \item $H$ agisce transitivamente su $\mathbb{S}^2$
    \item Esiste un asse $\langle e \rangle \subseteq \R^3$ tale che $H$ contiene tutte le rotazioni intorno a quell'asse.
  \end{enumerate}

  Allora si ha $H = SO(3)$
\end{lemma}

\begin{proof}
  Consideriamo $|e| = 1$. Dato che $H$ agisce transitivamente su $\mathbb{S}^2$, $\forall g \in SO(3)\ \exists h \in H$ tale che
  \[ ge = he \Rightarrow e = h^{-1}ge \Rightarrow h^{-1}g \in \Stab(e) \]

  Ma lo stabilizzatore di $e$ è una rotazione intorno a quell'asse, quindi è in $H$, dunque $h^{-1}g\in H\ \forall g\in SO(3)$, ma allora $\forall g\in SO(3)$ abbiamo che $g\in H$
\end{proof}
















\begin{proof}[Dimostrazione della proposizione \ref{prop:omo su2 so3}] Dimostriamo ora i tre punti della proposizione applicando il lemma appena dimostrato per il primo punto:
   \begin{enumerate}
   \item{
     Dimostriamo che l'azione di $\Phi(SU(2))$ su $\mathbb{S}^2$ è transitiva. Prendiamo una matrice $x \in \mathbb{E} \cap \mathbb{S}^2$, essendo una matrice autoaggiunta, per il teorema spettrale è unitariamente diagonalizzabile, ovvero $\exists g \in U(2) | gxg^{-1}$ è una matrice diagonale. In particolare, dato che $|\det(g)| = 1$, possiamo considerare $\displaystyle\hat g = \frac{g}{\sqrt{\det(g)}}$ per dire che possiamo scegliere una matrice $\hat g \in SU(2)$. Nella sua forma diagonale la matrice sarà
     \[ \hat g x \hat g^{-1} =
     \left(
     \begin{array}{cc}
       \lambda & 0 \\
       0 & -\lambda \\
     \end{array}
     \right)
     \qquad \lambda \in \R
     \]
     e dovrà essere $-\det(m) = \lambda^2 = 1$, per cui $\lambda = \pm 1$. Dato che in $SU(2)$ c'è una matrice che scambia $\lambda$ con $-\lambda$, possiamo prendere WLOG $\lambda > 0$
     \[ \hat g x \hat g^{-1} =
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \]

     E questo vuol dire che tutte le matrici di $\mathbb{E}$ sono coniugate alla stessa. Dato che l'azione di $SU(2)$ è per coniugio, questo vuol dire che c'è un'orbita sola, ovvero l'azione è transitiva.

     Mostriamo ora che $\Phi(SU(2))$ contiene tutte le rotazioni intorno ad un asse. Questo è semplice in quanto sappiamo che $\mathbb{S}^1 \subset SU(2)$. Vediamo come agisce
     \[
     \left(
     \begin{array}{cc}
       e^{i\theta} & 0 \\
       0 & e^{-i\theta} \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       x_1 & x_2 + i x_3 \\
       x_2 - ix_3 & -x_1 \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       e^{-i\theta} & 0 \\
       0 & e^{i\theta} \\
     \end{array}
     \right)
     =
     \left(
     \begin{array}{cc}
       x_1 & e^{2i\theta}(x_2 + i x_3) \\
       e^{-2i\theta}(x_2 - ix_3) & -x_1 \\
     \end{array}
     \right)
     \]
     Che è palesemente una rotazione intorno all'asse $x_1$ di angolo $2\theta$.
   }
   \item{
     Bisogna mostrare che $\Ker \Phi = \{\Id, -\Id\}$. Se vale $\Phi(g) = \Id$, allora vale che 

     \begin{align*}
     \left(
     \begin{array}{cc}
       z_1 & z_2 \\
       -\overline{z_2} & \overline{z_1} \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       \overline{z_1} & -z_2 \\
       \overline{z_2} & z_1 \\
     \end{array}
     \right)
     &=
      \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right) \\
     \Rightarrow
      \left(
     \begin{array}{cc}
       |z_1|^2-|z_2|^2 & -2z_1z_2 \\
       -2 \overline{z_1z_2} & |z_2|^2 - |z_1|^2 \\
     \end{array}
     \right)
     &=
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \end{align*}
     e questo è vero solo per $z_2 = 0$ e $|z_1| = 1$. Se andiamo a considerare il caso studiato al punto precedente, allora è evidente che deve essere solo $z_1 = \pm 1$.


   }
     \item La tesi segue dalle proposizioni precedenti applicate insieme al primo teorema di omomorfismo.
   \end{enumerate}
	\end{proof}



   A questo punto abbiamo mostrato un isomorfismo di gruppi. Ci piacerebbe che il nostro isomorfismo fosse anche continuo, in modo da poter concludere quali siano tutte e sole le rappresentazioni irriducibili di $SO(3)$. Per farlo in qualche modo dobbiamo definire una topologia sul gruppo quoziente in modo astratto e verificare che corrisponda a quella che già conoscevamo.

   \begin{defn}
     Sia $G$ un gruppo topologico, $H$ un suo sottogruppo normale. Definiamo una topologia nel seguente modo su $G/H$: un sottoinsieme $U \subseteq G/H$ è aperto $\Leftrightarrow \pi^{-1}(U)$ è aperto in $G$. Sono banali verifiche che questa sia effettivamente una topologia su $G/H$. Inoltre questa definizione ci assicura che la mappa $\pi$ sia continua.
   \end{defn}

   \begin{rem}
     A questo punto possiamo affemare che $SU(2) / \Ker\Phi \cong SO(3)$ come gruppi topologici e non solo come gruppi.
   \end{rem}



   \begin{cor}
     Una rappresentazione irriducibile $\rho_m$ di $SU(2)$ si può proiettare su $SO(3)$ se e solo se $- \Id \in \Ker \rho_m$.
   \end{cor}


   \begin{thm}
     Le rappresentazioni irriducibili di $SO(3)$ sono tutte e sole quelle che si ottengono da $SU(2)$ che hanno $-\Id$ nel nucleo, ovvero le $\rho_m$ per $m$ pari.
   \end{thm}


   \begin{thm}[Formula di Clebsch-Gordon]
     Consideriamo il prodotto tensore di due rappresentazioni irriducibili di $SU(2)$. Vale la formula
     \[ \rho_n\otimes \rho_m = \dsum_{j=0}^m \rho_{m+n-2j} \qquad (n \geq m)\]
   \end{thm}

   \begin{proof}
     Si tratta di un banale conto con i caratteri.

     \begin{align*}
       \chi_{\rho_n\otimes\rho_m} &= \chi_{\rho_n} \chi_{\rho_m} = \dfrac{e^{i(n+1)\theta} - e^{-i(n+1)\theta}}{e^{i\theta} - e^{-i\theta}} \cdot \dsum_{j=0}^m e^{i(m-2j)\theta} =\\ 
                                  &= \cdots = \dsum_{j = 0}^m e^{i(n+m-2j)\theta}
     \end{align*}
   \end{proof}


   
   
\newpage
\section{Rappresentazioni di algebre}
\subsection{Prime definizioni e esempi}
	\begin{defn}
		Un insieme $A$ si dice \textit{algebra} sul campo $\K$ (o più semplicemente $\K$\textit{-algebra}) se
		\begin{itemize}
			\item $A$ è uno spazio vettoriale su $\K$
			\item il prodotto $\cdot:A\times A\to A$ è una funzione bilineare
		\end{itemize}
		Un'algebra si dice
		\begin{itemize}
			\item \textit{associativa} se il prodotto è associativo
			\item \textit{unitaria} se $\exists 1_A\in A$ l'elemento neutro per il prodotto
			\item \textit{commutativa} se il prodotto è commutativo
		\end{itemize}
	\end{defn}
	\begin{exmp} Alcuni esempi di algebre sono:
		\begin{enumerate}
			\item Algebre di divisione $/\R$ (che abbiamo già incontrato)
			\item $\K$ un campo
			\item Se $V$ è uno spazio vettoriale su $\K$ allora $\End(V)$ è una $\K$-algebra
			\item $\K[x_1,\ldots,x_n]$ è una $\K$-algebra
			\item $\K\langle x_1,\ldots,x_n\rangle$ è l'algebra libera con $n$ generatori, sono le parole nell'alfabeto $\{x_1,\ldots,x_n\}$, il prodotto è la concatenazione; è generata da $x_{i_1}\cdots x_{i_k}$ (simile come costruzione al gruppo libero su $n$ generatori)
			\item Se $A$ è una $\K$-algebra, $I$ un suo ideale bilatero, allora $A/I$ è una $\K$-algebra
		\end{enumerate}
	\end{exmp}
	\begin{exmp}
		QUI ANDREBBE L'ALGEBRA UNIVERSALE INVILUPPANTE MA NON SO QUANTO SERVA...
	\end{exmp}
	Vorremmo ritrovare la teoria delle rappresentazioni che abbiamo fatto per i gruppi anche nella teoria che faremo sulle algebre, in quest'ottica definiamo il seguente insieme
	\begin{defn}
		Sia $G$ un gruppo, definiamo l'insieme delle combinazioni lineari formali degli elementi di $G$ a coefficienti in $\K$
		\[
			\K[G] = \left\{ \sum a_g e_g, a_g\in \K \right\}
		\]
		dove abbiamo una corrispondenza biunivoca tra $\{ g\in G \}$ e $\{ e_g \}_{g\in G}$.
		Si osserva che $\K[G]$ è un $\K$-spazio vettoriale, definendo poi il prodotto sui vettori di base (ed estendendo per linearità) come $e_g\cdot e_h=e_{gh}$ si ottiene una $\K$-algebra. Essa è associativa perchè il prodotto in $G$ è associativo e unitaria poichè $e_1$ è l'unità di $\K[G]$.
	\end{defn}
	\begin{rem}
		L'azione che avevamo nella rappresentazione regolare diventa il prodotto in $\K[G]$.
	\end{rem}
	\begin{defn}
		Siano $A$ e $A'$ $\K$-algebre, sia $f:A\to A'$ una funzione, $f$ è un \textit{omomorfismo di $\K$-algebre} se
		\begin{itemize}
			\item $f:A\to A'$ è un omomorfismo di spazi vettoriali
			\item $f(ab)=f(a)f(b)$
			\item $f(1_A)=1_{A'}$
		\end{itemize}
	\end{defn}
	\begin{defn}
		Una \textit{rappresentazione di una $\K$-algebra} $A$ è un omomorfismo di $\K$-algebre
		\[ f:A\to \End(V)\]
		con $V$ un $\K$-spazio vettoriale, allora $V$ viene detto $A$\textit{-modulo}.
	\end{defn}
	\begin{defn}
		Siano $V$ e $W$ degli $A$-moduli, un omomorfismo di spazi vettoriali $f:V\to W$ si dice \textit{omomorfismo di rappresentazioni} se 
		\[\forall a\in A,\ \forall v\in V \quad f(av)=af(v)\]
	\end{defn}
	\begin{exercise}
		Se $f$ è un isomorfismo di $A$-moduli, allora $f^{-1}$ è un isomorfismo di $A$-moduli. 
	\end{exercise}
	\begin{exmp}
		Alcuni esempi di rappresentazioni sono
		\begin{enumerate}
			\item Se $A=\K$ allora le rappresentazioni di $A$ sono gli spazi vettoriali su $\K$ dove l'azione di $A$ è molto semplice: $\rho(a) = a\Id$.
			\item Sia $G$ un gruppo finito, $A=\K[G]$, allora le rappresentazioni di $A$ come $\K$-algebra sono tutte e sole le rappresentazioni di $G$ come gruppo (le verifiche sono abbastanza semplici, da un lato si restringe l'azione agli elementi di base, dall'altro si estende per linearità).
		\end{enumerate}
	\end{exmp}
	\begin{defn}
		Siano $\rho_1, \rho_2$ delle rappresentazioni di $A$, allora definiamo $\rho_1+\rho_2$ sulla somma diretta
		\[
			\rho_1+\rho_2:A\to \End(V_1\oplus V_2)
		\]
		allo stesso modo di come abbiamo definito la somma di rappresentazioni di gruppi (scrivendo le matrici a blocchi).
	\end{defn}

	\begin{defn}
		Sia $\rho:A\to \End(V)$ una rappresentazione, $\rho$ si dice \textit{irriducibile} se $V$ non ha sottomoduli (sottospazi invarianti per moltiplicazione per elementi di $A$) non banali, allora $V$ è detto $A$-modulo \textit{semplice}.
	\end{defn}
	\begin{defn}
		Sia $\rho:A\to V$ una rappresentazione, $\rho$ è \textit{indecomponibile} ($V$ è indecomponibile come $A$-modulo) se non è somma diretta di rappresentazioni non banali.
	\end{defn}
	\begin{exmp}
		Sia $A=\K[x]$ vista come $\K$-algebra, allora una rappresentazione è univocamente determinata dall'immagine di $x$, dunque per identificare una rappresentazione basterà sapere $V$ e $\rho(x)$. Se $\K = \overline{\K}$ si osserva che le rappresentazioni irriducibili sono quelle di grado $1$ e sono determinate dal parametro $\lambda\in \K$ tale che $\rho(x)=\lambda$. Si può vedere inoltre che le rappresentazioni indecomponibili sono solo quelle per cui $\rho(x)$ è simile a $J_{n,\lambda}$ dove $J_{n,\lambda}$ è il blocco di Jordan di dimensione $n$ e autovalore $\lambda$.\newline
		Da questo esempio si osserva che il concetto di irriducibilità è diverso da quello di indecomponibilità.
	\end{exmp}
	\begin{exercise}
		QUI CI SAREBBE UN ESERCIZIO CHE RIGUARDA LA MITICA ALGEBRA UNIVERSALE INVILUPPANTE...
	\end{exercise}
	\begin{lemma}[Schur]
		Sia $A$ una $\K$-algebra, $\phi:V\to W$ un omomorfismo di $A$-moduli con $\phi \neq 0$, allora
		\begin{enumerate}
			\item se $V$ è semplice allora $\phi$ è iniettivo
			\item se $W$ è semplice allora $\phi$ è surgettivo
		\end{enumerate}
		Dunque se $V$ e $W$ sono semplici allora $\phi$ è un isomorfismo.
	\end{lemma}
	\begin{proof}
		Identica a quella fatta per i gruppi, dove si usa che $\Ker \phi$ e $\Imm \phi$ sono sottomoduli rispettivamente di $V$ e $W$.
	\end{proof}
	\begin{cor}
		Se $\K$ è algebricamente chiuso, $\phi\in End_A(V)$, $V$ è un $A$-modulo semplice, allora $\exists \lambda \in \K^*$ tale che $\phi = \lambda \Id$.
	\end{cor}
	\begin{cor}
		Se $V$ è un $A$-modulo semplice allora $\End_A(V)$ è un'algebra di divisione.
	\end{cor}
	Quello che abbiamo definito prima più precisamente si chiama \textit{$A$-modulo sinistro}, andiamo a dare un'altra definizione
	\begin{defn}
		Chiamiamo $A^{OP}$ l'\textit{algebra opposta} di $A$, $A^{OP}=A$ come spazio vettoriale, ma il prodotto in $A^{OP}$ è $a*b = ba$.
	\end{defn}
	\begin{defn}
		Un \textit{$A$-modulo destro} è una rappresentazione di $A^{OP}$. Se $\rho:A^{OP}\to \End(V)$ è una rappresentazione allora $V$ è un $A$-modulo destro.
	\end{defn}
	\begin{exmp}
		Alcuni esempi di algebre isomorfe alle loro algebre opposte
		\begin{enumerate}
			\item $\phi:M_n(\K) \to M_n(\K)$ dove $\phi(x)= x^T$ è un isomorfismo di algebre tra $\left(M_n(\K) \right)$ e $\left( M_n(\K) \right)^{OP}$.
			\item $\phi:\K[G]\to \K[G]$ dove $\phi(e_g)=e_{g^{-1}}$ ed estendendolo per linearità, $\phi$ è un isomorfismo di algebre tra $\K[G]$ e $\K[G]^{OP}$.
		\end{enumerate}
	\end{exmp}
	\begin{defn} Sia $\rho:A\to \End(V)$ una rappresentazione di $A$, definiamo la \textit{rappresentazione duale} $\rho^*:A^{OP}\to \End(V^*)$ dove
	\[
		(\rho^*(x)f)(v) = f(x\cdot v)
	\]
	\end{defn}
	\begin{prop}
		Abbiamo un isomomorfismo di algebre $A^{OP}\cong \End_A(A)$ dove $A$ è pensato come $A$-modulo sinistro.
	\end{prop}
	\begin{proof}
		Se $x\in A$, definiamo $r_x:A\to A$ come $r_x(a)=ax$, allora $r_x\in \End_A(A)$ poichè $r_x(ba)=bax = br_x(a)$. Se $\phi\in \End_A(A)$ allora $\phi = r_{\phi(1)}$, e questo implica che l'applicazione che associa a $x$ $r_x$ è surgettiva, la verifica che è iniettiva è immediata.\\
		BISOGNEREBBE SCRIVERE LE VERIFICHE CHE LUI NON HA FATTO...
	\end{proof}
\subsection{Ideali e quozienti}
	\begin{defn}
		Un ideale \textit{sinistro} di $A\supseteq I$ è un sottospazio vettoriale tale che $aI\subseteq I\ \forall a\in A$. Un ideale \textit{destro} è un sottospazio chiuso per la moltiplicazione a destra per elementi di $A$, un ideale è \textit{bilatero} se è sia destro che sinistro.
	\end{defn}
	\begin{rem}
		$I$ è un ideale destro se e solo se $I$ è un $A^{OP}$-sottomodulo, $I$ è un ideale sinistro se e solo se è un $A$-sottomodulo.
	\end{rem}
	\begin{rem}
		Se $I\subseteq A$ è un ideale sinistro allora $A/I$ è un $A$-modulo, se è un ideale destro allora $A/I$ è un $A^{OP}$-modulo, se $I$ è bilatero allora $A/I$ è un'algebra definendo $(a+I)(b+I)=ab+I$.
	\end{rem}
	\begin{exmp}
		Sia $\phi:A\to A'$ un omomorfismo di algebre, allora $\Ker \phi$ è un ideale bilatero di $A$ e $\Imm \phi \cong A/\Ker \phi$ come algebre.
	\end{exmp}
	\begin{defn}
		$A$ è un'\textit{algebra semplice} se non ha ideali bilateri non banali.
	\end{defn}
	\begin{rem}
		Sia $V$ un $A$-modulo sinistro, fissiamo $v\in V$, sia $\phi:A\to V$ come $\phi(x)=x\cdot v$. Si osserva che $\phi$ è lineare e che è un omomorfismo di $A$-moduli vedendo $A$ come $A$-modulo, dunque $\Ker \phi$ è un ideale sinistro.
	\end{rem}
	\begin{rem}
		In aggiunta alle ipotesi di prima sia $V$ un $A$-modulo semplice, $v\neq 0$, allora $\Imm \phi = V$ e dunque $V\cong A/\Ker \phi$ come $A$-moduli.
	\end{rem}
	\begin{defn}
		Un $A$-modulo $V$ è detto \textit{semisemplice} se è somma diretta di $A$-moduli semplici. Equivalentemente $V$ è detto semisemplice se ogni sottomodulo ammette un complementare stabile.
	\end{defn}
	\begin{exercise}
		Sia $A=\K[G]$ (CON G FINITO?????), allora ogni $A$-modulo di dimensione finita è semisemplice.
	\end{exercise}
	\begin{prop}
		Sia $V$ un $A$-modulo semisemplice, allora $V$ è semplice se e solo se $\End_A(V)$ è un'algebra di divisione.
	\end{prop}
	\begin{proof}
		\begin{enumerate}
			\item[$\Rightarrow$)]Per il lemma di Schur.
			\item[$\Leftarrow$)]Supponiamo che $V=V_1\oplus V_2$ sia una decomposizione in $A$-moduli. Siano $\pi_1:V\to V$ e $\pi_2:V\to V$ le proiezioni associate alla decomposizione. Si osserva $\pi_1$ e $\pi_2$ sono omomorfismi di $A$-moduli e che entrambi sono diversi da $0$, tuttavia $\pi_1\circ \pi_2 = 0$, quindi avremmo che $\End_A(V)$ non è un'algebra di divisione, assurdo.
		\end{enumerate}
	\end{proof}
\subsection{Serie di composizione}
	\begin{defn}
		Sia $A$ una $\K$-algebra, $V$ un $A$-modulo di dimensione finita. Si chiama \textit{filtrazione} per $V$ una sequenza di sottomoduli tale che
		\[
			0=V_0\subseteq V_1\subseteq \ldots \subseteq V_n=V
		\]
		Diciamo che una filtrazione è una \textit{serie di composizione} (o serie di Jordan-Holder) se i quozienti $\displaystyle V_i/V_{i-1}$ sono tutti $A$-moduli semplici. Questi quozienti sono detti \textit{fattori di composizione} della serie.
	\end{defn}
	\begin{exercise}
		Se $W\subseteq V$ sono $A$-moduli, allora $W$ è un sottomodulo proprio massimale se e solo se $V/W$ è semplice.
	\end{exercise}
	\begin{prop}
		Ogni $A$-modulo $V$ di dimensione finita ammette una serie di composizione di lunghezza finita.
	\end{prop}
	\begin{proof}
		Per induzione su $\dim V$: se $V$ è irriducibile ho finito, altrimenti sia $V_1$ un sottomodulo irriducibile (basta prenderne uno minimale). Consideriamo ora $V/V_1$, per ipotesi induttiva esso ammette una serie di decomposizione di lunghezza finita, prendendo le controimmagini della serie per $V/V_1$ tramite la proiezione al quoziente si ottiene una serie di Jordan-Holder.
		\\FORSE DA SPIEGARE MEGLIO PERCHE'...
	\end{proof}
	\begin{rem}
		Se $V$ è somma diretta di irriducibili ($V$ semisemplice), allora i fattori di composizione di qualsiasi serie per $V$ sono proprio gli irriducibili di $V$ contati con molteplicità.
	\end{rem}
	\begin{thm}[di Jordan-Holder]\label{thm:jordan-holder}
		Sia $V$ un $A$-modulo di dimensione finita, allora tutte le serie di composizione per $V$ hanno la stessa lunghezza (che chiamiamo lunghezza del modulo), e gli stessi fattori di composizione (contati con molteplicità e a meno di permutazioni).
	\end{thm}
	\begin{proof}
		Per induzione su $\dim V$: se $V$ è irriducibile ho finito, supponiamo dunque che abbia sottomoduli non banali, siano
		\begin{gather*}
			0=V_0\subseteq\ldots \subseteq V_m = V\qquad W_i=V_i/V_{i-1}\\
			0=V'_0\subseteq\ldots \subseteq V'_n = V\qquad W'_i=V'_i/V'_{i-1}
		\end{gather*}
		due serie di Jordan-Holder per $V$. Se $V_1=V'_1$ considero $V/V_1$, per ipotesi induttiva ho l'unicità. Supponiamo dunque che $V_1\neq V'_1$, visto che $V_1$ e $V'_1$ sono irriducibili si ha che $V_1\cap V'_1=0$, dunque $V_1+V'_1=V_1\oplus V'_1$.\\
		Considero il modulo $\quot{V}{V_1\oplus V'_1}$, siano $Z_1,\ldots,Z_p$ i fattori di composizione di una serie di Jordan-Holder per $\quot{V}{V_1\oplus V'_1}$. Ma allora si ha che
		\begin{enumerate}
			\item $V/V_1$ ammette due serie di composizione con fattori
			\[
				V'_1,Z_1,\ldots,Z_p\qquad W_2,\ldots,W_m
			\]
			\item $V/V'_1$ ammette due serie di composizione con fattori
			\[
				V_1,Z_1,\ldots,Z_p\qquad W'_2,\ldots,W'_n
			\]
		\end{enumerate}
		Per ipotesi induttiva ho l'unicità delle serie per $V/V_1$ e $V/V'_1$, dunque si ha che $n=m=p+2$. Sempre per via dell'unicità si ha che
		\begin{gather*}
			\{ W'_1,Z_1,\ldots,Z_p\} = \{ W_2,\ldots,W_m\}\\
			\{ W_1,Z_1,\ldots,Z_p\} = \{ W'_2,\ldots,W'_m\}
		\end{gather*}
		da cui, aggiungendo $W_1$ agli insiemi\footnote{non sono esattamente insiemi dato che devono contare i fattori con molteplicità} nella prima riga e $W'_1$ a quelli della seconda si ottiene
		\begin{gather*}
			\{ W'_1,W_1,Z_1,\ldots,Z_p\} = \{ W_1,W_2,\ldots,W_m\}\\
			\{ W'_1,W_1,Z_1,\ldots,Z_p\} = \{ W'_1,W'_2,\ldots,W'_m\}
		\end{gather*}
		essendo uguali i membri di sinistra ho uguaglianza anche dei membri di destra e quindi segue la tesi.
	\end{proof}
	\begin{exmp}
		Sia $A=\K[G]$ con $\Char \K \not \mid \ord(G)$, allora ogni rappresentazione di dimensione dinita è somma di moduli irriducibili. I fattori di composizione coincidono con gli addendi irriducibili contati con molteplicità.
	\end{exmp}
	\begin{rem}
		Sia $V$ un $A$-modulo, $W\subseteq V$ un sottomodulo, allora esiste sempre una serie di Jordan-Holder che contiene $W$ ($W=V_i$ per qualche $i$): basta considerare prima una serie per $W$ e comporla con la controimmagine tramite la proiezione al quoziente di una serie per $V/W$.
	\end{rem}
	\begin{cor}
		Sia $V$ un $A$-modulo semplice con $\dim_{\K}A<\infty$, allora $V$ compare come fattore di composizione in qualsiasi serie di Jordan-Holder per $A$ (considerando $A$ come $A$-modulo sinistro).
	\end{cor}
	\begin{proof}
		Fisso $v\in V, v\neq 0$ e considero $\phi:A\to V$ tale che $\phi(a) = av$, allora $\phi$ è surgettiva (siccome $V$ è semplice), $I=\Ker \phi \subseteq A$ è un ideale sinistro e si ha che $V\cong A/I$ come $A$-moduli. $I$ è un sottomodulo di $A$, quindi esiste una serie di composizione per $A$ che contiene $I$: esistono $I_0,\ldots,I_n$ ideali sinistri di $A$ tali che
		\[
			0=I_0\subseteq\ldots\subseteq I_n = A\qquad \text{con }I_k/I_{k-1}\text{ semplice }\forall k
		\]
		Sia $p\in \N$ tale che $I=I_p$, si osserva che $p=n-1$ poichè $A/I$ è semplice (dunque non posso allungare la serie), quindi $I_n/I_{n-1} = A/I\cong V$. Abbiamo provato dunque che in una serie di composizione troviamo un fattore uguale a $V$, avendo dimostrato l'unicità dei fattori della serie col teorema \ref{thm:jordan-holder} segue la tesi.
	\end{proof}
	\begin{exmp}
		Sia $A=\quot{\K[x]}{(x^n)}$, consideriamo la serie
		\[
			0\subset \quot{(x^{n-1})}{(x^n)}\subset \quot{(x^{n-2})}{(x^n)}\subset \ldots\subset \quot{\K[x]}{(x^n)} 
		\]
		Tutti i fattori di composizione sono isomorfi alla rappresentazione banale (dove $x$ agisce come $0$ e gli scalari agiscono come scalari), dunque l'unico modulo irriducibile è quello banale (OVVERO IL CAMPO K???). In particolare $\K[x]/(x^n)$ ammette un'unica rappresentazione irriducibile a meno di isomorfismo.
	\end{exmp}
	\begin{exmp}
		Sia $A=M_n(\K)$, un modulo irriducibile è $\K^n$, per ogni $i=1,\ldots,n$ sia
		\[
			V_i = \left\{ M\in A : M_{p,q} = 0\ \forall q>i \right\}
		\]
		ovvero $V_i$ è l'insieme delle matrici appartenenti ad $A$ con le ultime $n-i$ colonne nulle (in realtà è un sottomodulo). Consideriamo la serie
		\[
			0\subset V_1\subset\ldots\subset V_n=M_n(\K)
		\]
		si osserva che $V_i/V_{i-1}\cong \K^n$, dunque tutti i quozienti sono irriducibili e isomorfi tra loro, ma allora $\K^n$ è l'unica rappresentazione irriducibile di $M_n(\K)$ a meno di isomorfismo.
	\end{exmp}
	\begin{thm}[Krull-Schmidt]\label{thm:krull-schmidt}
		Sia $A$ una $\K$-algebra, $V$ un $A$-modulo con $\dim V<\infty$. Allora $V$ si decompone in modo unico come somma di rappresentazioni indecomponibili (a meno di isomorfismo e permutazioni).
	\end{thm}
	FORSE CONVIENE SPOSTARE L'ENUNCIATO DEL TEO PIÙ AVANTI...
	\begin{defn}
		Sia $x\in A$, $x$ è \textit{nilpotente} se $\exists\ n\in \N$ tale che $x^n=0$, $x$ è \textit{idempotente} se $x^2=x$.
	\end{defn}
	\begin{prop}
		Sia $V$ un $A$-modulo, allora $V$ è indecomponibile se e solo se per ogni $\phi\in \End_A(V)$ idempotente si ha che $\phi=0$ o $\phi=\Id$.
	\end{prop}
	\begin{proof}
		\begin{enumerate}
			\item[$\Leftarrow)$] Se $V$ non è indecomponibile allora $V = W\oplus W'$ con $W,W'\neq V$, sia $p\in \End_A(V)$ la proiezione su $W$ associata, allora $p^2=p, p\neq 0, p\neq \Id$, assurdo.
			\item[$\Rightarrow$)]Sia $p\in \End_A(V)$ tale che $p^2=p$, voglio vedere che $p=0$ o $p=\Id$. Si osserva subito che $\Ker p \cap \Imm p = 0$ (per l'ipotesi $p^2=p$), dunque $V=\Ker p\oplus \Imm p$, ma allora deve succedere che $\Ker p = 0$ o $\Imm p = 0$ (poichè $V$ è indecomponibile), se $\Imm p = 0$ allora $p=0$, se $\Ker p=0$ allora $p=\Id$ poichè $p^2=p$.
		\end{enumerate}
	\end{proof}
	\begin{lemma}
		Siano $V,W$ degli $A$-moduli indecomponibili di dimensione finita su $\K$, siano $\alpha\in \Hom_A(V,W)$ e $\beta\in \Hom_A(W,V)$ tali che $\beta\circ\alpha$ è un isomorfismo su $V$, allora $\alpha$ e $\beta$ sono isomorfismi.
	\end{lemma}
	\begin{proof}
		Vediamo che $W=\Ker\beta\oplus\Imm \alpha$:
		\begin{enumerate}
			\item $\Ker \beta\cap\Imm\alpha = 0$ poichè altrimenti $\beta\circ\alpha$ avrebbe nucleo diverso da $0$ ma sappiamo che $\beta\circ\alpha$ è un isomorfismo.
			\item Sia $x\in W$, sia $y=\left( \alpha\circ(\alpha\circ\beta)^{-1}\circ\beta \right)(x)$, allora $y\in \Imm \alpha$, consideriamo ora $\beta(y) = (\beta\circ\alpha)\circ(\beta\circ\alpha)^{-1}\circ\beta(x) = \beta(x)$, dunque $x-y\in \Ker \beta$, quindi $W=\Imm \alpha \oplus \Ker \beta$.
		\end{enumerate}
		Ma $W$ è indecomponibile, quindi $\Ker \beta = 0$ o $\Imm \alpha = 0$, la seconda situazione non può accadere (altrimenti $\beta\circ\alpha$ non sarebbe un isomorfismo), ma allora $\Ker \beta = 0$ e $\Imm \alpha = W$, visto che necessariamente $\alpha$ è iniettivo (altrimenti non lo sarebbe $\beta\circ\alpha$) si ha che $\alpha$ è un isomorfismo, e dunque anche $\beta$ lo è.
	\end{proof}
	\begin{prop}
		Sia $V$ un $A$-modulo indecomponibile e $\phi\in \End_A(V)$, allora $\phi$ è un isomorfismo oppure è nilpotente.
	\end{prop}
	\begin{proof}
		Sia $\phi\in \End_A(V)$, considero le due successioni
		\begin{gather*}
			\ldots\supseteq \Ker\phi^n\supseteq\Ker\phi^{n-1}\supseteq\ldots\supseteq\Ker\phi^0\\
			\ldots\subseteq\Imm\phi^n\subseteq\Imm\phi^{n-1}\subseteq\ldots\subseteq\Imm\phi^0
		\end{gather*}
		Queste successioni si devono stabilizzare, sia dunque $N$ tale che $\Ker\phi^N=\Ker\phi^{N+1}$ e $\Imm\phi^N=\Imm\phi^{N+1}$, allora si ha che $\Ker\phi^N\cap\Imm\phi^N=0$ (altrimenti non si sarebbero stabilizzate le successioni), ma allora $V=\Ker\phi^N\oplus\Imm\phi^N$ per questioni di dimensioni.\\
		Per ipotesi $V$ è indecomponibile dunque $\Imm\phi^N=0$ oppure $\Ker\phi^N=0$, se $\Imm\phi^N=0$ allora $\phi$ è nilpotente, se invece $\Ker\phi^N=0$ allora a maggior ragione $\Ker \phi=0$ e dunque $\phi$ è un isomorfismo.
	\end{proof}
	\begin{cor}
		Sia $V$ un $A$-modulo indecomponibile, siano $\theta_1,\ldots,\theta_s\in \End_A(V)$ con $\theta_i$ nilpotente per $i=1,\ldots,s$, allora $\theta_1+\ldots+\theta_s$ è nilpotente.
	\end{cor}
	\begin{proof}
		Sia $\displaystyle \theta=\sum_{i=1}^{s}\theta_i\in \End_A(V)$, per la proposizione precedente sappiamo che $\theta$ è un isomorfismo oppure è nilpotente. Procediamo per induzione su $s$:
		\begin{itemize}
			\item Se $s=1$ allora $\theta=\theta_1$ che è nilpotente.
			\item Supponiamo la tesi vera per $s=n-1$ e dimostriamola per $s=n$: supponiamo per assurdo che $\theta$ sia un isomorfismo, allora avremmo che
			\[
				\Id = \theta^{-1}\theta_1+\ldots+\theta^{-1}\theta_n
			\]
			Si osserva subito che $\theta^{-1}\theta_i$ è nilpotente (dato che ha nucleo non nullo non è un isomorfismo e dunque è nilpotente), quindi
			\[
				\Id-\theta^{-1}\theta_n = \theta^{-1}\theta_1+\ldots+\theta^{-1}\theta_{n-1}
			\]
			Sappiamo per ipotesi induttiva che la somma di $n-1$ endomorfismi nilpotenti è ancora nilpotente, dunque il termine di destra dell'equazione è nilpotente, ma questo è assurdo essendo quello di sinistra un isomorfismo: se $\phi$ è nilpotente con indice di nilpotenza $k$ allora $\Id-\phi$ è un isomorfismo, infatti
			\[
				(\Id-\phi)\cdot(1+\phi+\ldots+\phi^{k-1}) = \Id-\underbrace{\phi^k}_{=0} = \Id
			\]
		\end{itemize}
	\end{proof}
	
	
\subsection{Un teorema di divisibilità}
  L'obiettivo di questa sezione è dimostrare il seguente teorema:
  \begin{thm}\label{dim_mid_ordine}
    Sia $G$ un gruppo finito, $V$ una rappresentazione irriducibile di $G$ su $\C$ (ovvero un $\C[G]$ modulo semplice). Allora $$\dim V\mid  \ord(G)$$
  \end{thm}
  \begin{rem}$ $
   \begin{itemize}
    \item Il teorema è falso su campi non algebricamente chiusi: basta considerare $\Z/3\Z$ su $\R$ che ha una rappresentazione irriducibile di grado 2.
    \item Il teorema vale più in generale su un campo $\K$ algebricamente chiuso per cui $\Char \K \nmid \ord(G)$.
    \item Se invece $\Char \K\mid \ord(G)$, il teorema è in generale falso: consideriamo $G=SL_2(\mathbb F_{13})$ e $\K=\overline{\mathbb F_{13}}$; prendiamo poi la rappresentazione $V=\K[x,y]_5$, che è irriducibile (IO NON SO FARLO) di grado $6$. Tuttavia $\ord(G)=\binom{14}{3}=2^2\cdot7\cdot13$, quindi non c'è divisibilità.
   \end{itemize}
  \end{rem}
  Introduciamo ora alcuni concetti di teoria degli anelli.\\
  Sia $R$ un anello commutativo unitario.\\
  \begin{defn}
    Un elemento $x\in R$ si dice \textit{integrale} su $\Z$ se è radice di un polinomio monico a coefficienti interi.\\
    Se $x\in\C$ lo chiamo \textit{intero algebrico}.
  \end{defn}
  \begin{rem}\label{int_alg_raz}
   Gli interi algebrici di $\Q$ sono esattamente gli interi, per il criterio delle radici razionali.
  \end{rem}
  Possiamo inoltre definire una struttura di modulo usando come scalari gli elementi di $R$:
  \begin{defn}
   Sia $M$ un gruppo abeliano; allora $M$ è un $R$-modulo se esiste un'applicazione $\cdot:R\times M\to M$ che soddisfa
   \begin{itemize}
    \item $r\cdot(m+n)=r\cdot m+r\cdot n\;\forall r\in R\;\forall m,n\in M$
    \item $(r+s)\cdot m=r\cdot m+s\cdot m\;\forall r,s\in R\;\forall m\in M$
    \item $r\cdot(s\cdot m)=(ab)\cdot m\;\forall r,s\in R\;\forall m\in M$
   \end{itemize}
   Diciamo inoltre che $M$ è finitamente generato se lo è come gruppo.
  \end{defn}
  \begin{rem}
   Gli $\Z$-moduli sono esattamente i gruppi abeliani.
  \end{rem}
  Definiamo ora un'importante proprietà degli anelli
  \begin{defn}
   Un anello $R$ è detto \textit{Noetheriano} se tutti i suoi ideali sono finitamente generati; o equivalentemente, se ogni catena ascendente di ideali si stabilizza.
  \end{defn}
  \begin{thm}[base di Hilbert]
   Se $R$ è un anello Noetheriano, allora anche $R[x]$ è Noetheriano.
  \end{thm}
  \begin{proof}
   Sia $I\subset R[x]$ un ideale per assurdo non finitamente generato. Allora posso costruire una successione di polinomi $p_n$ tali che, se $I_n=(p_1,\dots,p_{n-1})$ allora $p_n\in I\setminus I_n$, e sceglo $p_n$ di grado minimale.\\
   Definiamo ora $a_n$ come il termine di testa di $p_n$, e consideriamo $R\supset J=(a_1,a_2,\dots)$ l'ideale generato da tutti gli $a_i$.\\
   Poiché $R$ è Noetheriano, vale che $J=(a_1,\dots,a_{N-1})$ per un certo $N$; ma allora posso scrivere $a_N=\sum_{i<N}b_ia_i$ con $b_i\in R$.\\
   Prendo ora il polinomio $\displaystyle f=\sum_{i<N}b_i x^{\deg p_N -\deg p_i}p_i$ (è una buona definizione, poiché i gradi dei polinomi sono crescenti).
   Osserviamo che $f\in I_N$ essendo combinazione lineare dei $p_i$; inoltre ha lo stesso grado e lo stesso coefficiente di testa di $p_N$.\\
   Ma allora $p_N-f\in I\setminus I_N$, perché se fosse $p_N-f\in I_N$ avremmo $p_N=(p_N-f)+f\in I_N$; tuttavia $\deg (p_N-f)<\deg p_N$, che è assurdo in quanto $p_N$ ha grado minimo in $I\setminus I_N$
  \end{proof}
  \begin{cor}
   Si vede facilmente per induzione che $R[x_1,x_2,\dots,x_n]$ è Noetheriano se $R$ lo è.
  \end{cor}
  \begin{prop}\label{sottomod_fin_gen}
   Sia $R$ un anello Noetheriano, $M$ un $R$-modulo finitamente generato. Allora ogni sottomodulo di $M$ è finitamente generato.
  \end{prop}
  LUI FA UNA ZINGARATA ASSURDA. CREDO CHE INVECE DEL TEO DI HILBERT SERVA CHE $R^n$ SIA NOETH.
  \begin{proof}
   Scriviamo $M=Rm_1+Rm_2+\dots+Rm_n$, dove $m_1,\dots,m_n$ sono i generatori di $M$.\\
   Consideriamo la mappa $\varphi:R^n\to M$ tale che $\varphi(a_1,\dots,a_n)=a_1m_1+\dots+a_nm_n$, che è surgettiva per ipotesi.\\
   Detto allora $I=\ker\varphi$, si ha $M\cong\faktor{R^n}{I}$ e in particolare si ha la corrispondenza tra ideali di $M$, ovvero sottomoduli, e ideali di $R^n$ che contengono $I$.
   Ovvero, se $N\subset M$ è un sottomodulo allora esiste un ideale $J\subset R^n$ tale che $N\cong\faktor{J}{I}$.\\
   Se $R$ è Noetheriano, anche $R^n$ è Noetheriano, quindi $J$ è finitamente generato, e quindi lo è anche $N$ (i generatori sono le proiezioni di quelli di $J$).
  \end{proof}
  Possiamo ora dare un criterio di integralità per elementi di $R$.
  \begin{prop}\label{crit_integ}
   Dato $R$ anello commutativo unitario e un elemento $x\in R$, sono equivalenti:
   \begin{itemize}
    \item $x$ è integrale su $\Z$
    \item Detto $\Z[x]\subset R$ il sottoanello generato da $x$, $\Z[x]$ è uno $\Z$-modulo finitamente generato
    \item Esiste uno $\Z$-sottomodulo di $R$ finitamente generato che contiene $\Z[x]$
   \end{itemize}
  \end{prop}
  \begin{proof}
   \begin{itemize}
    \item[$2)\iff 3)$] Usiamo la proposizione \ref{sottomod_fin_gen} ed il fatto che $\Z$ è Noetheriano.
    \item[$1)\Rightarrow2)$] Sia $t^n+a_{n-1}t^{n-1}+\dots+a_0$ un polinomio a coefficienti interi che annulla $x$; allora $\Z[x]$ è generato da $\{1,x,\dots,x^{n-1}\}$.
    \item[$2)\Rightarrow1)$] Osserviamo che $\Z[x]=\bigcup R_n$, con $R_n$ il sottomodulo generato da $\{1,x,\dots,x^{n-1}\}$. Se $\Z[x]$ è finitamente generato, allora deve essere $\Z[x]=R_N$ per un certo $N$.
    Ma allora si ha semplicemente che $x^N=\sum_{i<N}a_ix^i$ con $a_i\in\Z$
   \end{itemize}
  \end{proof}
  Per dimostrare un'importante proprietà degli elementi integrali, dobbiamo costruire il prodotto tensore di moduli.\\
  \begin{defn}
   Dati due $R$-moduli $M,N$ si dice prodotto tensore $M\otimes N$ un $R$-modulo $V$ per cui esiste una funzione bilineare $\psi:M\times N\to V$ tale che per ogni $R$-modulo $V'$ con una funzione bilineare $\phi:M\times N\to V'$ esiste un'unica funzione lineare $\phi':V\to V'$ che commuta.
   \[\tridiag{M\times N}{ \psi }{M \otimes N}{\phi'}{V'}{\phi}\]
   Si indica $m\otimes n=\psi(m,n)$
  \end{defn}
  Si dimostra che il prodotto tensore esiste ed è unico; un'idea di costruzione è la seguente.\\
  Consideriamo il prodotto $M\times N$ con tutti i suoi elementi $(m,n)$ che identifichiamo con i simboli $m\ast n$; prendiamo ora $P$ il gruppo libero generato da tutti i simboli $m\ast n$.\\
  Quozientiamo poi $P$ per le relazioni che ci interessano, ovvero:
  \begin{itemize}
   \item $m\ast n+m\ast n'-m\ast(n+n')$
   \item $m\ast n+m'\ast n-(m+m')\ast n$
   \item $(rm)\ast n-m\ast(rn)$
  \end{itemize}
  con $m\in M,n\in N,r\in R$.\\
  L'ultima relazione ci dice come dotare questo gruppo di un prodotto per scalare, ovvero $r\cdot(m\otimes n)=(rm)\otimes n=m\otimes(rn)$.\\
  Possiamo ora andare avanti con la seguente
  \begin{prop}
   L'insieme degli elementi di $R$ integrali su $\Z$ è un sottoanello di $R$
  \end{prop}
  \begin{proof}
   Per il criterio \ref{crit_integ} se $x,y$ sono integrali su $\Z$, allora $\Z[x]$ e $\Z[y]$ sono finitamente generati.\\
   Considero allora l'applicazione lineare $\phi:\Z[x]\otimes_\Z\Z[y]\to\Z[x,y]$ data da $\phi(p\otimes q)=p\cdot q$; questa è surgettiva poiché posso scomporre monomio per monomio.\\
   Inoltre il prodotto tensore di due moduli finitamente generati è finitamente generato; ma allora $\Z[x,y]$ è immagine di un modulo finitamente generato, quindi anche lui è finitamente generato.\\
   Applicando di nuovo il criterio \ref{crit_integ}, abbiamo che $x+y$ e $xy$ sono integrali su $\Z$, in quanto $\Z[x+y]$ e $\Z[xy]$ sono sottomoduli di $\Z[x,y]$
  \end{proof}

  Ritorniamo ora al nostro $\C[G]$; ricordiamo che per UN CRITERIO A CASO FATTO PRIMA (MAGARI SI PUÒ SPOSTARE QUA?) $Z(\C[G])$ ha per base gli elementi $c_i=\sum_{x\in\mathcal C_i}x$ dove $\mathcal C_i$ sono le classi di coniugio di $G$.\\
  \begin{prop}
   Sia $Z(\C[G])\ni u=\sum u_ic_i$; se $u_i$ sono interi algebrici, allora $u$ è integrale su $\Z$.
  \end{prop}
  \begin{proof}
   Poiché gli elementi integrali sono un sottoanello, ci basta mostrare che tutti i $c_i$ sono integrali.\\
   Osserviamo che per ogni $j,k$ l'elemento $c_jc_k$ appartiene ancora al centro, e dunque è combinazione lineare dei $c_i$; questo vuol dire che $\Z[c_1,\dots,c_k]$ è finitamente generato (in particolare dai $c_i$).\\
   Ma allora di nuovo per il criterio \ref{crit_integ}, dato che $\Z[c_i]\subset\Z[c_1,\dots,c_k]$, abbiamo che $c_i$ è integrale.
  \end{proof}
  \begin{lemma}\label{u_int_alg}
   Sia $V$ una rappresentazione irriducibile di $G$, $\chi$ il suo carattere, $Z(\C[G])\ni u=\sum gu_g$ con $u_g$ interi algebrici. Allora $$\frac1{\dim V}\sum u_g\chi(g)$$ è un intero algebrico.
  \end{lemma}
  \begin{proof}
   Per Schur, $u$ agisce su $V$ come lo scalare $\displaystyle\frac{\tr(u)}{\dim V}=\frac{\sum u_g\chi(g)}{\dim V}$.\\
   Abbiamo allora un'omomorfismo $\varphi:Z(\C[G])\to\C$ dato proprio da $\displaystyle\varphi(u)=\frac{\sum u_g\chi(g)}{\dim V}$, in quanto $(u_1u_2)v=u_1(u_2v)$ con $v\in V$, cioè $\varphi(u_1u_2)v=\varphi(u_1)\varphi(u_2)v$.\\
   Per il lemma precedente sappiamo che $u$ è integrale, quindi soddisfa un polinomio monico; sfruttando il fatto che $\varphi$ è omomorfismo, sappiamo che anche $\varphi(u)$ soddisfa lo stesso polinomio monico, ovvero $\varphi(u)$ è un intero algebrico.
  \end{proof}
  Armati di questo lemma possiamo finalmente dimostrare il teorema \ref{dim_mid_ordine}
  \begin{proof}
   Sia $\chi$ il carattere della rappresentazione; poiché il gruppo è finito vale $g^{\ord(G)}=1$, da cui gli autovalori di $g$ sono tutti radici dell'unità; allora $\chi(g)$ è intero algebrico, essendo somma di radici dell'unità che sono interi algebrici.\\
   Poniamo $\displaystyle u=\sum_{g\in G}\chi(g^{-1})g$ e osserviamo che appartiene al centro: $\displaystyle hu=\sum_{g\in G} \chi((hg)^{-1}h)hg=\sum_{\sigma\in G} \chi(\sigma h)\sigma$ e similmente $\displaystyle uh=\sum_{\tau\in G}\chi(h\tau)\tau$; poiché $\tr(\sigma h)=\tr(h\sigma)$, abbiamo $uh=hu\;\forall h\in G$.\\
   Ma allora per il lemma \ref{u_int_alg} $z=\displaystyle\frac1{\dim V}\sum_{g\in G}\chi(g^{-1})\chi(g)$ è un intero algebrico.\\
   Osserviamo che $\displaystyle z=\frac{\ord(G)}{\dim V}\cdot \frac{\sum \chi(g^{-1})\chi(g)}{\ord(G)}=\frac{\ord(G)}{\dim V}\cdot\langle \chi,\chi\rangle$; essendo $V$ irriducibile, il prodotto dei caratteri vale $1$.\\
   Quindi si ha che $\dfrac{\ord(G)}{\dim V}\in\Q$ è un intero algebrico, e quindi per l'osservazione \ref{int_alg_raz} sta in $\Z$.
  \end{proof}


  



  

















	



\section{Rappresentazioni di $S_n$}
Cerchiamo ora quali sono le rappresentazioni del gruppo simmetrico; in particolare caratterizziamo i moduli semplici di $A=\Q[S_n]$.\\
Come prima osservazione, sappiamo che le rappresentazioni irriducibili di un gruppo $G$ sono al più tante quante le classi di coniugio di $G$; per $G=S_n$, le classi di coniugio sono in corrispondenza biunivoca con le partizioni di $n$: infatti tutti gli elementi di una classe condividono la stessa struttura ciclica.\\
A loro volta, le partizioni possono essere pensate come particolari tabelle, nel modo seguente.
\begin{defn}
	Una $n$-tabella è un insieme di $n$ quadratini disposti come in una scacchiera, in modo che il numero di quadratini presenti in ogni riga sia minore o uguale al numero di quadratini presenti nella riga sopra.
\end{defn}
Per costruire i moduli che ci interessano, è necessario riempire le tabelle con dei numeri.
\begin{defn}
	Un $n$-diagramma $D$ è una $n$-tabella dove in ogni quadratino vi è un numero da $1$ a $n$, in modo che ogni numero compaia una ed una sola volta nell'intera tabella.
\end{defn}
\begin{exmp}
	Sia $n=7$, $\mathcal{C}$ la classe di coniugio di $(123)(45)(67)$; allora la partizione corrispondente a $\mathcal{C}$ è $3,2,2$, che ha la seguente tabella; uno dei possibili diagrammi è \\ $\ydiagram{3,2,2}$ $\ytableaushort{356,14,27}$
\end{exmp}
Osserviamo che $S_n$ agisce transitivamente sull'insieme dei diagrammi con la stessa forma nel modo ovvio.\\
Fissato un diagramma $D$, siamo interessati ai seguenti due sottogruppi di $S_n$:
\begin{defn}$ $
	\begin{itemize}
		\item $R(D)=\{\sigma\in S_n : \sigma(i) \text{ sta nella stessa riga di } i\;\forall i=1,\dots,n\}$
		\item $C(D)=\{\sigma\in S_n : \sigma(i) \text{ sta nella stessa colonna di } i\;\forall i=1,\dots,n\}$
	\end{itemize}
\end{defn}
Osserviamo che $R(D)\cup C(D)=\{id\}$.\\
Possiamo ora definire l'elemento generatore degli ideali che che ci interessano.
\begin{defn}
	Dato un diagramma $D$, il suo simmetrizzatore è $$e_D=\left(\sum_{p\in R(D)}p \right)\left(\sum_{q\in C(D)}q\varepsilon_q\right)$$
	Inoltre il modulo corrispondente è $V_D=Ae_D$
\end{defn}
\begin{prop}\label{prop:assorbimento eD}
	Se $p\in R(D)$ e $q\in C(D)$ allora $pe_D=e_D$ e $e_Dq=e_D\varepsilon_q$
\end{prop}
\begin{prop}
	Sia $g\in S_n$; allora $e_{gD}=ge_Dg^{-1}$
\end{prop}
\begin{proof}
	Vediamo che $R(gD)=gR(D)g^{-1}$. Infatti $\sigma\in R(gD)\iff\sigma(i)$ sta nella stessa riga di $i$ all'interno del diagramma $gD$, ovvero $\sigma g (j)=g(g^{-1}\sigma g)(j)$ sta nella stessa riga di $g(j)$ (abbiamo scritto $i=g(j)$ con $j$ pensato in $D$); questo si verifica se e solo se $g^{-1}\sigma g(j)$ sta nella stessa riga di $j$ in $D$, ovvero $g^{-1}\sigma g(j)\in R(D)$, che è quello che volevamo.\\
	Similmente $C(gD)=gC(D)g^{-1}$.\\
	Allora $\displaystyle e_{gD}=\left(\sum_{p\in R(gD)}p \right)\cdot\left(\sum_{q\in C(gD)}q\varepsilon_q\right)=\left(\sum_{p'\in R(D)}gp'g^{-1} \right)\cdot\left(\sum_{q'\in C(D)}gq'g^{-1}\text{sgn}(gq'g^{-1})\right)$; tirando fuori dalle parentesi le $g$ e usando $\text{sgn}(gq'g^{-1})=\text{sgn}(q')$ si ha la tesi.
\end{proof}
\begin{cor}
	Se $D$ e $D'$ sono diagrammi con la stessa forma, allora $V_D\cong V_{D'}$
\end{cor}
\begin{proof}
	Poiché $D$ e $D'$ hanno la stessa forma, esiste un $g\in S_n$ tale che $D'=gD$, e perciò $e_{D'}=ge_Dg^{-1}$.\\
	Allora $Ae_{D'}=Age_Dg^{-1}$. Osserviamo che $1=g^{-1}g\in Ag$ perciò $Ag=A$; inoltre $Ae_Dg^{-1}\cong Ae_D$ grazie all'omomorfismo $xe_Dg^{-1}\mapsto xe_D$ che è iniettivo
\end{proof}
\begin{lemma}\label{Sn:g=pq}
	Sia $D$ un diagramma e $g\in S_n$. Allora possiamo scrivere $g=pq$ con $p\in R(D)$ e $q\in C(D)$ se e solo se nessuna coppia di numeri che si trova nella stessa diga di $D$ sta anche sulla stessa colonna di $gD$.
\end{lemma}
\begin{proof}$ $
	\begin{itemize}
		\item[$\Rightarrow$] Scrivo $g=pq$; siano $a,b$ due numeri sulla stessa riga di $D$. Allora $a,b$ stanno sulla stessa riga anche in $pD$. Inoltre $gD=(pqp^{-1})pD$ e $pqp^{-1}\in C(pD)$; perciò $gD$ ha le stesse colonne di $pD$, e in particolare poiché $a,b$ stavano in colonne diverse di $pD$, stanno anche su colonne diverse di $gD$.
		\item[$\Leftarrow$] Per transitività mostro che $gD=pqD$ con certi $p,q$ della forma cercata. Guardo la prima colonna di $gD$: tutti i suoi elementi stanno in righe diverse di $D$ per ipotesi, quindi posso scegliere un $p_1\in R(D)$ tale che $p_1D$ e $gD$ abbiano gli stessi elementi nella prima colonna. Continuando in questo modo trovo un $p\in R(D)$ tale che $pD$ e $gD$ hanno le stesse colonne; allora esiste un $q'\in C(pD)$ per cui $q'pD=gD$; ma $q'=pqp^{-1}$ con $q\in C(D)$, perciò si ha proprio $pqD=gD$.
	\end{itemize}
\end{proof}
Osserviamo che le partizioni di $n$ possono essere ordinate in modo lessicografico.\\
Dimostriamo ora questo importante lemma di ortogonalità dei simmetrizzatori.
\begin{lemma}\label{Sn:ortogonalita}
	Sia $D$ un diagramma di forma $\lambda$ e $D'$ un diagramma di forma $\lambda'$; allora se $\lambda>\lambda'$ vale $e_{D'}\cdot e_{D}=0$
\end{lemma}
\begin{proof}
	Per prima cosa vogliamo trovare due simboli $a,b$ nella stessa riga di $D$ e nella stessa colonna di $D'$; supponiamo per assurdo che non esistano.\\
	Diciamo che $\lambda: a_1\ge\dots a_k$ e $\lambda':a_1'\ge\dots a_h'$; notiamo che gli $a_1$ simboli della prima riga di $D$ devono stare tutti in colonne diverse di $D'$, per l'ipotesi di assurdo. Ciò vuol dire che $D'$ ha almeno $a_1$ colonne, ma valeva anche $a_1\ge a_1'$: perciò si deve avere $a_1=a_1'$. Sia ora $m$ il numero di righe lunghe $a_1$ elementi di $\lambda$ e $m'$ il suo analogo; poiché $\lambda>\lambda'$ si ha $m\ge m'$. Inoltre tutti gli elementi delle prime $m$ righe di $D$ devono andare nelle prime $m'$ righe di $D'$, ma allora in $D$ ho $ma_1$ elementi da mettere in $m'a_1$ posti in $D'$, ovvero $m'\ge m$, perciò $m=m'$.\\
	Non può essere che queste $m$ righe fossero tutta la tabella, poiché $\lambda$ e $\lambda'$ sono diversi; allora le tabelle continuano, e poiché gli elementi delle prime $m$ righe restano nelle prime $m$ righe, possiamo dimenticarci di loro e ripetere il ragionamento togliendo le prime $m$ righe sia da $\lambda$ che da $\lambda'$.\\
	Ma quindi abbiamo creato un processo infinito, in cui ogni volta la lunghezza della prima riga decresce strettamente, il che è assurdo.\\
	Allora abbiamo davvero $a,b$ nella stessa riga di $D$ e nella stessa colonna di $D'$; sia $t=(a,b)\in S_n$, allora $t\in R(D)\cup C(D')$.\\
	Si ha dunque $e_{D'}\cdot e_{D}=e_{D'}\cdot t\cdot t\cdot e_{D}=(-e_{D'})\cdot e_D$ per l'osservazione \eqref{prop:assorbimento eD}, ovvero $2\cdot e_{D'}\cdot e_{D}=0$
\end{proof}
Troviamo ora una caratterizzazione di $V_D$
\begin{lemma}\label{Sn:caratterizzazione}
	Fissiamo un diagramma $D$; sia $x\in A$ tale che $\forall p\in R(D),q\in C(D)$ si abbia $pxq=\varepsilon_qx$. Allora $\exists\gamma\in\Q$ tale che $x=\gamma e_D$.
\end{lemma}
\begin{proof}
	Scriviamo $x=\sum a_gg$; l'ipotesi è che $x=\varepsilon_q p^{-1}x^{-1}=\varepsilon_q=\varepsilon_q\sum a_g p^{-1}gq^{-1}\sum a_{pgq}g$ shiftando gli indici $g\mapsto pgq$. Questo ci dice che $\varepsilon_q a_{pgq}=a_g$ per tutti i $p,q$; in particolare con $g=\mathrm{id}$ otteniamo $a_pq=\varepsilon_q a_1$, che sono esattamente i coefficienti di $e_D$.\\
	Dimostriamo che se $g\neq pq$ con $p\in R(D),q\in C(D)$ allora $a_g=0$; per il lemma \eqref{Sn:g=pq} esistono $a,b$ sulla stessa riga di $D$ e sulla stessa colonna di $gD$. Allora $t=(a,b)\in R(D)\cap C(gD)$, e inoltre $q=g^{-1}tg\in C(D)$ è una trasposizione.\\
	Allora usiamo la formula trovata con $p=t,q=g^{-1}tg$, da cui $a_g=-a_{tgg^{-1}tg}=-a_g$, cioè $a_g=0$
\end{proof}
\begin{prop}
	Esiste un $\gamma\in\Q^\ast$ tale che $e_D^2=\gamma e_D$, ovvero $u_D=\gamma^{-1}u_D$ è idempotente
\end{prop}
\begin{proof}
	Per il lemma precedente, mi basta far vedere che $pe_D^2q=\varepsilon_q e_D^2$; ma questa è ovvia poiché $pe_D^2q=(pe_D)(e_Dq)=e_D\cdot\varepsilon_q e_D$. Allora $e_D^2=\gamma e_D$.\\
	La cosa che vogliamo è $\gamma\neq0$. Sia $T:A\mapsto Ae_D$ l'omomorfismo $T(x)=xe_D$, e calcoliamo $\tr T$ in due modi diversi.\\
	Una base di $A$ è $S_n=\{g_1,g_s\dots,g_{n!}\}$ con $g_1=\mathrm{id}$. Se scriviamo $e_D=\sum a_i g_i$, si vede che il coefficiente di $g_j$ nella scrittura di $T(g_j)$ è esattamente $a_1$, poiché la moltiplicazione per un elemento di base permuta i coefficienti. Allora vale $\tr T=a_1\cdot n!=n!$ in quanto $a_1=1$ dalla definizione.\\
	Consideriamo ora $\Imm T=Ae_D$ che è un sottospazio di $A$; se $x\in Ae_D$, allora $x=ye_D$ per cui $T(x)=ye_D^2=y\gamma e_D=\gamma x$, ovvero in $Ae_D$ tutti gli elementi sono autovettori. Allora è chiaro che se prendo una base di $Ae_D$ e la completo a base di $A$, $T$ si scriverà nella forma $\left(\begin{array}{ccccc}
	\gamma & & & & \\
	& \ddots & & & \\
	& & \gamma & & \\
	& & & \ddots & \\
	& & & & 0
	\end{array}\right)$ perciò $tr T=\gamma\cdot\dim Ae_D=\gamma m$.\\
	Si ha dunque $n!=\tr T = \gamma m$, da cui $\gamma=\frac{n!}{m}\neq0$; dato che $e_D$ ha come coefficienti solo $\pm1$, allora $e_D^2\in\Z[S_n]$, perciò si deve avere $\gamma\in\N$, ovvero $\dim V_D\mid |S_n|$
\end{proof}

Armati di tutti questi lemmi possiamo avviarci alla conclusione, ovvero dimostrare che i $V_D$ sono tutte le rappresentazioni irriducibili, al variare della forma di $D$.

\begin{prop}
	Dato un diagramma $D$, $V_D=Ae_D$ è un ideale minimale, ovvero un sottomodulo semplice
\end{prop}
\begin{proof}
	Grazie alla proposizione precedente, sappiamo che $u_D$ è idempotente ed è un multiplo scalare di $e_D$; perciò $Ae_D=Au_D$.\\
	Usando il criterio METTERE IL REF QUANDO SARÀ SCRITTO, $Au_D$ è minimale se e solo se $u_DAu_D$ è un corpo, cosa che avviene se e solo se $e_DAe_D$ lo è.\\
	Prendiamo $x\in e_DAe_D$, cioè $x=e_Dye_D$; osserviamo che $pxq=(pe_D)y(e_Dq)=e_Dye_D\varepsilon_q=\varepsilon_q x$, perciò per il lemma \eqref{Sn:caratterizzazione} abbiamo $x=ke_D$ con $k\in\Q$.\\
	Allora $e_DAe_D=\Q e_D=\Q u_D$ che è un corpo isomorfo a $\Q$ con elemento neutro del prodotto proprio $u_D$.
\end{proof}
\begin{prop}
	Siano $D,D'$ diagrammi di forma diversa $\lambda\neq\lambda'$, allora $V_D\not\cong V_{D'}$
\end{prop}
\begin{proof}
	Supponiamo che $\lambda>\lambda'$. Per assurdo $V_D\cong V_{D'}$.\\
	Essendo minimali, esiste un $a\in A$ tale che $Au_Da=Au_{D'}$, cioè $u_{D'}=bu_{D}a$. Allora $u_{D'}^2=u_{D'}\cdot u_{D'}=u_{D'}bu_{D}a$.\\
	Tuttavia, $\forall g\in S_n$ vale che $u_{D'}gu_{D}g^{-1}=u_{D'}\cdot u_{gD}=0$ per il lemma \eqref{Sn:ortogonalita}. Ma allora $u_{D'}bu_{D}=0\;\forall b\in A$, che porterebbe a $u_{D'}=0$, assurdo.
\end{proof}
Possiamo ora unificare tutto il nostro lavoro nel seguente
\begin{thm}$ $
	\begin{enumerate}
		\item C'è una corrispondenza biunivoca $\{\text{classi di coniugio di }S_n\} \leftrightarrow \{\text{partizioni di }n\} \leftrightarrow \{n-\text{tabelle}\}$
		\item Abbiamo una mappa surgettiva $\{\text{diagrammi}\}\to\text{Irr}(S_n)$ data da $D\mapsto V_D=Ae_D$
		\item $V_D\cong V_{D'}$ se e solo se $D$ e $D'$ hanno la stessa forma
	\end{enumerate}
\end{thm}

Facciamo ora un po' di esempi e casi particolari.\\
\begin{itemize}
	\item 
	\begin{ytableau}
		$ $ & &\none[\cdots] & \\
	\end{ytableau}\\
	La tabella di una sola riga lunga $n$, a cui corrisponde $e_D=\sum_{\sigma\in S_n}\sigma$ cioè la rappresentazione banale.
	
	\item 
	\begin{ytableau}
		$ $ \\
		\\
		\none[\vdots]\\
		\\
	\end{ytableau}\\
	La tabella di una sola colonna lunga $n$, a cui corrisponde $e_D=\sum_{\sigma\in S_n}\sigma\varepsilon_\sigma$ che è la rappresentazione segno.
	
	\item 
	\begin{ytableau}
		$ $ & &\none[\cdots] & \\
		$ $\\
	\end{ytableau}\\
	La tabella con $n-1$ quadratini nella prima riga e $1$ nella seconda è la rappresentazione standard di $S_n$
\end{itemize}

Concludiamo con una formula che ci permette di calcolare la dimensione di una rappresentazione a partire dalla sua tabella.
\begin{thm}[formula degli uncini]
	Sia $\lambda$ una $n$-tabella, allora $$\dim V_\lambda=\frac{n!}{\prod_{i=1}^n\ell_i}$$ dove $\ell_i$ è la lunghezza dell'uncino che parte dalla casella $i$, ovvero il massimo percorso a L rovesciata che passa dalla casella $i$
\end{thm}



\end{document}
