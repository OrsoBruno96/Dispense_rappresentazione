\documentclass[11pt]{article}


\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[a4paper]{geometry}
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{array}
\usepackage{xy}
\usepackage{multicol}
%\usepackage{slashbox}
\usepackage{fancyhdr}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[T1,OT1]{fontenc}
\usepackage[nohug,small]{diagrams}
\usepackage{bm}
\usepackage{faktor}
\usepackage{mathtools}


%\usepackage{genyoungtabtikz}
\usepackage{ytableau}

\usepackage{grffile}
\usepackage{tikz}
\usepackage{pgf,tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc}

\usetikzlibrary{arrows}
\topmargin 0cm
\oddsidemargin 0cm
\evensidemargin 0cm
\textwidth 16.5cm
\textheight	23.5cm
\marginparwidth 2cm
\marginparpush 2cm



\title{Dispense del corso di Teoria delle Rappresentazioni}
\author{Matematici e fisici del secondo anno, a.a. 2016/2017}
\date{\today}



\makeindex

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{post}[thm]{Postulato}
\newtheorem*{cor}{Corollario}

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{prob}{Problema}[section]
\newtheorem{exercise}{Esercizio}[section]
\newtheorem{hint}{Suggerimento}[section]
\newtheorem{sol}{Soluzione}[section]
\newtheorem*{rem}{Osservazione}

\theoremstyle{remark}
\newtheorem*{note}{Nota}





\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}
\newcommand*\quot[2]{{^{\textstyle #1}\big/_{\textstyle #2}}}
\newcommand{\dx}{\text{d}x}
\newcommand{\diff}{\text{d}}
\newcommand{\matrices}{\mathcal{M}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Bil}{Bil}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\Supp}{Supp}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\iso}{\simeq}
\DeclareMathOperator{\noniso}{\not\simeq}



\newcommand{\tridiag}[6]{
	  \begin{diagram}
	  #1 & \rTo^{#2}  & #3        \\
	     & \rdTo_{#6} & \dTo>{#4}   \\
	     &          & #5
	  \end{diagram}
}
\newcommand{\quaddiag}[8]{
	\begin{diagram}
	#1     & \rTo^{#2} & #3 \\
	\dTo<{#6} &         & \dTo>{#4} \\
	#7     & \rTo^{#8} & #5
	\end{diagram}
}
















\begin{document}
\maketitle
\tableofcontents

\newpage
\section*{Introduzione}
Qui dovremmo scrivere una introduzione, specificare quali sono le finalità e a chi sono rivolti determinati capitoli, eccetera.


\newpage
\section{Nozioni di base di teoria dei gruppi}

\subsection{Definizioni e primi risultati}

\begin{defn}[Gruppo] Un gruppo è un insieme $G$ dotato di un'operazione binaria $\cdot : G\times G \to G$ che gode delle seguenti proprietà:
\begin{enumerate}
	\item Associatività: presi comunque $a,b,c\in G$ vale che $(a\cdot b)\cdot c = a\cdot(b\cdot c)$
	\item Esiste $e\in G$, chiamato \emph{unità}, o \emph{identità}, o \emph{elemento neutro}, tale che $\forall a\in G$ vale $e\cdot a = a = a\cdot e$
	\item Per ogni $a\in G$ esiste un $a'$ tale che $a'\cdot a$ e $a\cdot a'$ sono unità, ovvero si comportano come l'elemento $e$ al punto precedente.
		  Un tale $a'$ si dice \emph{inverso} di $a$.
\end{enumerate}
Per comodità di solito si omette il puntino. Se $G$ è finito e ha cardinalità $n$, si dice che $G$ ha \emph{ordine} $n$,
e si scrive $|G|=n$ oppure $\ord(G)=n$.
\end{defn}

\paragraph{Primi esempi di gruppi}
\begin{enumerate}
	\item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}$ con l'operazione di somma.
	\item $\mathbb{Q}^*, \mathbb{R}^*, \mathbb{C}^*$ con l'operazione di moltiplicazione (dagli insiemi è stato tolto $0$).
	\item $GL_n(\mathbb{\K})$ dove $\K$ è un campo o in generale $GL(V)$ dove $V$ è uno spazio vettoriale.
	\item L'insieme delle funzioni $f:I\to I $ biunivoche, dove $I$ è un insieme e l'operazione di gruppo è la composizione. Nel caso in cui $I$ sia un insieme finito, tanto vale scegliere $I = \{1,2,3,\ldots, n\}$. In tal caso questo gruppo si indica con $S_n$.
\end{enumerate}

\paragraph{Alcune proprietà elementari}
\begin{enumerate}
	\item L'unità $e$ è unica.

	Dimostrazione: supponiamo che $e$ ed $e'$ siano entrambe unità. Allora vale
	\[e = ee' = e' \]
	\item Dato $a\in G$, l'inverso di $a$ è unico (e usualmente si denota con $a^{-1}$).

	Dimostrazione: supponiamo che $a', a''$ siano entrambi inversi di $a$. Allora
	\[(a' a)a'' = a'(aa'') \implies e a'' = a' e \implies a'' = a' \]
	
	\item Dati $a_1, a_2, \ldots, a_n$, il prodotto $a_1\cdot a_2 \cdots\cdot a_n$ è ben definito senza bisogno di parentesi.
	
	\item Se $ab = e$, allora anche $ba = e$, dunque $a$ e $b$ sono uno l'inverso dell'altro.

	Dimostrazione: $ba = bae = babb^{-1} = beb^{-1} = bb^{-1} = e$.

	\item Dato un intero positivo $k$ e un elemento $a\in G$, definiamo $a^k=\underbrace{a\cdot a \cdots\cdot a}_{k\text{ volte}}$.
	Inoltre poniamo $a^0 = e$ e infine $a^{-k} = (a^{-1})^k$, così abbiamo definito le potenze con esponente in $\Z$.
	Non è difficile dimostrare che, se $k,h$ sono interi (non necessariamente positivi), valgono le usuali proprietà:
	\[a^{k+h} = a^k \cdot a^h \quad\quad\quad (a^k)^h = a^{kh} \]
	Però non è vero in generale che $(ab)^k = a^kb^k$ (sarebbe vero se l'operazione fosse commutativa). Osserviamo infine che
	\[ (ab)^{-1} = b^{-1}a^{-1}\]
	infatti $(ab)(b^{-1} a^{-1}) = a(bb^{-1})a^{-1} = aea^{-1} = aa^{-1} = e$.
\end{enumerate}



\begin{defn}[Sottogruppo]
Sia $G$ un gruppo, $H\subseteq G$ si dice sottogruppo di $G$ se:
\begin{itemize}
	\item $e\in H$
	\item $x,y\in H \implies xy\in H$
	\item $x\in H \implies x^{-1}\in H$
\end{itemize}
e si indica $H \leq G$. Si verifica facilmente che queste condizioni sono quelle necessarie (e sufficienti) a fare in modo che $H$ sia esso stesso un gruppo con l'operazione ereditata da $G$.
\end{defn}

\begin{exmp}[Sottogruppo generato da un elemento]
Sia $G$ un gruppo e $a$ un suo elemento. L'insieme delle potenze di $a$, ovvero $\{a^k | k\in\Z\}$, è un sottogruppo di $G$,
che di solito viene denotato con $\langle a\rangle$.
\end{exmp}

\begin{rem}
Se $G$ è gruppo, $a\in G$ ed esiste un intero $n>0$ tale che $a^n=e$, allora tutti gli elementi di $\langle a\rangle$ sono della forma
$a^k$ per qualche $0\le k < n$. Infatti se si considera un qualsiasi $a^s$ con $s\in\Z$, si può scrivere $s=nq+r$ con $0\le r<n$.
Allora \[a^s=a^{nq+r} = (a^n)^qa^r = e^qa^r = a^r\]
Se $n$ è il minimo intero positivo tale che $a^n=e$, allora si dice che $a$ ha \emph{ordine} $n$. In tal caso è facile verificare che
l'insieme $\langle a\rangle$ contiene esattamente $n$ elementi distinti, ovvero $a^0, a^1, \dots a^{n-1}$.
Infatti, se fosse $a^i=a^j$ con $0\le i<j<n$, allora $a^{j-i}=e$, che sarebbe assurdo siccome $0<j-i<n$.

Se $\langle a\rangle$ è finito (il che è certo se ad esempio $G$ è finito) allora di sicuro esiste $n>0$ tale che $a^n=e$. Infatti basta prendere $0\le i < j$
tali che $a^i = a^j$ e osservare che $a^{j-i} = e$. Questi $i$ e $j$ esistono per forza perché se tutte le potenze fossero distinte allora $\langle a\rangle$ sarebbe infinito.
\end{rem}


\begin{defn}[Laterale]
	Sia $G$ un gruppo e $H<G$ un suo sottogruppo: definiamo \emph{laterale destro} o \emph{classe laterale destra} di $H$
	un sottoinsieme di $G$ del tipo
	\[gH=\{ gh\ |\ h\in H\}\]
\end{defn}

\begin{rem}
	Dato un gruppo $G$ e un sottogruppo $H<G$, tutte le classi laterali di $H$ hanno la stessa cardinalità di $H$.
	Consideriamo infatti una classe laterale $gH$ dove $g$ è un certo elemento di $G$. Allora l'applicazione $\phi_g:H\to gH$
	tale che $\phi_g(h) = gh$ per ogni $h\in H$ è certamente surgettiva. Se fosse $\phi_g(h_1) = \phi_g(h_2)$ allora $gh_1 = gh_2$ e moltiplicando
	a sinistra per l'inverso di $g$ si otterrebbe $h_1 = h_2$. Quindi $\phi_g$ è iniettiva e in conclusione è bigettiva.
\end{rem}

\begin{defn}[Quoziente]\label{defn:quoziente}
	Sia $G$ un gruppo e $H$ un suo sottogruppo: chiamiamo \emph{quoziente} di $G$ per $H$ l'insieme delle classi laterali di $H$, che indicheremo con $G/H$, ovvero
	\[G/H=\{gH\ |\ g\in G\}\]
\end{defn}

\begin{thm}
	Il quoziente di un gruppo $G$ per un suo sottogruppo $H$ fornisce una partizione di $G$: per ogni $g\in G$ esiste un unico $H$-laterale destro $g'H$ tale che $g\in g'H$.
\end{thm}
\begin{proof}
	L'esistenza è facile: si osserva che $g\in gH$ visto che $gH=\{gh\ |\ h\in H\}$ e che $e\in H$.
	Supponiamo ora che $g\in \alpha H$, dunque $g=\alpha h_1$ per qualche $h_1$. Si osserva allora che i due laterali coinciderebbero:
	\[\alpha H=\{ \alpha h\ |\ h\in H\} = \{ \alpha h_1 h\ |\ h\in H \} = \{ gh\ |\ h\in H\} = gH\]
\end{proof}

Da quanto abbiamo visto segue subito il seguente risultato:
\begin{thm}[Teorema di Lagrange]
	Sia $G$ un gruppo finito, $H<G$, allora $|H|$ divide $|G|$ e, in particolare, $\displaystyle |G/H|=\frac{|G|}{|H|}$; il numero $|G/H|$ viene chiamato \emph{indice} di $H$ in $G$.
\end{thm}


\begin{defn}[Sottogruppo normale]
Sia $G$ un gruppo, $H \leq G$ si dice \emph{normale} in $G$ se
\[\forall h\in H, \forall g\in G\qquad ghg^{-1}\in H\]
e si indica $H \trianglelefteq G$.
\end{defn}

\begin{defn}[Gruppo quoziente]
	Sia $G$ gruppo, $H\trianglelefteq G$ (osservare che si richiede che il sottogruppo sia \textit{normale}), allora chiameremo \textit{gruppo quoziente} di $G$ su $H$ l'insieme quoziente come l'abbiamo definito \eqref{defn:quoziente} munito della seguente operazione:
	\[(g_1H)\cdot(g_2H)=g_1g_2H\]
	Lasciamo come esercizio la dimostrazione del fatto che tale operazione è ben definita, ovvero non dipende dai ``rappresentanti'' $g_1,g_2$, e che rispetta effettivamente gli assiomi di gruppo. In particolare è essenziale il fatto che $H$ sia normale per dimostrare la buona definizione dell'operazione.
\end{defn}


\begin{defn}[Classi di coniugio]
Sia $G$ un gruppo, $x \in G$, la classe di coniugio di $x$ è l'insieme $\{ gxg^{-1} | g\in G \}$. Si dimostra facilmente che le classi di coniugio di tutti gli elementi di $G$ formano una partizione del gruppo stesso. Si osserva inoltre che un sottogruppo è normale se e solo se è unione di classi di coniugio (ma in generale non è vero che unendo delle classi di coniugio si ottiene un sottogruppo).
\end{defn}
\begin{exmp}[Le classi di coniugio di $GL_n(\C)$]
Nel caso del gruppo $GL_n(\C)$ due matrici stanno nella stessa classe di coniugio se e solo se sono simili, quindi per ogni classe di coniugio esiste un rappresentante canonico che è la forma di Jordan di una qualsiasi matrice nella classe (con opportune convenzioni sull'ordine dei blocchi e degli autovalori).
\end{exmp}

\begin{defn}[Centro di un gruppo]
	Sia $G$ un gruppo, il \emph{centro} di $G$ si indica con $Z(G)$ ed è il sottoinsieme degli elementi che commutano con tutto $G$:
	\[Z(G)=\{ h\in G\ |\ hg=gh\ \forall g\in G \}\]
	\`E immediato verificare che $Z(G)$ è un sottogruppo normale di $G$.
\end{defn}

\begin{defn}[Prodotto diretto di gruppi]
Siano $G$ e $H$ gruppi. Si definisce prodotto diretto di $G$ e $H$ il gruppo formato dall'insieme $G \times H = \{ (g, h) | g \in G, h \in H\}$ con l'operazione componente per componente, ovvero separatemente per i due gruppi di partenza.
\end{defn}


\begin{defn}[Omomorfismo (isomorfismo) di gruppi]
Siano $G$ ed $H$ gruppi, un'applicazione $\varphi:G\to H$ si dice \textit{omomorfismo di gruppi} se
\[\forall g_1,g_2\in G\qquad \varphi(g_1 g_2)=\varphi(g_1)\varphi(g_2)\]
dove la prima moltiplicazione è fatta in $G$ mentre la seconda in $H$.
Se $\varphi$ è bigettiva, allora si dice \emph{isomorfismo}, e i due gruppi si dicono \emph{isomorfi} (in tal caso si scrive $G\iso H$).
Indichiamo con $\Hom(G,H)$ l'insieme degli omomorfismi da $G$ ad $H$.
\end{defn}

Due gruppi isomorfi sono a tutti gli effetti ``lo stesso gruppo'', nel senso che dall'uno all'altro cambiano
solo i simboli con cui indichiamo gli elementi, e il modo con cui passare da una ``scrittura'' all'altra è dato dall'isomorfismo.

\begin{exercise}
	Dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, $f(e_G) = e_H$ dove $e_G,e_H$ sono le unità dei rispettivi gruppi.
\end{exercise}

\begin{defn}
	Siano $G$ e $H$ gruppi, $f:G\to H$ un omomorfismo di gruppi, allora definiamo il nucleo (kernel) e l'immagine di $f$ rispettivamente come:
	\begin{gather*}
		\Ker f = \{g\in G\ |\ f(g)=e_H\}\\
		\Imm f =\{ h\in H\ |\ \exists g\in G\text{ tale che }f(g)=h\}
	\end{gather*}
	dove $e_H$ è l'unità di $H$.
	Non è difficile verificare che sia $\Ker f$ che $\Imm f$ sono sempre sottogruppi rispettivamente di $G$ e di $H$.
\end{defn}
\begin{exercise}
	Dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, $\Ker f$ è un sottogruppo normale di $G$.
\end{exercise}
\begin{exercise}
	Dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, esso è \emph{iniettivo} se e solo se $\Ker f = \{e\}$ dove $e\in G$ è l'elemento neutro.
\end{exercise}


\begin{thm}[Primo teorema di omomorfismo]\label{alg:primo_teo_omo}
	Dati due gruppi $G$ e $H$ e un omomorfismo $f:G\to H$, vale che
	\[ G/\Ker f \iso \Imm f\]
\end{thm}
\begin{proof}[Cenno di dimostrazione]
	Considerare l'applicazione $\phi:G/\Ker f \to \Imm f$ definita da 
	\[\phi(g\Ker f) = f(g)\]
	Dimostrare innanzitutto che è ben definita (questo è il punto fondamentale) e poi che è un omomorfismo di gruppi. Infine, mostrare che è bigettiva.
\end{proof}
\begin{rem}
	Se la $f$ del teorema precedente è iniettiva, allora $G/\Ker f\iso G$ e quindi $G\iso \Imm f$. Invece se $f$ è surgettiva, allora $G/\Ker f\iso H$.
\end{rem}

\begin{rem}
	Ci si può chiedere se, dato un gruppo $G$ e due sottogruppi normali $H,K\trianglelefteq G$ tali che $H\iso K$, si possa concludere che $G/H\iso G/K$. Questo in generale è falso! Come esempio si può prendere $G=\Z$, $H=5\Z$ e $K=7\Z$, dove
	\[5\Z=\{ 5t|\ t\in \Z\}\qquad 7\Z=\{ 7t|\ t\in \Z\}\]
	sono i sottogruppi generati rispettivamente da $5$ e $7$.
	Infatti è facile esibire degli isomorfismi $H\iso \Z\iso K$, ma $G/H$ ha $5$ elementi mentre $G/K$ ne ha $7$, quindi non possono essere isomorfi.\footnote{Esistono controesempi anche con gruppi finiti.}
\end{rem}


\begin{defn}[Azione di un gruppo su un insieme] Sia $G$ un gruppo e $I$ un insieme. Chiamiamo azione di $G$ su $I$ una funzione $a:G\times I \to I$ che rispetti la regola di composizione, ovvero tale che se $h,g\in G$ e $i \in I$, valga
\[ a(h,a(g,i)) = a(hg, i) \]
e tale che l'elemento neutro $e\in G$ agisca ``banalmente'', ovvero $a(e, i) = i$ per ogni $i\in I$.
Normalmente si usa una notazione abbreviata in cui invece di scrivere $a(g,i)$ si scrive direttamente $g\cdot i$ o addirittura $gi$, quindi le condizioni date si riscrivono:
\begin{gather*}
	h\cdot(g\cdot i) = (hg)\cdot i\\
	e\cdot i = i
\end{gather*}
\label{defn:azione}
\end{defn}

In pratica un'azione di un gruppo $G$ su un insieme $I$ ci consente di ``interpretare'' ogni elemento $g\in G$ come una funzione $g:I\to I$, e l'operazione
di gruppo diventa la composizione delle funzioni.
\begin{exercise}
Mostrare che in realtà queste funzioni date dagli elementi di $G$ sono bigettive, e che si ha un omomorfismo di gruppi $G\to Big(I)$ dove $Big(I)$ è il gruppo delle funzioni bigettive da $I$ in sé.
\end{exercise}

\begin{defn}[Azione transitiva]
Un'azione di un gruppo $G$ su un insieme $I\neq \emptyset$ si dice \emph{transitiva} se dati comunque due elementi $i,j\in I$ esiste un elemento del gruppo $s\in G$ tale che $j=s\cdot i$.
\label{defn:azione transitiva}
\end{defn}


\begin{defn}[Orbita di un elemento]
Sia $G$ un gruppo che agisce sull'insieme $I$, dato $x\in I$ si chiama \textit{orbita} di $x$ indotta dall'azione di $G$ l'insieme $\Orb_{G}(x)=\{ g\cdot x\ |\ g\in G \}$, se il gruppo utilizzato è chiaro si può scrivere semplicemente $\Orb(x)$. Si osserva subito che un'azione è transitiva se e solo se induce una unica orbita.
\label{defn:orbita}
\end{defn}

\begin{exercise}
Le orbite indotte da un'azione del gruppo $G$ sull'insieme $I$ formano una partizione dell'insieme $I$. 
\end{exercise}

\begin{exercise}
Supponiamo di avere un gruppo $G$ che agisce su un insieme $I$. Se $x\in I$ chiamiamo \emph{stabilizzatore} di $x$ l'insieme
$\Stab(x) = \{g\in G| g\cdot x = x\}$.
Mostrare che per ogni $x$:
\begin{enumerate}
\item $\Stab(x)$ è un sottogruppo di $G$.
\item $\Orb(x)$ è in corrispondenza biunivoca con l'insieme quoziente $G/\Stab(x)$.
\end{enumerate}
Dedurre che se $G$ è un gruppo finito allora vale la formula $|\Orb(x)| \cdot |\Stab(x)| = |G|$.
\end{exercise}



\begin{rem}
	Un gruppo $G$ può agire su se stesso per coniugio, ovvero dati $g\in G$ (qui $G$ è pensato come gruppo che agisce) e $x\in G$ ($G$ pensato come insieme che ``subisce'' l'azione),
	si pone $g\cdot x = gxg^{-1}$. Non è difficile verificare che si tratta davvero di una azione.
	Osserviamo che le classi di coniugio sono le orbite degli elementi generate mediante l'azione per conugio.
\end{rem}




\begin{defn}[Azione semplicemente transitiva]
Un'azione di $G$ su un insieme $I\neq \emptyset$ si dice \emph{semplicemente transitiva}
se presi comunque $i,j\in I$ esiste un unico $s\in G$ tale che $j=s\cdot i$.
\end{defn}


\begin{defn}[Funzione $G-$equivariante]
Dato un gruppo $G$ che agisce su due insiemi $I$ e $J$, una funzione $\phi: I \to J$ si dice $G$ equivariante se
\[ \phi(s \cdot_I i) = s \cdot_J \phi(i) \qquad \forall s \in G, \ \ \forall i \in I \]
\end{defn}





\newpage
\subsection{Gruppi ciclici}

\begin{defn}[Gruppo ciclico] Un gruppo $G$ si dice ciclico se esiste un elemento $a\in G$ tale che ogni
elemento di $G$ è una potenza di $a$, ovvero $G=\langle a\rangle$. Si dice che $a$ è un generatore di $G$.
\end{defn}

\begin{exercise}
Sia $G$ un gruppo ciclico di cardinalità $n$ e generatore $a$. Allora $n$ è il più piccolo intero positivo tale che $a^n = e$,
e ogni elemento di $G$ si scrive in modo unico come $a^k$ con $0\le k < n$.
\end{exercise}

\begin{exmp}[Radici dell'unità]
Dato $n>0$ intero, l'insieme $\mu_n\subset \C^*$ delle radici $n$-esime dell'unità è un gruppo ciclico con $n$ elementi.
\end{exmp}

\begin{rem} Se $n$ è un intero positivo esiste (a meno di isomorfismo) un unico gruppo ciclico di cardinalità $n$.
Abbiamo già visto che esiste (basta considerare $\mu_n$),
inoltre dati due gruppi ciclici di cardinalità $n$ e generatori rispettivamente $a$ e $b$ è immediato costruire un
isomorfismo $f:\langle a\rangle\to\langle b\rangle$ ponendo $f(a^k) = b^k$ per $0\le k < n$.
\end{rem}


\begin{prop} Sia $C_n$ un gruppo ciclico di cardinalità $n$. Allora
\[ n = |\Hom(C_n,\C^*)|\]
\label{prop:hom_ciclici}
\end{prop}
\begin{proof} Sia $a$ un generatore di $C_n$. Fissato $\omega\in\mu_n$ posso definire
una funzione $f:C_n\to\C^*$ ponendo $f(a^k) = \omega^k$ per $0\le k < n$.
Verifichiamo che $f\in \Hom(C_n, \C^*)$. A tal fine prendiamo due elementi di $C_n$, che sono della forma $a^k, a^h$ per certi interi $0\le k,h < n$.
\[f(a^k \cdot a^h) = f(a^{k+h}) = \omega^{k+h} = \omega^k \omega^h = f(a^k)f(a^h)\]
Dunque $f$ è omomorfismo. Variando la scelta di $\omega\in\mu_n$ si producono effettivamente $n$ omomorfismi differenti (infatti se $\omega$ cambia allora cambia anche $f(a)$).
Mostriamo che non ci sono altri omomorfismi oltre a questi.
Sia $f\in \Hom(C_n,\C^*)$. Visto che $a^n=e$, deve valere $f(a)^n = f(a^n) = 1$. Allora $f(a)$ deve essere una radice $n$-esima
dell'unità, che chiamiamo $\omega$. A questo punto il fatto che $f$ è omomorfismo implica che $f(a^k) = \omega^k$ per ogni intero $k$.
\end{proof}


\subsection{Gruppi abeliani}
\begin{defn}[Gruppo abeliano] Un gruppo $G$ si dice abeliano se l'operazione di gruppo è commutativa, cioè $\quad\forall a,b\in G\quad ab=ba$.
\end{defn}

\begin{exercise} Un gruppo ciclico è sempre abeliano.
\end{exercise}

Potrebbe essere utile (ma non è strettamente necessario ai fini di questo corso) conoscere il seguente risultato, la cui dimostrazione richiederebbe una conoscenza più approfondita della teoria dei gruppi.
\begin{thm}Ogni gruppo abeliano finito è isomorfo al prodotto diretto di gruppi ciclici.
\end{thm}
Sia $G$ un gruppo abeliano. Allora usando il fatto che $G$ è isomorfo al prodotto diretto di alcuni gruppi ciclici e facendo un ragionamento simile a quello
della proposizione analoga per gruppi ciclici si ottiene che
\[ |G| = |\Hom(G,\C^*)|\]
Se invece $G$ non è abeliano allora nella formula precedente all'uguale va sostituito un $>$.
Vedremo comunque come questi risultati seguono facilmente dalla teoria delle rappresentazioni che svilupperemo.



\subsection{Gruppi simmetrici}
Il gruppo simmetrico $S_n$ è stato introdotto come l'insieme delle funzioni bigettive da $\{1,2,\dots,n\}$ in sé, dotato dell'operazione di composizione.
Dunque $S_n$ agisce in modo naturale su $\{1,2,\dots,n\}$, permutandone gli elementi. Per descrivere un elemento $\sigma \in S_n$
è spesso conveniente usare la notazione di prodotto di cicli disgiunti, che ora descriviamo informalmente.

Si comincia a costruire la lista $(1, \sigma(1), \sigma^2(1), \dots)$. Visto che abbiamo a disposizione un numero finito di elementi,
ci si convince facilmente (ma non è neanche del tutto banale) che ad un certo punto sarà $\sigma^k(1) = 1$. Allora se scriviamo $(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ tutti i numeri nella lista
saranno diversi tra loro (questo segue dal fatto che $\sigma$ è bigettiva). Inoltre ognuno dei numeri scritti viene mandato da $\sigma$
nel numero immediatamente successivo nella lista, e l'ultimo numero viene mandato nel primo. Quello che abbiamo appena scritto è un \emph{ciclo}.
\`E anche possibile che la lista sia semplicemente $(1)$, il che vorrebbe dire che $1$ viene lasciato fisso da $\sigma$.
Se avessimo cominciato il procedimento con $\sigma(1)$ al posto di $1$ avremmo ottenuto $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$, che
descrive ugualmente bene il modo in cui $\sigma$ sposta gli elementi scritti. Anche se la scrittura è diversa, per noi
$(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ e $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$ sono esattamente lo stesso ciclo,
e un ragionamento analogo vale per gli altri numeri facenti parte della lista: non importa da quale si parte.
Può darsi che non tutti i numeri da $1$ a $n$ compaiano nel ciclo appena scritto: in tal caso si prende un numero ancora non scritto e si ricomincia da capo
da lui, creando una nuovo ciclo, e si continua così finché non sono stati scritti tutti i numeri.
Alla fine ci ritroviamo con un elenco di cicli che sono necessariamente disgiunti per via della bigettività di $\sigma$.
\`E facile convincersi che in questo modo si descrive completamente $\sigma$. Inoltre a meno di variare
l'ordine con cui sono scritti i cicli e di cambiare i ``punti di partenza'' dei singoli cicli questa scrittura come cicli disgiunti è unica.
Riassumiamo quanto detto nel seguente teorema:

\begin{thm}
Ogni elemento $\sigma \in S_n$ si scrive in modo unico come prodotto di cicli disgiunti a meno dell'ordine dei fattori e a meno di
cambiare il modo in cui i singoli cicli sono presentati.
\end{thm}

Spesso nella scrittura in cicli disgiunti si tralasciano i cicli di lunghezza uno. Ad esempio $(3,5,7)(4,1)\in S_{12}$ ha perfettamente senso:
i numeri $1,3,4,5,7$ vengono ``spostati'' da $\sigma$ nel modo descritto e tutti gli altri vengono lasciati fissi.

I cicli, più che essere delle liste, vanno pensati come elementi di $S_n$, ovvero come funzioni bigettive da $\{1,\dots,n\}$ in sé.
In quanto tali possono essere moltiplicati, nel senso di composizione delle funzioni. Ad esempio
\[(1,2,3)\cdot(3,5)\]
chiaramente non è la scrittura come prodotto di cicli disgiunti di un elemento di $S_n$, in quanto appunto i due cicli scritti
non sono disgiunti. Ma il loro prodotto ha perfettamente senso, e usando la stessa convenzione
che si usa di solito per la composizione di funzioni vanno fatti ``agire'' da destra a sinistra.
Ad esempio il prodotto scritto manda il numero $5$ nel numero $1$ (infatti il ciclo a destra manda $5$ in $3$, il quale viene mandato in $1$ dal ciclo
a sinistra). Se lo volessimo scrivere come prodotto di cicli disgiunti otterremmo:
\[(1,2,3,5)\]

\begin{thm}
Ogni elemento $\sigma \in S_n$ si può scrivere come prodotto di \emph{trasposizioni}, ovvero cicli di lunghezza $2$, non necessariamente disgiunti.
\end{thm}
\begin{proof}
Considerato il teorema sulla decomposizione in cicli, basta mostrare la tesi nel caso in cui $\sigma$ è un ciclo.
Supponiamo $\sigma = (a_1, a_2, \dots, a_k)$. Allora è facile verificare che
\[\sigma = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Qualcuno potrebbe essere turbato dal caso in cui il ciclo ha lunghezza $1$, ossia $\sigma$ è l'identità.
In tal caso possiamo dire che $\sigma$ è il prodotto di un insieme vuoto di trasposizioni.
Chi fosse ancora turbato potrebbe scrivere, almeno nel caso $n\ge 2$, $\sigma = (1,2)(1,2)$.
\end{proof}

Osserviamo che il teorema precedente assicura solo l'esistenza di una scrittura come prodotto di trasposizioni
ma non l'unicità. In effetti questa non sussiste, infatti se in fondo ad un prodotto di trasposizioni si aggiunge $(1,2)(1,2)$
allora il risultato non cambia. Per avere un esempio leggermente più sofisticato:
\[(2,1)(2,3) = (1,3)(1,2)\]
Tuttavia quello che non cambia è la parità del numero di trasposizioni, come precisato dal seguente teorema.
\begin{thm}
Siano $\tau_1, \tau_2, \dots, \tau_t, \sigma_1, \sigma_2, \dots, \sigma_s$ trasposizioni in $S_n$. Supponiamo che
\[\tau_1\tau_2\dots\tau_t = \sigma_1\sigma_2\dots\sigma_s\]
Allora $s\equiv t \mod 2$.
\end{thm}
\begin{proof}[Cenno di dimostrazione]
Definiamo la seguente funzione $f:S_n\to\N$:
\[f(\rho) = |\left\{\quad(a,b)\in\{1,\dots,n\}^2 \quad | \quad a < b, \quad\rho(a) > \rho(b) \quad\right\}|\]
Non è difficile verificare che se $\tau\in S_n$ è una trasposizione allora $f(\rho)$ e $f(\tau\rho)$ hanno parità diversa.
Il risultato segue immediatamente visto che $f(\tau_1\tau_2\dots\tau_t) = f(\sigma_1\sigma_2\dots\sigma_s)$.
\end{proof}

\begin{defn}[Segno di una permutazione]
Il teorema appena visto permette di definire il \emph{segno} di ogni elemento $\sigma\in S_n$, che si pone uguale a $1$
se $\sigma$ si scrive come prodotto di un numero pari di trasposizioni, si pone uguale a $-1$ altrimenti.
\end{defn}

\begin{prop}
Il segno di un ciclo di lunghezza $k$ è esattamente $(-1)^{k-1}$
\end{prop}
\begin{proof}
Abbiamo già visto un modo in cui un ciclo di lunghezza $k$ si può scrivere come prodotto di trasposizioni:
\[(a_1, a_2, \dots, a_k) = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Dunque la tesi segue immediatamente.
\end{proof}

\begin{defn}
L'insieme degli elementi di $S_n$ aventi segno $+1$ è un sottogruppo di $S_n$,
chiamato \emph{gruppo alterno} e indicato con $A_n$.
\end{defn}


\subsection{Gruppi diedrali}

\begin{defn}[Gruppo diedrale]
Consideriamo in $\R^2$ un poligono regolare di $n$ lati con centro nell'origine.
L'insieme $D_n$ delle isometrie di $\R^2$ che mandano il poligono in sé è un gruppo con l'operazione di composizione.
Si verifica che questo gruppo ha $2n$ elementi, di cui $n$ rotazioni (ovvero elementi di $O_2(\R)$ con determinante $1$)
e $n$ riflessioni (ovvero elementi di $O_2(\R)$ con determinante $-1$).
Inoltre, detta $\rho$ una rotazione di $2\pi/n$ (che ha ordine $n$, e per inverso ha $\rho^{n-1}$) e $\sigma$ una qualunque riflessione (che ha ordine $2$), esse generano
il gruppo $D_n$, che si può presentare nel seguente modo: $$D_n=\langle\rho,\sigma|\rho^n=\sigma^2=id,\ \sigma\rho\sigma=\rho^{-1}\rangle$$
\end{defn}

\begin{rem}
 Le $n$ potenze distinte di $\rho$ sono tutte e sole le rotazioni di $D_n$, mentre gli elementi della forma $\sigma\rho^{i},\ i=0,1,..,n-1$
 sono tutte e sole le riflessioni.
\end{rem}

\begin{rem}
 Si dimostra facilmente che la relazione $\sigma\rho\sigma=\rho^{-1}$ è verificata da qualsiasi rotazione $\rho$
 e qualsiasi riflessione $\sigma$.
\end{rem}











\newpage
\section{Alcuni risultati di algebra lineare}
In questa sezione diamo alcune definizioni e teoremi di algebra lineare che sono stati utilizzati nel corso o che sono utili per avere una visione d'insieme di certi argomenti. Non saranno presenti le dimostrazioni che possono essere trovate su molti libri di algebra lineare.
\begin{defn}[Spazio vettoriale duale]
Se $V$ è un $\K-$spazio vettoriale, indichiamo con $V^*$ lo spazio duale dei funzionali lineari $f:V\to\K$.

Se $V$ è finitamente generato con base $\{v_i\}_{i=1}^n$, allora $V^*$ ammette come base $\{v_i^*\}_{i=1}^n$ (detta \emph{base duale}), dove gli elementi $v_i^*\in V^*$
sono definiti da $v_i^*(v_j) = \delta_{i,j}$.
\end{defn}

\begin{thm}[Diagonalizzazione simultanea]
\label{thm:diag_sim}
	Date due matrici $M, N\in \matrices_n(\K)$, diremo che sono \emph{simultaneamente diagonalizzabili} se esiste una base comune di autovettori per entrambe.\\
	Date $M, N\in \matrices_n(\K)$, se esse commutano e sono entrambe diagonalizzabili allora sono simultaneamente diagonalizzabili.
\end{thm}
\begin{cor}
	Date $M_1,\ldots,M_k \in \matrices_n(\K)$, se $M_iM_j=M_jM_i\ \forall\ i, j$ e ogni $M_i$ è diagonalizzabile, allora esiste una base comune di autovettori per tutte quante.
\end{cor}


\begin{defn}[Ideale di un endomorfismo]
	Se $p(x)=a_n x^n+\ldots+a_0$, allora scriviamo $p(f)$ per intendere $a_nf^n+\ldots+a_0f^0$ dove $f^0=\Id$ e $f^k=\underbrace{f\circ\ldots\circ f}_{k \text{ volte}}$.\\
	Sia $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$. Definiamo \textit{ideale di $f$} l'insieme
	\[I(f)=\left\{ p(x)\in \K[x]\ |\ p(f)=0 \right\}\]
\end{defn}


\begin{thm}[Teorema di decomposizione primaria]
\label{thm:dec_primaria}
	Siano $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$ e $q(x)\in I(f)$. Sia $q=q_1\cdot\ldots\cdot q_k$ tale che $MCD(q_i,q_j)=1\ \forall\ i\neq j$, allora $V=\Ker(q_1(f))\oplus\dots\oplus \Ker(q_k(f))$ e gli addendi sono $f$-invarianti.\\
	In particolare se $f$ è triangolabile e $\lambda_1,\ldots,\lambda_k$ sono gli autovalori di $f$ con molteplicità algebrica rispettivamente $\alpha_1,\ldots,\alpha_k$, allora $V=\Ker\left((f-\lambda_1 \Id)^{\alpha_1}\right)\oplus\dots\oplus \Ker\left((f-\lambda_k \Id)^{\alpha_k}\right)$.
\end{thm}

\begin{thm}[Forma canonica di Jordan]
	Sia $M\in \matrices_n(\K)$ una matrice triangolabile, siano $\lambda_1,\ldots,\lambda_k$ i suoi autovalori, allora $M$ è simile alla sua \textit{forma canonica di Jordan} che è nella forma
	\begin{align*}
		&\begin{pmatrix}
			J_1 & & \\
			& \ddots & \\
			& & J_t
		\end{pmatrix}
		&\text{ dove }J_i=\begin{pmatrix}
		                  	\lambda & 1 & & & \\
							& \lambda & 1 & & \\
		                  	& & \ddots & \ddots & \\
		                  	& & & \ddots & 1 & \\
		                  	& & & & \lambda
		                  \end{pmatrix} \text{ per qualche }\lambda \in \left\{ \lambda_1,\ldots,\lambda_k \right\}
	\end{align*}
	La dimensione e il numero di blocchi di ciascun tipo sono univocamente determinati dalla matrice $M$, ne segue che la forma canonica di Jordan è unica a meno di permutazione dei blocchi e dunque, scelta una convenzione sull'ordine dei blocchi, essa è un sistema completo di invarianti per similitudine: due matrici sono simili se e solo se hanno la stessa forma di Jordan.

\end{thm}
\begin{defn}[Forma hermitiana]
Siano $V,W$ due $\C$-spazi vettoriali, una funzione $h:V\times V\to W$ si dice \textit{forma hermitiana} se $\forall v,w,z\in V,\ \forall \alpha \in \C$ vale che
\begin{gather*}
	h(v,w) = \overline{h(w,v)}\\
	h(\alpha v, w) = \alpha h(v,w)\\
	h(v+z,w)=h(v,w)+h(z,w)
\end{gather*}
\end{defn}

\begin{defn}
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{definita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V, v\neq 0$ si ha che $\phi(v,v)>0$ (rispettivamente $\phi(v,v)<0$), ossarvare che $\phi(v,v)\in \R\ \forall\ v\in V$, dunque ha senso chiedere che sia maggiore o minore di $0$.\newline
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{semidefinita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V$ si ha che $\phi(v,v)\geq 0$ (rispettivamente $\phi(v,v)\leq 0$)
\end{defn}

\begin{thm}
	Ogni forma hermitiana definita positiva su uno spazio vettoriale $V$ di dimensione finita ammette una \textit{base ortonormale}, ovvero esiste una base $\{v_1,\ldots,v_n\}$ di $V$ tale che $\phi(v_i,v_j)=\delta_{ij}$.
\end{thm}




\newpage
\section{Algebra multilineare}
\subsection{Base di uno spazio vettoriale}

\begin{defn}[Base di uno spazio vettoriale]
Sia $V$ un $\K-$spazio vettoriale e $I$ un insieme; una base di $V$ è una funzione $e: I \to V$ tale che
per ogni $v \in V$ esiste un'unica funzione $a: I \to \K$ a supporto finito per cui vale $v=\sum_{i\in I}a_i e_i$.
Questa definizione è compatibile con la definizione di base come insieme di vettori generatori linearmente indipendenti.

Spesso useremo una notazione del tipo $\{e_i\}_{i\in I}$ per indicare una base di uno spazio vettoriale $V$. Ciò sottintende
una funzione $e:I\to V$ che manda $i\to e_i$, in accordo con la definizione che abbiamo appena dato.
\end{defn}

Alcuni dei risultati a cui arriveremo sono validi anche per $I$ infiniti, ma per semplicità consideriamo solo
spazi vettoriali finitamente generati (per cui esiste una base $e: I \to V$ con $I$ insieme finito).

Questo primo lemma dovrebbe essere noto a chiunque abbia un minimo di familiarità con l'algebra lineare:
\begin{lemma}
Siano $V,W$ degli spazi vettoriali, sia $e:I\to V$ una base di $V$ e $f: I \to W$ una funzione. Allora esiste un'unica $\phi: V \to W$ lineare tale che
\[\phi(e_i) = f_i \]
Inoltre $\phi$ è un isomorfismo se e solo se $f$ è una base.
\end{lemma}

\begin{lemma}
Dato $I$ insieme, esiste uno spazio vettoriale $V$ con base una certa $e:I\to V$.
\end{lemma}
\begin{proof}
Definisco il seguente insieme, che è in modo naturale un $\K-$spazio vettoriale:
\[ \K^I = \{ v:I\to\K \quad|\quad v \text{ ha supporto finito}\}\]
Ora è facile osservare che $e:I\to\K^I$ definita da $e_i(j) = \delta_{i,j}$ è una base.
\end{proof}


\subsection{Prodotto tensoriale}

\begin{defn}[Prodotto tensoriale]
   Siano $V, W$ due $\K$-spazi vettoriali. Si dice prodotto tensore di $V$ e $W$,
   e si indica come $V\otimes W$, uno spazio vettoriale con una funzione bilineare
   $\otimes: V \times W \to V\otimes W$ tale che per ogni data funzione bilineare $h: V\times W \to  Z$,
   esiste unica $\phi: V\otimes W \to Z$ lineare per cui $\phi(v \otimes w)=h(v,w)$. Ovvero questa $\phi$ fa commutare il diagramma:
   \[\tridiag{V\times W}{ \otimes }{V \otimes W}{\phi}{Z}{h}\]
   Questa proprietà viene detta proprietà universale del prodotto tensoriale e la funzione $\otimes: V \times W \to V\otimes W$
   viene detta funzione universale.
\label{defn:prodotto tensoriale}
\end{defn}

\begin{exmp} $V=W=V\otimes W = \C$ e come $\otimes$ prendo il prodotto ovvero $v\otimes w=vw$.
\end{exmp}
\begin{exmp} $W$ un qualsiasi $\K-$spazio vettoriale, $V=\K$. Allora posso prendere $\K\otimes W = W$ e come $\otimes$ prendo il prodotto per scalari $\alpha\otimes w=\alpha w$.

Infatti se $h:\K\times W\to Z$ è una funzione bilineare, allora mi basta definire $\phi:W\to Z$ ponendo $\phi(w) = h(1, w)$. Si vede facilmente
che così il diagramma commuta e non esiste nessun'altra $\phi$ che fa commutare il diagramma.
\end{exmp}

\begin{prop}
Se ho due prodotti tensoriali $V \otimes W$ e $V \overline{\otimes} W$, allora esiste un unico isomorfismo
$\phi: V \otimes W \to V \overline{\otimes} W$ tale che, fissati comunque $v\in V, w\in W$, valga
$\phi (v\otimes w) = v \overline{\otimes} w$.
\end{prop}
\begin{proof}
Considero i seguenti due diagrammi:
\[
\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ V \overline{\otimes} W }{\overline{\otimes}} \qquad
\tridiag{V\times W}{ \overline{\otimes} }{V \overline{\otimes} W}{}{V \otimes W}{\otimes}
\]
La proprietà universale per il prodotto tensore $\otimes$ dice che $\exists !\phi:V\otimes W\rightarrow V\overline{\otimes}W$ lineare che fa commutare il primo diagramma: analogamente $\exists !\psi:V\overline{\otimes}W\rightarrow V\otimes W$ lineare che fa commutare il secondo diagramma. Ora consideriamo i seguenti due diagrammi:
\[
\begin{diagram}
            &                            & V\otimes W               \\
            & \ruTo^{\otimes}            & \dTo>{\phi}              \\
 V\times W  & \rTo^{\overline{\otimes}}  &  V\overline{\otimes} W   \\
            & \rdTo_{\otimes}            & \dTo>{\psi}              \\
            &                            & V\otimes W               \\
\end{diagram}
\qquad
\begin{diagram}
            &                            & V\overline{\otimes} W    \\
            & \ruTo^{\overline{\otimes}} & \dTo>{\psi}              \\
 V\times W  & \rTo^{\otimes}             &  V\otimes W              \\
            & \rdTo_{\overline{\otimes}} & \dTo>{\phi}              \\
            &                            & V\overline{\otimes} W    \\
\end{diagram}
\]
Ma noi conosciamo già delle applicazioni lineari che fanno commutare i due diagrammi ``più grandi'':
le identità sui rispettivi spazi vettoriali. Quindi per unicità possiamo concludere che 
$\phi\psi=\Id_{V\overline{\otimes}W}, \psi\phi=\Id_{V\otimes W}$,
ovvero $\phi$ e $\psi$ sono una l'inversa dell'altra e in particolare $\phi$ è un isomorfismo.
\end{proof}

Si può dimostrare inoltre che dati due spazi vettoriali $V$ e $W$ esiste sempre
un loro prodotto tensoriale, dunque abbiamo il seguente risultato:
\begin{thm}
$V\otimes W$ esiste ed è unico a meno di isomorfismo.
\end{thm}


\begin{note}
\`E importante notare che non tutti gli elementi $z \in V \otimes W$ si scrivono come $z = v \otimes w$.
In particolare, per fare un esempio concreto che mostra che questa cosa non funziona, prendiamo $W = V^*$.
Vedremo fra poco che $V\otimes V^*$ è canonicamente isomorfo allo spazio degli endomorfismi di $V$,
i cui elementi si possono rappresentare con matrici $n\times n$.
Tuttavia gli elementi del tipo $z = v\otimes w$ avranno matrice associata di rango minore o uguale a $1$:
è chiaro che questi elementi non bastano a ``coprire'' tutto lo spazio.
\end{note}


\begin{prop}
L'insieme degli elementi di $V\otimes W$ della forma $v\otimes w$ con $v\in V, w\in W$ genera tutto lo spazio $V\otimes W$.
\end{prop}

\begin{proof} Consideriamo il sottospazio generato dagli elementi della forma $v\otimes w$:
\[\langle\{v\otimes w:v\in V,w\in W\}\rangle=A\]
Vogliamo quindi dimostrare che $A=V\otimes W$. Consideriamo il diagramma:
\[\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ A }{h}\]
dove $h(v,w)=v\otimes w$ è ovviamente un'applicazione bilineare. Quindi $\exists !\psi:V\otimes W\rightarrow A$ che fa commutare il diagramma. Detta $i:A\rightarrow V\otimes W$ l'inclusione consideriamo il seguente diagramma:
\[
\begin{diagram}
            &                       & V\otimes W               \\
            & \ruTo^{\otimes}       & \dTo>{\psi}              \\
 V\times W  & \rTo^{h}              & A   \\
            & \rdTo_{i\circ h}      & \dTo>{i}              \\
            &                       & V\times W               \\
\end{diagram}
\]
tuttavia il diagramma grande commuta anche con $\Id_{V\otimes W}$ e quindi $i\circ\psi=\Id_{V\otimes W}$ dunque $i$ è surgettiva e pertanto $A=V\otimes W$.
\end{proof}


\begin{defn}[Prodotto tensoriale di mappe lineari]
Date $f:V \to V'$ e $g:W \to W'$ funzioni lineari, si definisce prodotto tensoriale tra $f$ e $g$ l'unica funzione lineare $f \otimes g : V \otimes W \to V' \otimes W'$ tale che $(f \otimes g)(v \otimes w)=f(v) \otimes g(w)$ $\forall v\in V, w\in W$.

Una funzione con tale proprietà esiste ed è unica poiché l'applicazione $V\times W \to V'\otimes W'$ che manda $(v,w)$ in $f(v)\otimes g(w)$ è bilineare.
\end{defn}

\begin{rem}
$\Id_V \otimes \Id_W = \Id_{V\otimes W}$, poiché coincidono su un insieme di generatori.
\end{rem}


\begin{prop}
Siano $V$ e $W$ $\K-$spazi vettoriali e sia $\{e_i\}_{i\in I}$ una base di $V$. Allora ogni elemento di $V\otimes W$ si scrive in modo unico come:
\[ \sum_{i\in I} e_i \otimes w_i\]
con $w_i\in W$ tutti nulli tranne al più una quantità finita di essi.
\end{prop}
\begin{proof}
Sia $x$ un elemento di $V\otimes W$.
Visto che gli elementi della forma $v\otimes w$ generano l'intero spazio $V\otimes W$, possiamo scrivere
\[ x = \sum_{j=1}^N v_j\otimes w_j \]
scegliendo opportunamente $N\in \N$, $v_j\in V$, $w_j\in W$ per $j=1\dots N$. Visto che $\{e_i\}_{i\in I}$ è base di $V$:
\[ x = \sum_{j=1}^N v_j\otimes w_j  = \sum_{j=1}^N \left(\sum_{i\in I} a_{i,j}e_i\right)\otimes w_j = \sum_{i,j} a_{i,j}(e_i\otimes w_j) = \sum_{i\in I} e_i\otimes\tilde w_i\]
dove si è posto $\tilde w_i = \sum_j a_{i,j}e_j$.
Quindi siamo riusciti ad ottenere una scrittura del tipo che volevamo. Resta da mostrare che questa scrittura è unica,
ovvero che gli elementi $\tilde w_i$ sono univocamente determinati.

Consideriamo la base $\{e_i^*\}_{i\in I}$ di $V^*$ duale rispetto a $\{e_i\}_{i\in I}$. Fissato un $k\in I$ possiamo considerare l'applicazione
lineare $e_k^*\otimes \Id_W: V\otimes W \to \K\otimes W$. Valutandola in $x$ otteniamo:
\[ (e_k^*\otimes \Id_W) \left(\sum_{i\in I} e_i\otimes\tilde w_i \right) = \sum_{i\in I} e_k^*(e_i)\otimes\tilde w_k = 1\otimes \tilde w_k\]
Ma $\K\otimes W$ è isomorfo in modo naturale a $W$, e questo isomorfismo porta l'elemento $1\otimes \tilde w_k$ in $\tilde w_k$.
Quest'ultimo, pertanto, è univocamente determinato.
\end{proof}

\begin{prop}
Se $\{e_i\}_{i\in I}$ è una base di $V$ e $\{f_j\}_{j\in J}$ è una base di $W$ allora $\{e_i \otimes f_j\}_{(i,j)\in I\times J}$ è una base di $V \otimes W$.
\end{prop}
\begin{proof}
Dato $x\in V\otimes W$ sappiamo che si può scrivere in modo unico $x = \sum_{i\in I} e_i\otimes w_i$.
Ora, per ogni $i\in I$, il vettore $w_i$ si scrive in modo unico come $w_i = \sum_{j\in J} a_{i,j}f_j$.
Da questo segue abbastanza facilmente che $x$ si scrive in modo unico come $\sum_{i,j} a_{i,j}(e_i\otimes f_j)$.
\end{proof}

\begin{cor}
$\dim(V \otimes W) = \dim V \cdot \dim W$
\end{cor}


\begin{prop}
Siano $V$ e $W$ spazi vettoriali. Allora $V^*\otimes W$ è isomorfo allo spazio vettoriale $\Hom(V,W)$ delle applicazioni lineari da $V$ a $W$.
\end{prop}
\begin{proof}
Definiamo $\theta: V^*\times W \to \Hom(V,W)$ come la funzione che manda la coppia $(f, w)\in V^*\times W$ nella funzione  $h_{f,w}$ definita da:
$h_{f,w}(v) = f(v)w$ per ogni $v\in V$. Non è difficile osservare che $\theta$ è bilineare, quindi induce una funzione lineare
$\phi : V^*\otimes W \to \Hom(V,W)$ tale che, dati comunque $f\in V^*, w\in W, v\in V$, soddisfa $\phi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\phi$ è un isomorfismo di spazi vettoriali.

Fissiamo $\{v_i\}_{i\in I}$ una base di $V$, $\{v_i^*\}_{i\in I}$ la base duale, $\{w_j\}_{j\in J}$ una base di $W$.
Ora abbiamo
\[\phi(v_i^*\otimes w_j)(v_k) = \delta_{i,k} w_j\]
Ciò vuol dire che, se scriviamo la matrice dell'applicazione $\phi(v_i^*\otimes w_j):V\to W$ secondo le basi date, questa presenta un $1$ all'incrocio
tra l'$i-$esima colonna e la $j-$esima riga, mentre è nulla altrove.
Dunque $\phi$ manda la base $\{v_i^*\otimes w_j\}_{(i,j)\in I\times J}$ dello spazio $V^*\otimes W$ in una base dello spazio $\Hom(V,W)$,
quindi è un isomorfismo.
\end{proof}

\begin{rem}
In particolare, ponendo $W=V$, otteniamo che $\End(V)$ è isomorfo a $V^*\otimes V$.

Esiste un'applicazione in un certo senso ``naturale'' $t:V^*\otimes V \to \K$, dove $\K$ è il campo degli scalari, definita da
$t(f\otimes v) = f(v)$.
Se indichiamo con $\phi$ l'isomorfismo $V^*\otimes V\to \End(V)$ che abbiamo definito nel corso della precedente dimostrazione,
otteniamo una funzione lineare $\tr = t\circ \phi^{-1}: \End(V)\to \K$. Non è difficile vedere che questa $\tr$ che abbiamo appena definito
coincide con la classica funzione ``traccia''. Il modo in cui l'abbiamo definita noi rende evidente il fatto che la traccia non dipende
dalla base scelta per scrivere la matrice di un endomorfismo.
\end{rem}


\begin{thm}
Se $f:V\to V$ e $g:W\to W$ sono endomorfismi di spazi vettoriali, allora vale la formula
$\tr(f\otimes g) = \tr(f) \tr(g)$
\label{thm: tracciaprodotto}
\end{thm}
\begin{proof}
Sia $\{v_i\}_{i\in I}$ una base di $V$, $\{w_j\}_{j\in J}$ una base di $W$, e come al solito indichiamo gli elementi delle basi duali aggiungendo un asterisco.
\begin{align*}
 \tr(f\otimes g) &= \sum_{(i,j)\in I\times J} (v_i^*\otimes w_j^*) (f\otimes g)(v_i\otimes w_j) =\\
                 &= \sum_{(i,j)\in I\times J} v_i^*(f(v_i))\cdot w_j^*(g(w_j)) =\\
                 &= \left(\sum_{i\in I} v_i^*(f(v_i))\right) \cdot \left(\sum_{j\in J} w_j^*(g(w_j))\right) =\\
                 &= \tr(f) \cdot \tr(g)
\end{align*}
\end{proof}
\begin{proof}[Dimostrazione alternativa per $\C-$spazi vettoriali]
Iniziamo a considerare il caso in cui sia $f$ che $g$ siano diagonalizzabili: prendendo due basi $a:I\rightarrow V$ , $b:J\rightarrow W$ di autovettori rispettivamente per $f$ e per $g$, si verifica facilmente la verità della proposizione nella base indotta su $V\otimes W$ (ovvero in quella formata dagli $a_i\otimes b_j$).

Ora, essendo la traccia una funzione continua e le matrice diagonalizzabili dense nello spazio delle matrici, la proprietà affermata dal lemma si estende al caso generale per continuità.
\end{proof}

\begin{exercise}
Se $V,U,W$ sono spazi vettoriali sul campo $\K$ dimostrare che esistono (unici) i seguenti isomorfismi:
\begin{enumerate}
\item $V\otimes U \iso U\otimes V$, in modo tale che i vettori $v\otimes u$ vengano mandati in $u\otimes v$;
\item $(V\otimes U)\otimes W \iso V\otimes(U\otimes W)$ con $(v\otimes u)\otimes w$ che viene mandato in $v\otimes(u\otimes w)$;
\item $(V\oplus U)\otimes W \iso (V\otimes W) \oplus (U\otimes W)$ con $(v,u)\otimes w$ che viene mandato in $(v\otimes w, u\otimes w)$.
\end{enumerate}
\end{exercise}
In particolare il punto 2 ci dice che possiamo parlare senza ambiguità dello spazio $V\otimes U \otimes W$, senza bisogno di specificare le parentesi.
In realtà potevamo definire questo spazio in modo diretto con una proprietà universale che fa corrispondere alle forme trilineari che partono
da $U\times U\times W$ applicazioni lineari da $U\otimes U\otimes W$. Come esercizio è utile formulare per bene questa proprietà universale 
e dimostrare che $(V\otimes U)\otimes W$ la soddisfa. Allo stesso modo si possono trattare prodotti tensoriali di un numero arbitrario di spazi vettoriali.
\begin{exercise}
Siano $V,W$ spazi vettoriali. Allora $V^*\otimes W^*$ è isomorfo allo spazio vettoriale delle forme bilineari $V\times W\to \K$.
\end{exercise}


\subsection{Prodotto esterno e prodotto simmetrico}
In questa sezione supponiamo che il campo degli scalari di ogni spazio vettoriale sia $\C$, o comunque un campo a caratteristica $0$.


\begin{defn}[Applicazione $n-$lineare simmetrica/alternante]
 Una applicazione $\phi: V^n \to Z$ si dice $n-$lineare se è lineare in ogni componente dopo aver fissato le altre $n-1$.

 Inoltre $\phi$ si dice simmetrica se $\phi(v_{s(1)},\ldots,v_{s(n)})=\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$, mentre si dice
 alternante se $\phi(v_{s(1)},\ldots,v_{s(n)})=\mathrm{sgn}(s)\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$.
\end{defn}

\begin{exercise}
  Un'applicazione $n-$lineare $h: V^n \to W$ è alternante se e solo se, presi comunque $v_1,\dots,v_n\in V$
  non tutti distinti tra loro, si ha che $h(v_1,\dots,v_n)=0$.
\end{exercise}

\begin{exercise}
  Sia $h: V^n \to W$ un'applicazione $n-$lineare alternante. Se i vettori $v_1,\dots,v_n$ sono linearmente dipendenti allora
  $h(v_1,\dots,v_n)=0$.
\end{exercise}


\begin{defn}[Prodotto esterno]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto esterno è uno spazio vettoriale indicato con $\bigwedge^n V$
dotato di una funzione $n-$lineare alternante $\wedge: V^n \to \bigwedge^n V$ che manda le $n-$uple $(v_1,\ldots,v_n)$ in
elementi di $\bigwedge^n V$ che indichiamo con $v_1\wedge v_2\wedge\ldots\wedge v_n$,
tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare alternante,
esiste un'unica $\phi: \bigwedge^n V \to Z $ lineare per cui vale $\phi(v_1\wedge v_2\wedge \ldots \wedge v_n)=h(v_1,\ldots,v_n)$
per ogni $n-$upla di vettori di $V$.
\label{defn:prodotto esterno}
\end{defn}

\begin{defn}[Prodotto simmetrico]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto simmetrico è uno spazio vettoriale indicato con $S^n V$
dotato di una funzione $n-$lineare simmetrica $V^n \to S^n V$ che manda $(v_1,\ldots,v_n)$ in
$v_1 v_2\ldots v_n \in S^n V$, tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare simmetrica,
esiste un'unica $\phi: S^n V \to Z $ lineare per cui vale $\phi(v_1 v_2 \ldots v_n)=h(v_1,\ldots,v_n)$.
\label{defn:prodotto simmetrico}
\end{defn}


\begin{prop}[Proprietà dei prodotti esterni e simmetrici]
Sia $V$ uno spazio vettoriale e $n$ un numero naturale.
\begin{itemize}
\item $\bigwedge^nV$ e $S^nV$ esistono.
\item $\bigwedge^nV$ e $S^nV$ sono unici a meno di un unico isomorfismo.
\item $\bigwedge^nV$ è generato dai vettori del tipo $v_1\wedge \dots \wedge v_n$, e analogamente
      $S^nV$ è generato dai vettori del tipo $v_1\dots v_n$.
\end{itemize}
\end{prop}
\begin{proof}
Le dimostrazioni sono del tutto analoghe a quelle viste per il prodotto tensore, e vengono lasciate per esercizio.
\end{proof}

\begin{defn}[Potenza simmetrica e potenza esterna di un'applicazione lineare]
Sia $f: V\to V$ un endomorfismo di uno spazio vettoriale. Definiamo
$\bigwedge^k f : \bigwedge^k V \to \bigwedge^k V$ la funzione che manda $v_1 \wedge \ldots \wedge v_n$ in $f(v_1) \wedge \ldots \wedge f(v_n)$.

In modo analogo si definisce la potenza simmetrica (verificare, tramite le proprietà universali, che queste definizioni hanno senso).
\end{defn}


\`E facile osservare che se $V$ è uno spazio vettoriale allora $\bigwedge^k \Id_V = \Id_{\bigwedge^kV}$,
e che date due applicazioni lineari $f:V\to W$ e $g:W\to Z$ si ha che $\bigwedge^k(g\circ f) = \bigwedge^kg\circ\bigwedge^kf$.
Chiaramente valgono osservazioni analoghe per i prodotti simmetrici.

\begin{thm}[Base del prodotto esterno]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme
\[E=\{e_{i_1} \wedge e_{i_2}\wedge \ldots \wedge e_{i_k} \quad|\quad 1 \leq i_1 < i_2 <\ldots< i_k \leq n\}\]
è una base di $\bigwedge^k V$ di cardinalità $|E|= \binom {n}{k}$.
\label{thm:prodotto esterno}
\end{thm}
\begin{proof}
\`E facile osservare che l'insieme $E$ genera lo spazio $\bigwedge^k V$. Mostriamo ora che gli elementi di $E$ sono indipendenti.
Fissiamo degli indici $1 \leq i_1 < i_2 <\ldots< i_k \leq n$.
Per prima cosa osserviamo che la funzione $h:V^k\to\C$ definita da
\[ h(v_1,\dots,v_k) = e_{i_1}^*(v_1)\dots e_{i_k}^*(v_k) \] 
è $k-$lineare. Vorremmo in qualche modo renderla alternante, e per farlo usiamo lo stratagemma seguente.
Presa una qualsiasi permutazione $s\in S_k$
abbiamo che $h_s(v_1,\dots,v_k) = sgn(s)h(v_{s(1)}, \dots, v_{s(k)})$ è una funzione $k-$lineare. Consideriamo la somma
\[ H(v_1,\dots, v_k) = \sum_{s\in S_k} h_s(v_1,\dots,v_k) \]
Non è difficile convincersi che $H$ è alternante. Inoltre, se prendiamo $1 \leq j_1 < j_2 <\ldots< j_k \leq n$ vale la seguente proprietà:
\[H(e_{j_1}, \dots, e_{j_k}) = \begin{cases}
1 \qquad\text{ se } (j_1,\dots j_k) = (i_1, \dots i_k)\\
0 \qquad\text{ altrimenti}
\end{cases}\]
Con la proprietà universale del prodotto alternante troviamo un funzionale che vale $1$ su $e_{i_1} \wedge e_{i_2}\wedge \ldots \wedge e_{i_k}$
e $0$ su gli altri elementi di $E$. Questo dimostra che gli elementi di $E$ sono indipendenti.
\end{proof}

Osserviamo a questo punto che se $V$ è uno spazio vettoriale con base $\{e_i\}_{i=1}^n$, allora 
$\bigwedge^kV = 0$ se $k>n$, mentre $\bigwedge^nV\neq 0$. Questo fatto può essere usato
come dimostrazione dell'invarianza della cardinalità di una base per uno spazio vettoriale.

La potenza esterna permette di dare una definizione di determinante di un endomorfismo in un certo
senso più intrinseca di quella che si vede di solito nei corsi di algebra lineare.
Supponiamo infatti di avere un endomorfismo $f:V\to V$. Se $V$ ha dimensione $1$ allora $f$ non
è altro che la moltiplicazione per uno scalare, che chiamiamo $\det f$. Se invece $V$ ha dimensione $n$,
sappiamo che $\dim\bigwedge^nV=1$, quindi poniamo $\det f = \det(\bigwedge^nf)$.
Come si vede, questa definizione non dipende da nessuna base fissata per lo spazio $V$.
Con la definizione tramite potenza esterna risulta immediata la formula di Binet $\det (f\circ g) = \det f \cdot \det g$, in quanto
sappiamo che $\bigwedge^n(f\circ g) = \bigwedge^nf \circ \bigwedge^ng$.
\begin{exercise}
$v_1\wedge \dots \wedge v_k \neq 0$ se e solo se $v_1,\dots,v_k$ sono linearmente indipendenti.
Dedurre che per un endomorfismo $f:V\to V$ si ha che $\det f\neq 0$ se e solo se $f$ è isomorfismo.
\end{exercise}
\begin{exercise}
Dimostrare l'equivalenza tra la classica definizione di determinante e la definizione tramite potenza esterna.
\end{exercise}

\begin{lemma}
Sia $f:V\to W$ un'applicazione lineare e $k$ un intero positivo. Allora:
\begin{enumerate}
\item se $f$ è surgettiva allora $\bigwedge^k f:\bigwedge^kV\to \bigwedge^kW$ è surgettiva;
\item se $f$ è iniettiva allora $\bigwedge^k f:\bigwedge^kV\to \bigwedge^kW$ è iniettiva.
\end{enumerate}
\end{lemma}
\begin{proof}
Dimostriamo separatamente i due punti.
\begin{enumerate}
\item Sia $w_1\wedge\dots\wedge w_k\in \bigwedge^kW$ e supponiamo $f(v_i) = w_i$ (tali $v_i$ esistono per surgettività di $f$).
Allora $\bigwedge^kf(v_1\wedge\dots\wedge v_k) = w_1\wedge\dots\wedge w_k$. Visto che l'immagine contiene un insieme di generatori, allora $\bigwedge^kf$ è surgettiva.
\item Sia $\{e_i\}_{i=1}^n$ base di $V$. Allora $f(e_1),\dots,f(e_n)$ sono indipendenti in $W$ e dunque si possono completare ad una base di $W$.
Ora si vede chiaramente che la base di $\bigwedge^kV$ introdotta nel teorema \ref{thm:prodotto esterno} viene mandata iniettivamente in una parte
della base di $\bigwedge^kW$.
\end{enumerate}
\end{proof}
\begin{prop}
Sia $f:V\to W$ lineare e consideriamo una sua potenza esterna $\bigwedge^kf$. Allora $\bigwedge^kf=0$ se e solo se $k>rango(f)$.
\end{prop}
\begin{proof}
Sia $Z\subseteq W$ l'immagine di $f$. Allora posso scrivere $f = h\circ g$, dove $g:V\to Z$ è surgettiva (non è altro che $f$, con codominio ristretto)
e $h:Z\to W$ è l'inclusione (quindi iniettiva). Passando alle potenze esterne abbiamo $\bigwedge^kf = \bigwedge^k h \circ \bigwedge^k g$ e per
il lemma precedente $\bigwedge^k g$ è surgettiva mentre $\bigwedge^k h$ è iniettiva.
Dunque $\bigwedge^kf = 0$ se e solo se $\bigwedge^kZ=0$ da cui si conclude.
\end{proof}

\begin{thm}[Base del prodotto simmetrico]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme
\[E=\{e_{i_1}e_{i_2}\ldots e_{i_k} \quad|\quad 1 \leq i_1 \leq i_2 \leq\ldots\leq i_k \leq n\}\]
è una base di $S^k V$ di cardinalità $|E|= \binom {n+k-1}{k}$.
\label{thm:prodotto simmetrico}
\end{thm}
\begin{proof}
\`E simile a quella per il prodotto esterno, quindi viene lasciata per esercizio.
\end{proof}

Sia $V$ uno spazio vettoriale di dimensione $n$. Allora sappiamo che:
\begin{itemize}
\item $\dim V\otimes V = n^2$
\item $\dim S^2V = \frac{n(n+1)}{2}$
\item $\dim \bigwedge^2V = \frac{n(n-1)}{2}$
\end{itemize}
Possiamo definire in modo abbastanza naturale un'applicazione lineare $V\otimes V\to \bigwedge^2V\oplus S^2V$,
mandando il vettore $v\otimes w$ nella coppia $(v\wedge w, vw)$. In effetti questa applicazione è un isomorfismo.
\begin{proof}
Per uguaglianza delle dimensioni degli spazi di partenza e di arrivo, basta dimostrare che è surgettiva.
Per ogni coppia $v\in V$, $w\in W$ abbiamo che:
\[\frac{v\otimes w + w\otimes v}{2} \to (0, vw)\]
\[\frac{v\otimes w - w\otimes v}{2} \to (v\wedge w, 0)\]
Quindi $\bigwedge^2V$ e $S^2V$ sono contenuti nell'immagine. Ma da questo segue subito la tesi.
\end{proof}


\begin{exercise}
Sia $V$ un $\C-$spazio vettoriale. Allora $S^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari simmetriche $V\times V\to \C$.
\end{exercise}
\begin{exercise}
Sia $V$ un $\C-$spazio vettoriale. Allora $\bigwedge^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari antisimmetriche $V\times V\to \C$.
\end{exercise}


\begin{prop}
Sia $f: V \to V$ un endomorfismo di uno spazio vettoriale. Allora vale
\[
\begin{cases}
\tr(\bigwedge^2 f ) = \dfrac{(\tr(f))^2 - \tr(f^2)}{2} \\
\tr(S^2 f ) = \dfrac{(\tr(f))^2 + \tr(f^2)}{2} \\
\end{cases}
\]
\label{thm:tracciasymalt}
\end{prop}

\begin{proof} Sia $\{e_1,\ldots,e_n\}$ una base di $V$, allora $\exists a_{ij}: f(e_j)=\sum_i a_{ij}e_i$. Se $i<j$
\begin{gather*}
\bigwedge^2f(e_i\wedge e_j)=f(e_i)\wedge f(e_j)=\left(\sum_k a_{ki}e_k\right)\wedge\left(\sum_l a_{lj}e_l\right)=\\
=\sum_{k,l}a_{ki}a_{lj}(e_k\wedge e_l)=\sum_{k<l}(a_{ki}a_{lj}-a_{li}a_{kj})(e_k\wedge e_l)\\
\Rightarrow \tr(\bigwedge^2f)=\sum_{i<j}(a_{ii}a_{jj}-a_{ij}a_{ji})=\frac{1}{2}\sum_{i,j}(a_{ii}a_{jj}-a_{ij}a_{ji})= \dfrac{(\tr(f))^2 - \tr(f^2)}{2}
\end{gather*}
Per $S^2f$ si dimostra in maniera analoga.
\end{proof}













\newpage
\section{Prime proprietà delle rappresentazioni}
\subsection{Definizione e primi esempi}
\begin{defn}[Rappresentazione]
	Sia $G$ un gruppo e $V$ uno spazio vettoriale. Una funzione $\rho: G \to GL(V)$ che manda ciascun elemento del gruppo in un'applicazione lineare invertibile di $V$
	si dice rappresentazione di $G$ se è un omomorfismo di gruppi. Quindi deve valere
	\[ \rho(st) v = \rho(s)\rho(t) v \qquad \forall v \in V, \quad \forall s,t \in G\]
\end{defn}

La dimensione di $V$ (che supporremo sempre finita) viene detta grado della rappresentazione.
Per il momento consideriamo solo rappresentazioni su spazi vettoriali complessi, ovvero supponiamo che $V$ sia $\C-$spazio vettoriale.

Con un piccolo abuso di linguaggio, spesso indicheremo con il nome di ``rappresentazione'' direttamente $V$, se è chiaro quale sia
la rappresentazione $\rho$ che stiamo considerando. Spesso indicheremo gli spazi vettoriali
su cui è definita una rappresentazione $\rho$ con nomi del tipo $V_\rho$, dove il pedice ci ricorda la rappresentazione che stiamo considerando.

Detto in altri termini, una rappresentazione di un gruppo $G$ è un'azione lineare di $G$ su uno spazio vettoriale $V$.
Per allegerire la notazione è comodo usare una scrittura del tipo $g\cdot_\rho v$ in luogo di $\rho(g)v$.
Se non c'è ambiguità (cioè se è chiaro quale rappresentazione stiamo considerando)
possiamo anche omettere $\rho$ e scrivere semplicemente $g\cdot v$ oppure $g(v)$ o anche solo $gv$.
In pratica stiamo ``interpretando'' gli elementi di $G$ come endomorfismi di uno spazio vettoriale.

\begin{rem}
$\rho(G)$ è evidentemente un sottogruppo di $GL(V)$, quindi esistono sempre inversi, potenze e valgono tutte le cose che sono vere per i gruppi.
\end{rem}

\begin{exmp}
   La rappresentazione banale, di grado qualsiasi, che manda qualsiasi elemento di $g$ nell'identità di $GL(V)$, ovvero
	\[ \rho(s ) = \Id_{V} \qquad \forall s \in G\]
\end{exmp}
\begin{exmp}
	La rappresentazione di grado $0$: come spazio vettoriale si prende $V=\{0\}$, che chiaramente ha un unico endomorfismo: l'identità.
	Quindi non si può far altro che mappare ogni elemento di $G$ in quell'unico endomorfismo: si ha in sostanza un'unica rappresentazione di grado
	$0$ che, senza molta fantasia, chiamiamo ``rappresentazione $0$'' del gruppo $G$.
\end{exmp}
\begin{exmp}
   Dato $S_n$, il segno di un elemento $s\in S_n$ è una rappresentazione di grado 1. Infatti si ha $sgn(st) = sgn(s) sgn(t)$.
\end{exmp}
\begin{exmp}
   L'azione naturale di $S_n$ sui vettori della base. Prendiamo quindi $G = S_n$ e uno spazio vettoriale di dimensione $n$, che quindi è isomorfo a $\C^n$. Prendiamo la base canonica di $\C^n$ e la indichiamo con $\{e_i\}_{i=1}^n$. Descriviamo la rappresentazione $\rho: S_n \to GL(\C^n)$ dicendo cosa fa agli elementi della base: per linearità si estenderà a tutto lo spazio.
	\[ \rho(s) e_i = e_{s(i)}\]
	Notare che in questo caso $\deg(\rho) = n$. Notiamo inoltre che se rappresentiamo nella base canonica le matrici associate a $\rho(s)$ queste matrici sono unitarie. Inoltre, ogni colonna (e anche ogni riga) contiene esattamente un 1 e tutti gli altri sono 0.

	Prendiamo come esempio $S_3$ che agisce su $\C^3$ e vediamo cosa succede.
	Una prima cosa a cui siamo interessati è capire se per caso ci sono dei sottospazi di $\C^3$ che sono invarianti rispetto
	all'azione del gruppo. La risposta in questo caso è semplice: c'è il sottospazio di dimensione $1$ generato da $(1,1,1)$ che
	viene lasciato ``indisturbato'' dall'azione di $S_3$, perché il gruppo non fa altro che scambiare le coordinate degli elementi,
	che però sono tutte uguali e quindi non cambia nulla.
	Riusciamo per caso a scrivere $\C^3 = \langle(1,1,1)\rangle \oplus W$ in  modo che anche $W$ sia invariante? La risposta è sì:
	basta considerare il sottospazio di dimensione $2$ dei vettori la cui somma delle coordinate è nulla: è chiaro che la somma rimane $0$ anche
	dopo aver permutato le coordinate a causa dell'azione di qualche permutazione di $S_3$. In questo modo abbiamo
	``scomposto'' la nostra rappresentazione di grado $3$ in due rappresentazioni di grado $1$ e $2$ rispettivamente.
	In pratica abbiamo diagonalizzato a blocchi gli endomorfismi dati dagli elementi di $S_3$.
	Osserviamo che non si può ottenere una scomposizione più fine di quella che abbiamo appena trovato, perché questo corrisponderebbe
	a diagonalizzare simultaneamente tutti gli endomorfismi dati dagli elementi di $S_3$: ciò non è possibile perché tali endomorfismi
	non commutano tutti tra di loro (invece matrici diagonali commutano).
\end{exmp}



\begin{prop}
Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\forall g \in G$ la matrice $\rho(g)$ ammette una base di autovettori in $V_\rho$, ovvero è diagonalizzabile. Inoltre, tutti gli autovalori di $\rho(g)$ sono radici $n-$esime dell'unità.

\begin{note} Per ogni matrice in generale la base è diversa, quindi le varie matrici in generale \emph{non} sono simultaneamente diagonalizzabili.
Però se $G$ è abeliano tutte le matrici $\rho(s)$ sono simultaneamente diagonalizzabili.
\end{note}
\label{prop:diagonalizzabilita rappresentazioni}
\end{prop}

\begin{proof} Se $G$ è un gruppo finito, allora se $g\in G$ esiste un intero positivo $k$ tale che $g^k = e$.
Dato che $\rho:G\to GL(V_\rho)$ è un omomorfismo, dovrà essere $\rho(g)^k = \Id$.

Visto che il polinomio minimo di $\rho(g)$ non ha radici multiple, con il teorema di decomposizione primaria \eqref{thm:dec_primaria} si mostra facilmente che $\rho(g)$ è diagonalizzabile. Inoltre da questa formula è anche evidente che tutti gli autovalori di $\rho(g)$ hanno modulo $1$ e in particolare saranno radici $k-$esime dell'unità.

Ricordiamo un teorema di algebra lineare per mostrare che se $G$ è abeliano allora
tutte le matrici $\rho(g)$ sono simultaneamente diagonalizzabili:
degli endomorfismi di uno spazio vettoriale diagonalizzabili sono simultaneamente diagonalizzabili se e solo se commutano tra loro.
\end{proof}




\begin{defn}[Omomorfismo di rappresentazioni]
Siano $\rho$ e $\sigma$ due rappresentazioni di $G$ su $V_{\rho}$ e $V_{\sigma}$ rispettivamente. Un omomorfismo di spazi vettorali $\varphi:V_{\rho}\to V_{\sigma}$ si dice \textit{omomorfismo di rappresentazioni} se
\[ \forall\ a\in G, \forall\ v\in V_{\rho}\quad \varphi(a\cdot_\rho v) = a\cdot_\sigma \varphi(v) \]
oppure equivalentemente
\[ \forall\ a\in G\quad \varphi\circ \rho(a) = \sigma(a)\circ \varphi \]
Ovvero se si fa prima agire un elemento del gruppo e poi si applica $\varphi$ si ottiene la stessa cosa che si sarebbe ottenuta
applicando prima $\varphi$ e poi facendo agire lo stesso elemento del gruppo.
\end{defn}

Indicheremo l'insieme degli omomorfismi tra due rappresentazioni di un gruppo $G$ con $\Hom_G(V_\rho, V_\sigma)$ oppure con $\Hom(\rho, \sigma)$.

\begin{defn}[Rappresentazioni isomorfe]
Due rappresentazioni si dicono \textit{isomorfe} se esiste un omomorfismo di rappresentazioni tra di loro che è anche bigettivo (un \emph{isomorfismo di rappresentazioni}).
\end{defn}

\begin{exercise}
Composizione di omomorfismi di rappresentazioni è ancora un omomorfismo di rappresentazioni.
\end{exercise}
\begin{exercise}
$\Hom_G(V_\rho, V_\sigma)$ è un sottospazio vettoriale di $\Hom(V_\rho, V_\sigma)$.
\end{exercise}


Dato un gruppo $G$, le sue rappresentazioni di grado $1$ sono per definizione
omomorfismi che vanno da $G$ all'insieme degli isomorfismi di $\C-$spazi vettoriali di dimensione $1$.
Senza perdere di generalità possiamo supporre che lo spazio vettoriale sia proprio $\C$. Dunque le
rappresentazioni di grado $1$ non sono altro che omomorfismi $G\to\C^*$.
\begin{thm}
Gli omomorfismi $G\to\C^*$ sono rappresentazioni di $G$ tra loro non isomorfe.
\end{thm}
\begin{proof}
Siano $\rho:G\to\C^*$ e $\sigma:G\to\C^*$ rappresentazioni isomorfe. Allora
esiste $\varphi:\C\to\C$ isomorfismo di $\C$ (ovvero $\varphi\in\C^*$) tale che per ogni $g\in G$ vale $\varphi \rho(g) = \sigma(g) \varphi$.
Visto che $\C^*$ è commutativo abbiamo allora che $\rho(g) = \sigma(g)$ per ogni $g\in G$, il che vuol dire che
in realtà $\rho$ e $\sigma$ sono proprio la stessa rappresentazione.
\end{proof}

Nel tentativo di trovare le possibili rappresentazioni di un gruppo $G$ un buon punto di partenza è cercare per prima cosa le rappresentazioni di grado 1.
\begin{exmp}[Rappresentazioni di grado 1 di $C_n$]
Nella proposizione \ref{prop:hom_ciclici} abbiamo già studiato l'insieme degli omomorfismi $C_n\to\C^*$.
Quindi $C_n$ ha esattamente $n$ rappresentazioni di grado $1$ a meno di isomorfismo.
\end{exmp}
C'è un metodo generale per trovare le rappresentazioni di grado $1$ di un gruppo finito $G$ (indicheremo con $\rho$ la rappresentazione cercata e con $\mu_m$ il sottoinsieme di $\C$ che contiene le radici $m$-esime dell'unità):\footnote{stiamo cercando un omomorfismo di gruppi da $G$ a $GL(\C)=\C^*$.}
\begin{enumerate}
	\item cercare dei generatori per il gruppo $G$: $g_1, \ldots, g_k$
	\item per ogni generatore $g_i$ trovare il suo ordine $n_i$ (ovvero il minimo intero $n_i$ tale che $g_i^{n_i}=e$)
	\item imporre che $\rho(g_i)\in \mu_{n_i}$ per ogni $i=1,\ldots,k$
	\item imporre infine che $\rho:G\to GL(\C)$ sia veramente un omomorfismo di gruppi (per ora abbiamo solo posto delle condizioni necessarie):
	 per fare ciò bisogna controllare che le \emph{relazioni} che intercorrono tra i generatori siano rispettate nell'immagine. Questo in parole povere significa che tutte le regole con cui vengono moltiplicati gli elementi devono essere rispettate\footnote{per dare una spiegazione formale servirebbe introdurre le nozioni di prodotto libero e di presentazione di un gruppo tramite relazioni, che però non sono necessarie per questo corso.}.
	 In effetti già le condizioni imposte al punto 3 possono essere considerate come relazioni imposte dai generatori: i passi 2 e 3 sono in realtà casi particolari del passo 4.
\end{enumerate}
Questo definisce un omomorfismo da $G$ a $\C^*$: dato $h\in G$ tale che $h=g_{i_1}^{a_1}\cdots g_{i_t}^{a_t}$, allora si pone $\rho(h) = \rho(g_{i_1})^{a_1}\cdots \rho(g_{i_t})^{a_t}$.
\begin{rem}
	Non è detto che tutte le rappresentazioni che si ottengono siano non isomorfe, questo metodo solamente le produce tutte.
\end{rem}

Il procedimento descritto è abbastanza
laborioso con gruppi complicati: vediamo un altro metodo che permette di trovare alcune rappresentazioni
di grado $1$ a patto di conoscere un quoziente ciclico di $G$ e le sue rappresentazioni di grado $1$.

Sia $G$ un gruppo, $H$ un suo sottogruppo normale tale che $G/H$ sia ciclico. Allora trovando le rappresentazioni di $G/H$ di grado 1 siamo capaci di ricostruire delle rappresentazioni di grado 1 di $G$: consideriamo $\pi:G\to G/H$ la proiezione al quoziente, ovvero $\pi(g)=gH$, e un omomorfismo di gruppi $\rho:G/H\to \C^*$ (la rappresentazione di $G/H$). Allora come rappresentazione di $G$ prendiamo l'omomorfismo che fa commutare il seguente diagramma:
\[\tridiag G \pi {G/H} \rho {GL(\C)} \sigma\]
ovvero $\sigma(g)=\rho(\pi(g))$.

Questo metodo è particolarmente efficace perché, per ogni rappresentazione $\sigma:G\to GL(\C)$, l'immagine di $\sigma$ è un gruppo ciclico\footnote{Generato dall'immagine del generatore di $G$.}, quindi, in virtù del primo teorema di omomorfismo\eqref{alg:primo_teo_omo}, l'immagine di una qualsiasi rappresentazione $\sigma$ di grado 1 è un quoziente ciclico di $G$.
Questo implica che in realtà il metodo esposto è in grado di trovare \emph{tutte} le rappresentazioni di $G$ di grado $1$
(a patto di esaminare tutti i quozienti ciclici le loro rappresentazioni di grado $1$).

\begin{rem}
	Non è necessario che il grado della rappresentazione sia $1$. Sia $G$ un gruppo, $H\trianglelefteq G$ tale che conosciamo alcune rappresentazioni di $G/H$, allora riusciamo a ricostruire delle rappresentazioni di $G$ allo stesso modo: se $\rho:G/H\to GL(V)$ è una rappresentazione di $G/H$ e $\pi:G\to G/H$ è la proiezione al quoziente, allora si considera lo stesso omomorfismo $\sigma$ di prima: $\sigma(g) = \rho(\pi(g))$.
\end{rem}

\begin{exmp}[Rappresentazioni di grado 1 di $S_3$]
Conosciamo già due rappresentazioni di $S_3$ di grado 1 (ovvero omomorfismi $G\to\C^*$): quella banale, in cui ogni elemento viene mandato in $1$, 
e la rappresentazione segno, che in questo caso manda le trasposizioni in $-1$ e gli altri elementi in $1$. Mostriamo che non ce ne sono delle altre.
Sia $\rho:S_3\to\C^*$ omomorfismo di gruppi.
\[\rho((1,2,3))^{-1} = \rho((1,3,2)) = \rho((1,2))\rho((1,3)) = \rho((1,3))\rho((1,2)) = \rho((1,2,3)) \]
Quindi $\rho((1,2,3))$ vale $1$ oppure $-1$. Però $(1,2,3)^3 = (1)$, quindi necessariamente $\rho((1,2,3)) = 1$.
Visto che $\rho((1,2))^2 = \rho((1)) = 1$, deve essere $\rho((1,2)) = \pm 1$. Distinguendo i due casi si conclude che
$\rho$ è la rappresentazione banale oppure è la rappresentazione segno.
\end{exmp}


\subsection{Operazioni con le rappresentazioni}
\begin{defn}[Somma di rappresentazioni]
\label{defn:somma di rappresentazioni}
Date due diverse rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V), \ \sigma: G \to GL(W)$ si può definire la rappresentazione somma $\rho + \sigma$ sullo spazio vettoriale $V \oplus W$ nel modo ovvio
\[ g\cdot_{\rho + \sigma} (v,w) = (g\cdot_\rho v, g\cdot_\sigma w) \qquad \forall (v,w) \in V\oplus W \]
\end{defn}
Quando avremo due rappresentazioni $V$ e $W$ di un certo gruppo $G$, potremo quindi parlare della rappresentazione $V\oplus W$ intendendo ovviamente che
stiamo considerando la somma delle rappresentazioni di partenza.
Chiaramente varrà la stessa tacita assunzione per ogni operazione tra rappresentazioni che introdurremo.

Se si considera una base di $V\oplus W$ che si ottiene ``incollando'' una base di $V$ con una di $W$, matricialmente $(\rho+\sigma)(g)$ si rappresenta come
\[ [(\rho+\sigma)(g)]= \begin{bmatrix}
[\rho(g)] & 0\\
0 & [\sigma(g)]
\end{bmatrix} \]

\begin{rem}
Abbiamo le seguenti proprietà della somma di rappresentazioni:
\begin{enumerate}
\item $\deg(\rho+\sigma)=\deg(\rho)+\deg(\sigma)$
\item $\rho + \sigma \iso \sigma + \rho$
\item $\rho + (\sigma + \tau) \iso (\rho + \sigma ) + \tau$
\item Esiste l'elemento neutro che è la rappresentazione di grado 0 ma non esiste l'inverso.
\end{enumerate}
\end{rem}




\begin{defn}[Prodotto di rappresentazioni]
\label{defn:prodotto di rappresentazioni}
  Date due rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \sigma: G \to GL(V_\sigma)$ possiamo definire il prodotto di rappresentazioni che si indica con $\rho \otimes \sigma$  ma anche con $\rho\sigma$\footnote{Quest'ultima notazione può portare a confusione in quanto può essere scambiata con la composizione se le due rappresentazioni sono definite sullo stesso spazio, quindi cercheremo di evitarla.} definito sullo spazio $V_\rho \otimes V_\sigma$ tale che
  \[ g\cdot_{\rho \otimes \sigma}(v \otimes w) = (g\cdot_\rho v) \otimes (g\cdot_\sigma w) \qquad \forall v \in V_\rho, w \in V_\sigma\]
\end{defn}
La definizione ha senso perché per ogni $g\in G$ l'applicazione che manda ogni coppia $(v,w)$ nell'elemento $gv\otimes gw$ del prodotto tensoriale è bilineare.

Il seguente esercizio è molto importante e dovrebbe essere svolto con calma, prestando attenzione al fatto
che si devono esibire degli isomorfismi di rappresentazioni, non semplicemente degli isomorfismi di spazi vettoriali.
\begin{exercise}
Abbiamo le seguenti proprietà del prodotto di rappresentazioni:
\begin{enumerate}
\item $\deg(\rho\otimes\sigma) = \deg\rho \cdot \deg\sigma$
\item $1\otimes \rho \iso \rho$, dove $1$ è la rappresentazione banale di grado $1$.
\item $\rho \otimes \sigma \iso \sigma \otimes \rho$
\item $0 \otimes \rho \iso 0$
\item $\rho \otimes (\sigma \otimes \tau) \iso (\rho \otimes \sigma)\otimes \tau$
\item $\rho \otimes (\sigma_1 + \sigma_2) \iso \rho \otimes \sigma_1 + \rho \otimes \sigma_2$
\end{enumerate}
\end{exercise}




\begin{defn}[Rappresentazione duale]
\label{defn:rappresentazione duale}
Sia $\rho$ una rappresentazione di $G$ su $V$. Allora la rappresentazione duale $\rho^*$ è la rappresentazione di $G$ su $V^*$ definita da:
\[\rho^*(s)f=f\circ \rho(s^{-1}) \]
\end{defn}
La verifica che si tratti effettivamente di una rappresentazione è molto semplice è viene lasciata per esercizio.
Osserviamo che avremmo potuto equivalentemente definire la rappresentazione duale di $\rho$ come l'unica rappresentazione $\rho^*:G\to GL(V^*)$ tale che
\[ (g\cdot_{\rho^*}f)(g\cdot_\rho v)=f(v) \qquad \forall g\in G,\forall v\in V,\forall f\in V^*\]

\begin{note}
Se la matrice di $\rho(s)$ rispetto ad una fissata base di $V$ è una certa $A\in\matrices_n(\C)$, allora la matrice di $\rho^*(s)$ rispetto alla base duale
è $(A^t)^{-1}$.
\end{note}


\begin{prop}
$(\rho + \sigma)^* \iso \rho^* + \sigma^*$
\end{prop}
\begin{proof}
Consideriamo la funzione $\Theta : (V_\rho \oplus V_\sigma)^*\to V_\rho ^* \oplus V_\sigma ^*$ definita da
\[ \Theta(f) = (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) \]
per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$, dove $\imath_{V_\rho}$ e $\imath_{V_\rho}$ sono le immersioni di $V_\rho$ e $V_\sigma$ dentro la loro somma diretta.
\`E facile osservare che si tratta di un'applicazione lineare. Consideriamo anche
la funzione $\Xi: V_\rho ^* \oplus V_\sigma ^* \to (V_\rho \oplus V_\sigma)^*$ definita da
\[ \Xi(h,k) = h\circ\pi_{V_\rho} + k\circ\pi_{V_\sigma} \]
per ogni coppia di funzionali $h\in V_\rho ^*$, $k\in V_\sigma ^*$,
dove $\pi_{V_\rho}$ e $\pi_{V_\sigma}$ indicano come al solito le proiezioni sui sottospazi.
Anche $\Xi$ è lineare, inoltre è facile osservare che $\Theta$ e $\Xi$ sono una l'inversa dell'altra. Dunque sono degli
isomorfismi. Resta da mostrare che $\Theta$ è omomorfismo di rappresentazioni. Prendiamo $g\in G$: dobbiamo mostrare che
\[ (\rho^*+\sigma^*)(g) \circ \Theta = \Theta \circ (\rho + \sigma)^*(g) \]
ovvero che per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$ vale
\[ [(\rho^*+\sigma^*)(g)] (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che si riscrive:
\[ ([\rho^*(g)](f\circ \imath_{V_\rho}), [\sigma^*(g)](f\circ \imath_{V_\sigma})) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che è equivalente a:
\[ (f\circ \imath_{V_\rho} \circ [\rho(g^{-1})], f\circ \imath_{V_\sigma}\circ[\sigma(g^{-1})]) = (f\circ [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho} , f\circ[(\rho+\sigma)(g^{-1})]\circ \imath_{V_\sigma}) \]
Per ottenere la tesi basta osservare che valgono
\[ \imath_{V_\rho} \circ [\rho(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho}\]
\[\imath_{V_\sigma} \circ [\sigma(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\sigma}\]
Notiamo infine che l'isomorfismo trovato è canonico, ovvero non dipende da alcuna scelta delle basi.
\end{proof}





\subsection{Sottospazi invarianti e scomposizione delle rappresentazioni}

\begin{defn}[Sottorappresentazione]
Sia $\rho$ una rappresentazione di $G$ su $V_{\rho}$. Una sottorappresentazione di $\rho$ è un sottospazio vettoriale $W\subseteq V_{\rho}$ tale che $\rho(s)(W)\subseteq W$ per ogni $s\in G$. Infatti su tale sottospazio c'è in modo naturale una rappresentazione $\sigma:G\to GL(W)$ con $\sigma(s)=\rho(s)|_W$ (potremo scrivere $\sigma\subseteq \rho$).
\end{defn}

\begin{exmp}
Consideriamo il seguente sottoinsieme di $V_\rho$
\[V_{\rho}^G=\{v\in V_{\rho}\text{ tale che } \forall s\in G \ \ s\cdot v=v\}\]
che si verifica facilmente essere un sottospazio vettoriale. Tale sottospazio è chiaramente una sottorappresentazione:
si tratta della rappresentazione banale di $G$ di dimensione $\dim V_\rho^G$.
\end{exmp}

\begin{exmp}
Se $V$ e $W$ sono rappresentazioni di $G$, allora sono sottorappresentazioni di $V\oplus W$.
\end{exmp}

\begin{prop} Se $V_\rho$ e $V_\sigma$ sono rappresentazioni di $G$ e $f: V_\rho \to V_\sigma$ è un omomorfismo di rappresentazioni, allora $\Imm(f)$ è una sottorappresentazione di $V_\sigma$ e $\Ker(f)$ è una sottorappresentazione di $V_\rho$.
\end{prop}
\begin{proof}
Se $v\in \Ker(f)$ allora per definizione di omomorfismo di rappresentazioni $\forall s\in G$ $f(s\cdot v)=s\cdot f(v)=0$ e quindi $s\cdot v\in \Ker(f)$.
 
Allo stesso modo, se $w\in \Imm(f)$ allora $w=f(v)$ per qualche $v\in V_\rho$ e quindi sempre per la definizione di omomorfismo di rappresentazioni 
$s\cdot w=s\cdot f(v)=f(s\cdot v)\in \Imm(f)$.
\end{proof}

\begin{defn}[Rappresentazione irriducibile]
Una rappresentazione $\rho$ di $G$ è \textit{irriducibile} se
\begin{enumerate}
	\item $\rho \neq 0$ (quindi $\deg(\rho) \geq 1$)
	\item $\rho$ non ha sottorappresentazioni non banali (diverse da 0 e $V_{\rho}$).
\end{enumerate}
\end{defn}
Per esempio ogni rappresentazione di grado $1$ è irriducibile, inoltre abbiamo già incontrato
un esempio di rappresentazione di grado $2$ irriducibile quando abbiamo studiato $S_3$ che agisce su $\C^3$ permutando gli elementi di una base.

\begin{defn}[Rappresentazione completamente riducibile]
Una rappresentazione si dice completamente riducibile se si può scrivere come somma di rappresentazioni irriducibili.
\end{defn}
Attenzione al gioco di parole in italiano: una rappresentazione irriducibile è completamente riducibile. Il nome della definizione può in effetti portare a confusione.

Una cosa che vorremmo saper fare è scomporre la rappresentazione di un gruppo come somma di rappresentazioni irriducibili, se è possibile farlo. Vedremo diversi teoremi che ci aiuteranno in questo tipo di problema. Un caso molto semplice è quello considerato nella seguente proposizione.
\begin{prop}
\label{prop:rapp abeliani}
Sia $G$ un gruppo abeliano finito. Allora ogni rappresentazione di $G$ è isomorfa alla somma di rappresentazioni di grado 1.
\end{prop}
\begin{proof}
	\`E una conseguenza immediata della proposizione \eqref{prop:diagonalizzabilita rappresentazioni}: sia $\rho:G\to GL(V_{\rho})$, con $\dim V_{\rho} = n$.
	Dato che $G$ è finito gli endomorfismi $\rho(g)$ sono tutti diagonalizzabili e inoltre, visto che il gruppo è abeliano, si sfrutta l'osservazione alla fine della proposizione per dedurre che le $\rho(g)$ sono simultaneamente diagonalizzabili. A questo punto il teorema è dimostrato: sia $\{v_1,\ldots,v_n\}$ una base comune di autovettori: per ogni $1\leq i\leq n$ il sottospazio $\langle v_i\rangle$ è una sottorappresentazione di grado 1 di $G$ e si ha la scomposizione di rappresentazioni: $V_{\rho}=\langle v_1\rangle\oplus\ldots\oplus \langle v_n\rangle$.
\end{proof}
\begin{note}
Alla fine della dimostrazione abbiamo scritto $V_{\rho}=\langle v_1\rangle\oplus\ldots\oplus \langle v_n\rangle$
precisando che si tratta di una \emph{scomposizione di rappresentazioni}. In generale quando si scrive uno spazio vettoriale (su cui è definita una 
rappresentazione) come somma di altri spazi vettoriali è bene precisare se si tratta di una semplice scomposizione come spazi vettoriali oppure
di rappresentazioni, condizione molto più forte.
\end{note}

Ma è vero che ogni rappresentazione di un gruppo $G$ è completamente riducibile?
Vedremo che è vero se il gruppo $G$ è finito. Ecco invece un controesempio con $G$ infinito:
\begin{exmp} Prendiamo $G=\Z$, $V=\C^2$, e come rappresentazione
\begin{align*}
	\rho:&\Z\to GL(\C^2)\\
	&k\to M^k
\end{align*}
dove $M=\begin{pmatrix}
        	1 & 1\\
        	0 & 1
        \end{pmatrix}$ è scritta nella base canonica.\newline
Si vede subito che una sottorappresentazine è $\langle e_1\rangle$ essendo $e_1$ autovettore per ogni $\rho(k)$,
ma non esiste un suo complementare $G-$invariante: se esistesse avremmo diagonalizzato $\rho(k)\ \forall k\in \Z$, ma sappiamo che tali endomorfismi non sono diagonalizzabili.
\end{exmp}



\begin{prop}[Prodotto hermitiano invariante] Sia $G$ un gruppo finito e $V$ una sua rappresentazione. Allora lo spazio vettoriale $V$ ammette una forma hermitiana definita positiva invariante sotto l'azione di $G$, ovvero tale che $h(v,w) = h(g\cdot v, g\cdot w) \ \forall v,w\in V_\rho, \forall g \in G$.
\label{thm:esistenza hermitiana}
\end{prop}
\begin{proof}
Lo spazio $V$ ammette sicuramente una forma hermitiana definita positiva, che chiamiamo $h$. Ora andiamo a fare una sorta di media per trasformare questa forma in una invariante. Consideriamo quindi
\[h_G(v, w) := \frac{1}{|G|} \dsum_{g \in G} h(g\cdot v,g\cdot w) \]
Ora $h_G$ è ancora una forma hermitiana definita positiva, ed è abbastanza facile mostrare che è invariante sotto l'azione di $G$.
Infatti, se $s\in G$ si ha che
\[ h_G(s\cdot v, s\cdot w) = \frac{1}{|G|}\dsum_{g \in G} h(gs\cdot v , gs\cdot w)\]
Ma questo vuol dire solo \emph{eseguire la somma in un ordine diverso}. Di conseguenza $h_G$ è $G-$invariante.
\end{proof}


\begin{lemma}
Sia $V$ una rappresentazione del gruppo $G$ e sia $h: V \times V \to \C$ una forma hermitiana definita positiva e $G-$invariante.
Sia $W\subseteq V$ una sottorappresentazione. Allora $W^\perp$, l'ortogonale di $W$, è una sottorappresentazione di $V$.
\end{lemma}
\begin{proof}
Innanzitutto sappiamo che, come spazi vettoriali,
\[ V \iso W \oplus W^\perp \]
Di conseguenza un generico vettore di $V$ si potrà scrivere come somma $w_1 + w_2$, con $w_1 \in W$ e $w_2 \in W^\perp$.
Mostriamo che $W^\perp$ è una sottorappresentazione: quello che dobbiamo mostrare è che per ogni $g\in G$ si ha $g\cdot w_2 \in W^\perp$.
Dato che abbiamo un prodotto hermitiano la cosa più facile da verificare è che, fissati $w_2\in W^\perp$ e $g\in G$, il vettore $g\cdot w_2$ sia ortogonale a $W$.
Noi sappiamo che per ogni $w_1\in W$ vale $h(w_1, w_2)=0$, quindi usando che $h$ è $G-$invariante otteniamo 
\[ 0 = h(w_1, w_2) = h(g\cdot w_1, g\cdot w_2)\]
Visto che ogni $w\in W$ si può scrivere come $g\cdot w_1$ per un certo $w_1\in W$ (basta prendere $w_1 = g^{-1}\cdot w$)
abbiamo mostrato che $g\cdot w_2 \in W^\perp$.
\end{proof}

Quindi se $V$ è rappresentazione di un gruppo finito $G$ e $W\subseteq V$ è una sottorappresentazione,
i due risultati appena dimostrati garantiscono l'esistenza di una sottorappresentazione $U\subseteq V$ in somma diretta con $W$,
quindi otteniamo una scomposizione $V = W\oplus U$ (di rappresentazioni, non solo di spazi vettoriali). 

\begin{thm}
  \label{thm:gruppo finito completamente riducibile}
  Sia $G$ un gruppo finito e $\rho: G \to GL(V)$ una sua rappresentazione. Allora $\rho$ è completamente riducibile.
\end{thm}
\begin{proof}
Sia $\rho: G \to GL(V)$ una rappresentazione di $G$. Se $\rho$ è irriducibile, allora è completamente riducibile e quindi segue la tesi. Se invece esiste un sottospazio invariante $W$, allora per i lemmi precedenti esiste $Z \subset V$ tale che $V = W \oplus Z$ e tale che $Z$ sia una sottorappresentazione. Per induzione si procede fino ad ottenere la tesi.
\end{proof}




\begin{defn}[Rappresentazione per permutazioni]
Supponiamo di avere un gruppo $G$ che agisce su un insieme finito $I$.
Possiamo considerare uno spazio vettoriale $V_I$ con base $\{e_i\}_{i\in I}$ e definire una rappresentazione di $G$ ponendo
$\rho(s)e_i = e_{s\cdot i}$ per ogni $s\in G, i\in I$ ed estendendo per linearità. 
\end{defn}
Se $|I|>1$ è facile osservare che tale rappresentazione 
non è irriducibile, infatti il sottospazio generato da $\sum_{i\in I} e_i$ è $G-$invariante.
Sappiamo però che se $G$ è finito $V_I$ è una somma diretta di rappresentazioni irriducibili: vedremo tra poco qualche esempio concreto.

\begin{exercise}
\label{ex:equivariante}
Se il gruppo $G$ agisce su due insiemi finiti $I,J$ allora possiamo considerare le rappresentazioni per permutazione $V_I, V_J$.
Indichiamo i vettori di base con $\{e_i\}_{i\in I}$, $\{f_j\}_{j\in J}$ rispettivamente.
Se $\varphi:I\to J$ è una funzione, si può ``estendere'' a una funzione lineare $\phi:V_I\to V_J$ che per i vettori di base si comporta comr segue: 
$\phi(e_i) = f_{\varphi(i)}$.
Dimostrare che l'applicazione lineare $\phi$ definita in questa maniera è un omomorfismo di rappresentazioni se e solo se la funzione 
$\varphi$ è $G-$equivariante, ovvero per ogni $s\in G$ e ogni $i\in I$ vale $\varphi(s\cdot i) = s\cdot\varphi(i)$.
\end{exercise}

\begin{defn}[Rappresentazione regolare]
  Consideriamo un gruppo finito $G$, di cui possiamo considerare l'azione su se stesso per moltiplicazione a sinistra.
  La rappresentazione per permutazioni che ne risulta viene chiamata \emph{rappresentazione regolare di $G$}.
  Vedremo in seguito alcuni risultati interessanti sulla rappresentazione regolare.
  \label{defn:rappresentazione regolare}
\end{defn}

\begin{exercise}
La rappresentazione regolare di $C_n$ è isomorfa alla somma delle $n$ rappresentazioni irriducibili di grado 1 di $C_n$.
\end{exercise}

\begin{exmp}[Rappresentazione regolare di $S_3$]
Proviamo a scomporre la rappresentazione regolare di $S_3$ come somma di rappresentazioni irriducibili.
Chiamiamo $V$ lo spazio vettoriale di dimensione $|S_3|=6$ con base $\{e_{(1)}, e_{(12)}, e_{(23)}, e_{(31)}, e_{(123)}, e_{(321)}\}$
su cui abbiamo la rappresentazione regolare $\mathcal{R}:S_3\to GL(V)$.\newline
La prima osservazione interessante è che se facciamo agire $S_3$ sull'insieme $\{1,2,3\}$ nel modo naturale, allora la funzione 
da $\varphi:S_3\to\{1,2,3\}$ che manda le permutazioni $g\in S_3$ in $\varphi(g) = g(1)$ è $G-$equivariante.
Grazie all'esercizio \ref{ex:equivariante} abbiamo allora un omomorfismo di rappresentazioni $\phi:V\to V_{\{1,2,3\}}$,
dove $V_{\{1,2,3\}} = \C^3$ è una rappresentazione che abbiamo già studiato: quella in cui $S_3$ permuta gli elementi della base canonica.\newline
Definendo $K = \Ker\phi$ abbiamo ora che $K$ è una sottorappresentazione di $V$, e dalla teoria che abbiamo sviluppato sappiamo di poter scrivere
$V = K\oplus W$ come somma di rappresentazioni. Inoltre non è difficile notare che l'omomorfismo di rappresentazioni $\phi$ è surgettivo:
da questo deduciamo, considerando $\phi_{|W}$, che $W\iso V_{\{1,2,3\}}$ sono rappresentazioni isomorfe. Dunque sappiamo già come si scompone $W$:
è la somma di una rappresentazione banale di grado $1$ e una irriducibile di grado $2$, che chiamiamo $\rho$.\newline
Ora resta da scomporre $K$. Una possibile base di $K$ è data dagli elementi:
\[\begin{cases}
u_1 = e_{(1)} - e_{(23)}\\
u_2 = e_{(123)} - e_{(12)}\\
u_3 = e_{(321)} - e_{(31)}
\end{cases}\]
ed è facile verificare che il sottospazio $\langle u_1+u_2+u_3\rangle\subset K$ è $S_3-$invariante, e più precisamente è la rappresentazione
segno di $S_3$, che indichiamo con $\varepsilon$. Questa ammetterà un complementare $G-$stabile in $K$, quindi riassumendo siamo arrivati ad una scomposizione
$\mathcal{R} = 1 + \varepsilon + \rho + \sigma$, con $\sigma$ di grado $2$ (la rappresentazione complementare a $\varepsilon$ in $K$).
Lasciamo come esercizio la dimostrazione del fatto che $\sigma$ è irriducibile e che in particolare è isomorfa a $\rho$, dunque in conclusione
$\mathcal{R} = 1 + \varepsilon + 2\rho$.
\end{exmp}


\begin{lemma}
Se $\rho_1, \rho_2, \sigma$ sono rappresentazioni di $G$, si ha il seguente isomorfismo di spazi vettoriali:
\[\Hom(\rho_1 + \rho_2, \sigma) \iso \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma)\]
\end{lemma}
\begin{proof}
La dimostrazione è abbastanza naturale. Consideriamo la mappa
\begin{align*}
\Xi :  \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma) &\to \Hom(\rho_1 + \rho_2, \sigma) \\
(\phi_1 , \phi_2)                                       &\to \phi_1\circ\pi_1 + \phi_2\circ\pi_2
\end{align*}
dove $\pi_1,\pi_2$ sono le proiezioni e mostriamo che è un isomorfismo.
La prima cosa da accertare è che se $\phi_1\in \Hom(\rho_1, \sigma)$ e $\phi_2\in \Hom(\rho_2, \sigma)$ sono omomorfismi di rappresentazioni, allora
$\phi_1\circ\pi_1 + \phi_2\circ\pi_2 \in \Hom(\rho_1 + \rho_2, \sigma)$. La verifica è molto semplice e viene tralasciata.\newline
Poi si osserva che la mappa $\Xi$ è lineare (anche questa verifica è molto semplice).\newline
Per quanto riguarda la bigettività si considera la mappa
\begin{align*}
\Theta:\Hom(\rho_1 + \rho_2, \sigma) &\to \Hom(\rho_1, \sigma) \oplus \Hom(\rho_2, \sigma) \\
\phi								   &\to (\phi\circ i_1, \phi\circ i_2)
\end{align*}
dove $i_1, i_2$ sono le immersioni, che si verifica essere l'inversa di $\Xi$.
\end{proof}

Quello che segue è un lemma di facile dimostrazione ma di grande importanza, e vale la pena di formularlo 
sotto ipotesi un po' più generali di quelle che abbiamo supposto fin'ora.
\begin{thm}[Lemma di Schur]
Siano $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ due rappresentazioni irriducibili di un gruppo $G$, dove $V_\rho$ e $V_\sigma$ sono spazi vettoriali di dimensione finita su un campo $\K$ e sia $f:V_\rho \to V_\sigma$ un omomorfismo di rappresentazioni. Allora:
\begin{enumerate}
\item $f$ è un isomorfismo oppure è identicamente nullo;
\item se $\rho=\sigma$, $V_\rho=V_\sigma=V$ e $\K$ è algebricamente chiuso (per esempio $\C$ lo è), allora $f$ è una moltiplicazione per scalare.
\end{enumerate}
\end{thm}
\begin{proof}
Supponiamo $f\neq 0$. Sappiamo che $\Ker(f)\subseteq V_\rho$ è una sottorappresentazione, ma $\rho$ è irriducibile e quindi $\Ker(f)=0$.
Pertanto $f$ deve essere iniettivo. $\Imm(f)\subseteq V_{\sigma}$ è una sottorappresentazione di $\sigma$ e, non essendo nulla ed essendo $\sigma$ irriducibile, coincide con tutto $V_\sigma$, dunque $f$ è suriettivo, e in conclusione è un isomorfismo.\newline
Dimostriamo ora il secondo punto: sia $\lambda$ un autovalore di $f$, che esiste perché stiamo lavorando su un campo algebricamente chiuso.
Consideriamo $f-\lambda \Id:V\to V$, che è un omomorfismo di rappresentazioni non iniettivo, quindi per la prima parte del lemma di Schur deve essere identicamente nullo. Segue subito che $f$ è la moltiplicazione per lo scalare $\lambda$.
\end{proof}

Come importante corollario notiamo che (sotto le ipotesi di campo algebricamente chiuso) prese due rappresentazioni irriducibili $\rho$ e $\sigma$ si ha che $\dim\left(\Hom(\rho,\sigma)\right)$ è uguale a 1 se $\rho\iso\sigma$, ed è uguale a 0 se $\rho\noniso\sigma$.
Generalizzando, se $\rho$ si scompone in somma di rappresentazioni irriducibili non isomorfe come $\rho=\sum n_i\rho_i$, dove gli interi positivi $n_i$ indicano le molteplicità con cui compaiono gli irriducibili nella somma, allora $\dim\left(\Hom(\rho,\rho_i)\right)=n_i$.
Quindi si ottiene immediatamente il risultato seguente:
\begin{cor}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione e
\[\rho = \dsum_{i=1}^N n_i \rho_i \]
una sua scomposizione come somma di rappresentazioni irriducibili con $n_i>0$ interi positivi e $\rho_i\noniso\rho_j$ se $i\neq j$. Allora la scomposizione è unica, nel senso che gli $n_i$ e i $\rho_i$ sono univocamente determinati (a meno di isomorfismo).
\end{cor}

Presentiamo ora un risultato estremamente interessante che riguarda la rappresentazione regolare, che essenzialmente ci dice
che essa contiene tutte le rappresentazioni irriducibili del gruppo.
\begin{thm}
Sia $\mathcal{R}$ la rappresentazione regolare di un gruppo finito $G$ e sia
\[ \mathcal{R} = \dsum_{i=1}^Nn_i \rho_i,\]
con $\rho_i$ irriducibili e a due a due non isomorfe.
Allora ogni rappresentazione irriducibile di $G$ è isomorfa ad una delle $\rho_i$ presenti nella somma. Inoltre $n_i = \deg(\rho_i).$
\label{thm: teorema importantissimo}
\end{thm}
\begin{proof}
Per il corollario del Lemma di Schur, la tesi è equivalente al seguente fatto: se $\rho$ è una rappresentazione irriducibile di $G$, allora $\dim\left(\Hom(\mathcal{R},\rho)\right)=\deg\rho$ (in realtà nella dimostrazione non useremo l'ipotesi di irriducibilità di $\rho$). Costruiamo dunque un isomorfismo tra gli spazi vettoriali $\Hom(\mathcal{R},\rho)$ e $V_\rho$.

Chiamando $e_g$ l'elemento della base di $V_\mathcal{R}$ associato a $g\in G$, notiamo preliminarmente che, presa $\varphi\in\Hom(\mathcal{R},\rho)$, vale $\varphi(e_g)=\varphi(\mathcal{R}(g)e_{1})=\rho(g)\varphi(e_1)$, dove con $1$ si indica l'elemento neutro di $G$. Quindi, il valore di $\varphi(e_1)$ determina completamente $\varphi(e_g)$ per ogni $g$, e quindi determina completamente $\varphi$.

Sia allora $\Phi\colon\Hom(\mathcal{R},\rho)\to V_\rho$ tale che $\Phi(\varphi)=\varphi(e_{1})$. Per quanto appena visto, $\Phi$ è iniettiva. D'altra parte, lo stesso ragionamento ci permette anche di dimostrare facilmente che $\Phi$ è suriettiva: per ogni $v\in V_\rho$, la funzione $\varphi_v$ tale che $\varphi_v(e_g)=\rho(g)v$ è tale che $\Phi(\varphi_v)=\varphi_v(e_1)=\rho(1)v=v$. La linearità di $\Phi$ consente di concludere.
\end{proof}

\begin{cor}
Sia $G$ un gruppo finito. $G$ ha un numero finito di rappresentazioni irriducibili, a meno di isomorfismi.
Inoltre, indicando con $n_i$ le dimensioni delle rappresentazioni irriducibili, vale
\[|G| = \dsum n_i^2\]
\end{cor}

Nel teorema \ref{prop:rapp abeliani} abbiamo visto che ogni rappresentazione di un gruppo abeliano finito è somma di rappresentazioni di grado 1.
Nella dimostrazione avevamo usato il fatto che se abbiamo degli endomorfismi diagonalizzabili che commutano tra loro allora sono 
simultaneamente diagonalizzabili. Vediamo ora una dimostrazione che invece fa uso del lemma di Schur:
\begin{thm}
Sia $G$ un gruppo abeliano finito. Allora le sue rappresentazioni irriducibili sono tutte di grado $1$.
\end{thm}
\begin{proof}
Sia $\rho:G\to GL(V)$ una rappresentazione irriducibile. Fissiamo $g\in G$ e consideriamo l'endomorfismo $\rho(g):V\to V$.
Se si prende un qualsiasi $h\in G$ abbiamo che $\rho(g)\rho(h) = \rho(h)\rho(g)$ visto che il gruppo è abeliano.
Ma allora $\rho(g)\in\Hom(\rho,\rho)$, e per il lemma di Schur $\rho(g) = \lambda\Id$ per un certo $\lambda\in\C$.
Lo stesso ragionamento vale per qualsiasi elemento del gruppo, ma allora tutti i sottospazi di $V$ sono $G-$invarianti.
Dall'ipotesi di irriducibilità otteniamo quindi che $\dim V = 1$.
\end{proof}

\begin{cor}
Se $G$ è un gruppo finito abeliano allora ha $|G|$ rappresentazioni irriducibili di grado 1 e la rappresentazione regolare è la somma di queste.
\end{cor}

\begin{exercise}
Se $G$ è un gruppo finito non abeliano allora ha almento una rappresentazione irriducibile di grado maggiore di $1$.
\end{exercise}


\subsection{Alcuni isomorfismi notevoli}
Sia $G$ un gruppo e siano $\rho:G\to GL(V)$ e $\sigma:G\to GL(W)$ sue rappresentazioni.
Sappiamo bene che l'insieme delle applicazioni lineari da $V$ in $W$,
che denotiamo con $\Hom(V, W)$, è uno spazio vettoriale.
Quello che vogliamo fare ora è rendere tale spazio vettoriale una rappresentazione di $G$,
ovvero vogliamo definire un'azione lineare di $G$ su $\Hom(V, W)$.
Il modo naturale di farlo è il seguente:
sia $\varphi\in\Hom(V, W)$. Definiamo $g\cdot \varphi\in\Hom(V, W)$ come la funzione che fa commutare il diagramma:
\[ \quaddiag V \varphi W {\sigma(g)} W {\rho(g)} V {g\cdot \varphi} \]
Ovvero esplicitamente: $g\cdot\varphi = \sigma(g) \circ \varphi\circ \rho(g^{-1})$

La verifica che si tratti di un'azione di gruppo è molto semplice, inoltre è evidente dalla definizione che l'azione è lineare:
dunque abbiamo davvero dotato $\Hom(V, W)$ di una struttura di $G-$rappresentazione.

Sappiamo già che $\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come spazi vettoriali.
Quello che vogliamo vedere è che sono isomorfi anche come $G-$rappresentazioni.
Ovviamente la rappresentazione che si considera su $V^*\otimes W$ è la rappresentazione $\rho^*\otimes\sigma$, che per semplicità indichiamo con $\tau$.

\begin{thm}
$\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come $G-$rappresentazioni.
\label{thm: isov*w}
\end{thm}
\begin{proof}
Conosciamo già un isomomorfismo di spazi vettoriali $\Psi:V^*\otimes W \to \Hom(V, W)$, costruito in
maniera tale che per ogni $f\in V^*, w\in W, v\in V$ si abbia $\Psi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\Psi$ è omomorfismo di rappresentazioni, ovvero che fissato $g\in G$ il seguente diagramma commuta:
\[ \quaddiag {V^*\otimes W} \Psi {\Hom(V, W)} {g} {\Hom(V, W)} {\tau(g)} {V^*\otimes W} \Psi \]
Ovvero dobbiamo controllare se
\[ g\circ \Psi = \Psi \circ \tau(g)\]
Possiamo limitarci a fare le verifiche sugli elementi decomponibili di $V^*\otimes W$, in
quanto essi generano tutto lo spazio. Prendiamo dunque $f\in V^*$, $w\in W$ e consideriamo $f\otimes w$. Vogliamo mostrare che
\[ g(\Psi(f\otimes w)) = \Psi(\tau(g)(f\otimes w))\]
Quindi prendiamo $v\in V$ e vediamo dove viene mandato dalla funzione a sinistra:
\begin{align*}
         & g(\Psi(f\otimes w))\ (v)\\
=  \qquad& \sigma(g)\ \Psi(f\otimes w)\ (\rho(g^{-1})\ v)\\
=  \qquad& \sigma(g)\ f(\rho(g^{-1})\ v)\ w\\
=  \qquad& f(\rho(g^{-1})\ v)\ \sigma(g)\ w
\end{align*}
Mentre dalla funzione a destra viene mandato in:
\begin{align*}
         & \Psi(\tau(g)(f\otimes w))\ (v)\\
=  \qquad& \Psi([f\circ\rho(g^{-1})] \otimes [\sigma(g)w])\ (v)\\
=  \qquad& f(\rho(g^{-1})v)\ \sigma(g)\ w
\end{align*}
Avendo ottenuto lo stesso risultato in entrambi i casi, si ha la tesi.
\end{proof}

\begin{rem}
I vettori invarianti di $\Hom(V,W)$ sono esattamente gli omomorfismi di rappresentazioni,
ovvero $\Hom(V,W)^G = \Hom_G(V,W)$.
\end{rem}

Un'altro spazio che vorremmo dotare di una struttura di $G-$rappresentazione è quello
delle forme bilineari $\Bil(V,W) = \{f:V\times W\to \C \ \text{bilineare}\}$. La strategia non cambia molto:
data $b\in \Bil(V,W)$ si definisce $g\cdot b$ in modo che faccia commutare il diagramma:
\[ \quaddiag {V\times W} b \C \Id \C {(\rho(g),\sigma(g))} {V\times W} {g\cdot b} \]
Ovvero esplicitamente $(g\cdot b)(v,w) = b(\rho(g^{-1})v, \sigma(g^{-1})w)$.

Le verifiche per stabilire che $\Bil(V,W)$ diventa così una $G-$rappresentazione sono immediate e vengono tralasciate.
Come abbiamo visto per $\Hom(V,W)$, anche $\Bil(V,W)$ può essere visto come prodotto tensoriale di opportune rappresentazioni.
Non riportiamo la dimostrazione, che dovrebbe essere semplice da fare dopo aver visto quella per $\Hom(V,W)$.
\begin{thm}
$\Bil(V, W)$ e $V^*\otimes W^*$ sono isomorfi come $G-$rappresentazioni.
\end{thm}
Se consideriamo $\Bil(V) = \{f:V\times V\to \C \ \text{bilineare}\}$ è facile osservare che
i sottospazi delle forme bilineari simmetriche e delle forme bilineari antisimmetriche sono sottorappresentazioni,
che sommate tra loro danno l'intero spazio $\Bil(V)$.
\begin{exercise}
La sottorappresentazione delle forme bilineari simmetriche è isomorfa a $S^2V^*$.
\end{exercise}
\begin{exercise}
La sottorappresentazione delle forme bilineari antisimmetriche è isomorfa a $\bigwedge^2V^*$.
\end{exercise}
\newpage

\section{Teoria dei caratteri}
In questa sezione tutti gli spazi vettoriali si intendono di dimensione finita (sul campo $\C$) e tutti i gruppi di cardinalità finita.

\subsection{Carattere di una rappresentazione}

\begin{defn}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo $G$.
Definiamo carattere di $\rho$, e lo indichiamo con $\chi_\rho$ (omettendo il pedice se è chiaro dal contesto),
la funzione che associa ad ogni elemento $x$ del gruppo $G$ la traccia dell'endomorfismo $\rho(x)$, ovvero
\[\chi_\rho(s) := \tr(\rho(s)) \qquad \forall s \in G \]
Quindi $\chi_{\rho}: G \to \C$ è una funzione che va dal gruppo in $\C$.
\end{defn}
Seguono alcune osservazioni preliminari sul carattere.
\begin{enumerate}
	\item Se $\deg(\rho) = 1$ allora $\chi_\rho(s) = \rho(s)$
	\item Se $\rho$ è una rappresentazione banale\footnote{ovvero manda ogni elemento del gruppo nell'identità $\Id_{V_\rho}$} allora $\chi_\rho = \deg(\rho)$.\\
	Questo è vero poichè la traccia della matrice identità è uguale alla dimensione dello spazio vettoriale.
	\item $\chi_{\rho + \sigma}(s) = \chi_\rho(s) + \chi_\sigma(s)$.\\
	Questo segue subito dal fatto di poter scrivere le matrici della somma di rappresentazioni come matrici diagonali a blocchi.
	\item $\chi_{\rho\sigma}(s) = \chi_\rho(s)\chi_\sigma(s)$. In particolare dunque $\chi_{\rho^2}=(\chi_\rho)^2$.\\
	Questo deriva immediatamente dal teorema \ref{thm: tracciaprodotto}.
	\item $\chi_{\rho}(s^{-1})=\overline{\chi_{\rho}(s)}$\\
Essendo $G$ un gruppo finito, $\forall s\in G\ \rho(s)^n = id$ dove $n=|G|$: dunque tutti gli autovalori di $\rho(s)$ sono radici $n-$esime dell'unità e $\rho(s)$ è diagonalizzabile\footnote{Si veda la proposizione \ref{prop:diagonalizzabilita rappresentazioni}}. In tale base è evidente che:
$$\chi_{\rho}(s^{-1})=\tr(\rho (s^{-1}))=\tr(\rho (s)^{-1})=\sum_i\lambda_i^{-1}=\sum_i\overline{\lambda_i}=\overline{\tr(\rho(s))}=\overline{\chi_{\rho}(s)}$$
in quanto, avendo gli autovalori modulo 1, l'inverso coincide con il coniugio.
	\item $\chi_{\rho^*}(s) = \overline{\chi_{\rho}(s)}$\\
		Segue dall'osservazione precedente, ricordando che la matrice associata a $\rho^*(s)$ nella base duale è la trasposta inversa di quella associata a $\rho(s)$.
	\item $\chi_{\rho}(hsh^{-1})=\chi_{\rho}(s)$ ovvero $\chi_\rho$ è costante sulle classi di coniugio di $G$. La motivazione è semplice: se due elementi sono coniugati tra loro questo significa che le matrici corrispondenti saranno simili e la traccia è un invariante di similitudine.

Di conseguenza, non sarà necessario calcolare il carattere per ogni elemento del gruppo ma basterà farlo per le classi di coniugio di $G$.

Le funzioni che sono costanti sulle classi di coniugio di un gruppo vengono dette \emph{funzioni di classe}.
L'insieme delle funzioni di classe di un gruppo viene normalmente indicato con $Cl(G)$ e si verifica molto facilmente che esso è un sottospazio di $\mathbb{C}^G$.
	\item Supponiamo di avere un gruppo $G$ che agisce su un insieme finito $I$: allora possiamo considerare la corrispondente
	rappresentazione per permutazioni $\rho_I$. Si ha che
	$$\chi_{\rho_{I}}(s) =  |I^s|$$
	dove $I^s=\{i\in I| s\cdot i=i\}$. La veridicità di questo fatto si vede scrivendo esplicitamente la matrice che rappresenta $\rho_I(s)$.
	\item Come caso particolare del punto precedente, se $\mathcal{R}$ è la rappresentazione regolare di $G$ allora:
	\[ \chi_{\mathcal{R}}(s) = \begin{cases}
|G| \qquad &\text{se } s=e \\
0 \qquad &\text{se } s\neq e\\
\end{cases} \]
semplicemente perché $s\cdot g=g\Leftrightarrow s=e$.
\end{enumerate}

\begin{rem}
\`E importante notare che se $\rho\iso\sigma$ sono rappresentazioni isomorfe allora $\chi_\rho = \chi_\sigma$.
Infatti si può supporre $V = V_\rho = V_\sigma$ e si nota subito che per ogni $s\in G$ gli endomorfismi $\rho(s)$ e $\sigma(s)$ sono
coniugati (tramite l'isomorfismo di rappresentazioni, che è in particolare un isomorfismo di spazi vettoriali).

Il fatto sorprendente che dimostreremo tra poco è che vale anche il viceversa: se due rappresentazioni hanno lo stesso carattere allora sono isomorfe.
\end{rem}

\begin{exmp}
$G=S_3$ agisce nel modo naturale su $I=\{1,2,3\}$. Allora
\[ \chi_{\rho_I}(s) = \begin{cases}
3 \qquad \text{se } s=e \\
1 \qquad \text{se } s\ \text{è una trasposizione}\\
0 \qquad \text{se } s\ \text{è un } 3-\text{ciclo}\\
\end{cases} \]
Abbiamo già visto che $\rho_I = 1 + \rho$ con $1$ rappresentazione banale di grado $1$ e $\rho$ rappresentazione irriducibile di grado $2$.
Quindi per differenza ricaviamo il carattere di $\rho$:
\[ \chi_{\rho}(s) = \begin{cases}
2 \qquad \ \ \text{se } s=e \\
0 \qquad \ \ \text{se } s\ \text{è una trasposizione}\\
-1\qquad \text{se } s\ \text{è un } 3-\text{ciclo}\\
\end{cases} \]
\end{exmp}

Il risultato più importante sui caratteri delle rappresentazioni è la ``relazione di ortogonalità''.
Per enunciare questo risultato è comodo definire un prodotto hermitiano tra funzioni a valori complessi:
Date due funzioni $f,g:G\to\C$ si definisce
\[ \langle f | g \rangle = \dfrac{1}{|G|} \dsum_{s \in G} f(s)\overline{ g(s)} \]
\begin{thm}[Relazioni di ortogonalità]
Se $\rho$ e $\sigma$ sono rappresentazioni irriducibili di $G$,
\[\langle \chi_{\rho}|\chi_{\sigma} \rangle = \begin{cases}
1 \qquad \text{se } \rho \iso \sigma \\
0 \qquad \text{altrimenti }\\
\end{cases} \]
\label{relazione di ortogonalita}
\end{thm}
Prima di procedere con la dimostrazione è utile conoscere il seguente risultato,
in cui la rappresentazione $\rho$ non è supposta necessariamente irriducibile.
Ricordiamo che con $V^G$ si indica il sottospazio dei vettori invarianti, cioè quelli
per cui l'azione del gruppo è banale (vengono sempre lasciati fissi).
\begin{lemma}
\label{lemma:reynolds}
Sia $\rho:G\to GL(V)$ rappresentazione di un gruppo $G$. Allora $\dim V^G = \langle\chi_\rho|\chi_1\rangle$.
\label{lemma:dim_invariante}
\end{lemma}
\begin{proof}
Definiamo un'applicazione lineare $R:V\to V$ nel seguente modo:
\[ R = \frac{1}{|G|} \sum_{g\in G}{\rho(g)} \]
Se prendiamo $v\in V$ e $s\in G$ allora
\[ \rho(s) R(v) = \rho(s)\frac{1}{|G|} \sum_{g\in G}{\rho(g)v} = \frac{1}{|G|} \sum_{g\in G}{\rho(sg)v} = R(v)\]
ovvero $R(v)\in V^G$.
Inoltre se $w\in V^G$ allora $R(w)=w$. Di conseguenza $R^2 = R$ (cioè $R$ è una cosiddetta \emph{proiezione})
e quindi $R$ si diagonalizza con autovalori $0$ e $1$. Inoltre l'autospazio relativo a $1$ è proprio $V^G$.
Ciò implica che
\[ \dim V^G = \tr(R) = \frac{1}{|G|} \sum_{g\in G}{\chi_\rho(g)} = \langle\chi_\rho|\chi_1\rangle \]
\end{proof}

\begin{note}
Il proiettore $R$ sopra trovato si chiama \emph{proiettore di Reynolds}.
\end{note}

Invece di dimostrare direttamente le relazioni di ortogonalità dei caratteri mostriamo il seguente fatto più generale:
La tesi del teorema \ref{relazione di ortogonalita} seguirà quindi dal lemma di Schur, poiché il termine $\dim(\Hom (\sigma, \rho))$, nel caso in cui $\rho$ e $\sigma$ sono irriducibili, vale
$1$ o $0$ a seconda che $\rho$ e $\sigma$ siano isomorfe oppure no.

\begin{lemma}
\label{lemma:dim_hom}
Siano $\rho:G\to V_\rho$ e $\sigma:G\to V_\sigma$ rappresentazioni \footnote{non necessariamente irriducibili} di $G$. Allora vale
\[ \langle \chi_\rho | \chi_\sigma \rangle  = \dim(\Hom (\sigma, \rho))\]
\end{lemma}
\begin{proof}
L'idea principale è di ridursi al caso più facile in cui una delle due rappresentazioni è banale. Per farlo basta riscrivere
\[ \langle \chi_\rho | \chi_\sigma \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \chi_\rho(s) \overline{\chi_\sigma(s)} = \dfrac{1}{|G|} \dsum_{s\in G} {\chi_{\sigma^*}(s)} \chi_{\rho}(s) = \dfrac{1}{|G|} \dsum_{s\in G} \chi_{\sigma^*\rho}(s)  = \langle \chi_{\sigma^*\rho}  | 1 \rangle\]
Lo spazio vettoriale su cui agisce la rappresentazione $\sigma^*\rho$ è
\[ V_{\sigma^* \rho} = V_{\sigma}^* \otimes V_\rho \iso \Hom(V_\sigma, V_\rho)\]
dove l'isomorfismo di rappresentazioni è quello dimostrato nel teorema \ref{thm: isov*w}.
In particolare abbiamo
\[ (V_{\rho^*\sigma})^G \iso \Hom(V_\rho, V_\sigma)^G = \Hom(\rho, \sigma)\]
Per cui dato che noi stiamo cercando $\dim(\Hom(\rho, \sigma))$, basterà trovare $\dim(V_{\rho^*\sigma})^G$.
Ma dal lemma \ref{lemma:reynolds} sappiamo che vale $\dim(V_{\rho^*\sigma})^G = \langle \chi_{\sigma^*\rho}  | 1 \rangle$.
Per cui abbiamo la catena di uguaglianze
\[\dim(\Hom(\sigma, \rho)) = \dim(V_{\rho^*\sigma})^G = \langle \chi_{\sigma^*\rho} | 1 \rangle = \langle \chi_\rho | \chi_\sigma \rangle\]
\end{proof}

Osserviamo che vale l'uguaglianza
$\langle \chi_\sigma | \chi_\rho \rangle=\overline{\langle \chi_\rho | \chi_\sigma \rangle}$
che si riscrive, per quanto appena dimostrato, come
$\dim(\Hom(\rho, \sigma))=\overline{\dim(\Hom(\sigma, \rho))}$
tuttavia essendo dei numeri naturali deduciamo che le eguaglianze sussistono anche senza il coniugio.


  Ricordiamo che se $\rho$ è una rappresentazione di $G$, allora $\rho$ si può scrivere in modo unico come
    \[ \rho = \dsum_i n_i \rho_i\]
    dove le $\rho_i$ sono le rappresentazioni irriducibili di $G$ e gli $n_i$ sono numeri naturali positivi. Dall'equazione scritta sopra segue subito che
    \[ \chi_\rho = \dsum_i n_i \chi_{\rho_i}\]
    Possiamo ottenere un'informazione utile prendendo il prodotto scalare dell'equazione precedente con il carattere di una delle rappresentazioni $\rho_j$
    \[ \langle \chi_\rho | \chi_{\rho_j} \rangle = \dsum_i n_i \langle \chi_{\rho_i} | \chi_{\rho_j} \rangle = n_j \]
    Visto che siamo in grado di ricavare le molteplicità $n_j$ delle rappresentazioni irriducibili usando unicamente il carattere della rappresentazione,
    è chiaro a questo punto che due rappresentazioni sono isomorfe se e solo se hanno lo stesso carattere.
  
  Come caso particolarmente interessante del fatto precedente consideriamo la rappresentazione regolare di un gruppo $G$. Difatti come sappiamo
    \[ \chi_{\mathcal{R}}(s) =
    \begin{cases}
      |G| \quad \text{se } s = e \\
      0 \quad \text{altrimenti}
    \end{cases}\]
    Quindi considerando una qualsiasi rappresentazione $\rho$ dello stesso gruppo si ha che
    \[ \langle \chi_{\mathcal{R}} | \chi_\rho \rangle = \frac{1}{|G|}\cdot|G|\cdot\chi_{\rho}(e)=\chi_{\rho}(e)=\deg(\rho) \]
    In particolare vale $\deg(\rho)=\dim(\Hom(\mathcal{R},\rho))$,
    che è un risultato che avevamo già dimostrato senza fare uso dei caratteri.

  Osserviamo infine che $\langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = \sum_i n_i^2$.
  Quindi abbiamo un efficace criterio di irriducibilità: $\rho$ è irriducibile se e solo se $\langle \chi_\rho | \chi_\rho \rangle = 1$.



\begin{cor}[Corollario del lemma \ref{lemma:reynolds}: Lemma di Burnside]
Sia $I$ un insieme finito e $G$ un gruppo finito che agisce su di esso. Vale allora la formula
\[ |I/G| = \dfrac{1}{|G|} \dsum_{s\in G} |I^s|\]
\end{cor}
\begin{proof}
Ricordiamo che con $|I/G|$ si indica il numero di orbite dell'azione.
Consideriamo la rappresentazione per permutazioni  $\rho_I:G\to V_I$ corrispondente all'azione di $G$ sull'insieme $I$. Consideriamo la quantità
\[\langle \chi_{\rho_I} | 1 \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \tr \rho_I(s)\]
Ma è molto semplice osservare che, per la definizione stessa della rappresentazione per permutazioni, vale $\tr \rho_I(s) = |I^s|$.
Lo spazio
\begin{align*}
V_{\rho_I}^G &= \{v\in V_{\rho_I}:\rho_I(s)v=v\  \forall s\in G\} = \\
&=\left\{\sum_i a_i e_i :\sum_i a_i e_{s\cdot i}=\sum_i a_i e_i\ \forall s\in G \right\}= \\
&=\left\{ \sum_i a_i e_i: \sum_i (a_{s^{-1}\cdot i}-a_i)e_i=0\ \forall s\in G \right\}
\end{align*}
sarà composto da i vettori che hanno i coefficienti $a_i$ costanti su ciascuna orbita di $G$ su $I$. Perciò
\[ \dim V_{\rho_I}^G = \text{numero delle orbite } = |I/G|\]
e con l'affermazione precedente si ottiene appunto il lemma di Burnside.
\end{proof}



\begin{thm} Sia $G$ un gruppo finito e siano $\rho_1, \ldots , \rho_r$ le sue rappresentazioni irriducibili. Sia inoltre $\C(G)^{\#}$ lo spazio delle funzioni da $G$ in $\C$ costanti sulle classi di coniugio di $G$.

Chiaramente $\dim \C(G)^\# = $ numero di classi di coniugio di $G$ $=: s$. La tesi del teorema è che $r = s$ dove $r$ è il numero di rappresentazioni irriducibili.
\end{thm}
\begin{proof}
Mostriamo intanto che $r \leq s$: i caratteri di $\rho_1, \ldots , \rho_r$ sono infatti ortonormali rispetto alla forma hermitiana
definita positiva $\langle \cdot | \cdot \rangle$, e quindi sono indipendenti (non posso avere più di $s$ vettori linearmente indipendenti
in uno spazio vettoriale di dimensione $s$).

Verifichiamo ora che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = {0}$. Sia $f \in \C(G)^\#$ e $\rho$ una rappresentazione,
definiamo $T_f= \frac{1}{|G|}\dsum_s f(s)\rho(s)$ e verifichiamo che è un omomorfismo di rappresentazioni:
\begin{align*}
 T_f \circ \rho(t) &= \frac{1}{|G|}\dsum_s f(s)\rho(s) \rho(t) = \frac{1}{|G|}\dsum_s \rho(t) \rho(t^{-1}) f(s)\rho(s) \rho(t) = \\
 &= \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}) \rho(s) \rho(t) = \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}st) =  \\
 &= \frac{1}{|G|}\rho(t) \dsum_{s'} f(s') \rho(s')= \rho(t) \circ T_f
\end{align*}

Abbiamo usato il fatto che $f(s)\in \C$, quindi commuta con $\rho(g)$, e che $f$ è una funzione di classe nella
sostituzione di $s$ con $s'= t^{-1}st$ (essendo il coniugio un automorfismo, cambia solo l'ordine della somma).

Se $\rho$ è irriducibile, $T_f= \alpha I$ è uno scalare per il lemma di Schur, $\alpha= \frac{\tr(T_f)}{\deg(\rho)}$.
Si ha $$\alpha =\frac{1}{\deg(\rho)}\tr(T_f)=\frac{1}{\deg(\rho)|G|}\dsum_s f(s) \chi_\rho(s)=
\frac{1}{\deg(\rho)} \langle f | \chi_{\rho^*} \rangle = 0$$ se $f \in \langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp}$.

Generalizziamo a quando $\rho$ non è irriducibile, ossia $\rho = \sigma_1+ \ldots + \sigma_n$, con $\sigma_i$ irriducibili. Allora
si ha $V_\rho = V_{\sigma_1}  \oplus \ldots \oplus V_{\sigma_n}$, ed essendo $T_f=0 $ su ogni $V_{\sigma_i}$, è nullo anche su $V_\rho$.

Mostriamo che questo implica $f=0$: sia $\mathcal{R}$ la rappresentazione regolare di $G$; si ha:
$$ 0= |G| T_f (e_1) = \dsum_s f(s) \mathcal{R}(s)(e_1) = \dsum_s f(s) e_s $$

Di conseguenza $f(s)=0 \quad \forall s \in G$ essendo gli $e_s$ una base di $V_R$.
In questo modo abbiamo mostrato che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = \{0\}$, quindi $r=s$.
\end{proof}

















\subsection{Tabella dei caratteri}


Dato un gruppo $G$, possimo costruire la $tabella\ dei\ caratteri$ nel seguente modo:
\begin{itemize}
\item su ogni colonna mettiamo un rappresentante della classe di coniugio con sotto la cardinalità dell'orbita ovvero

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c}
\hline
$G$  & $e$ & $\Orb(g_1)$ & $\Orb(g_2)$ & \\
 & 1 & $|\Orb(g_1)|$ & $|\Orb(g_2)|$ & \\
\hline
 & &  & \\
\end{tabular}
\end{table}

\item su ogni riga mettiamo una rappresentazione irriducibile del gruppo
\item all'incrocio tra la rappresentazione $\rho_i$ e la classe di coniugio di $g_j$ inseriamo il valore di $\chi_{\rho_i(g_j)}$.


\end{itemize}

\begin{rem}
 Sappiamo che il numero di rappresentazioni irriducibili è uguale al numero di classi di coniugio del gruppo, quindi
 la tabella dei caratteri è quadrata.
\end{rem}



\subsection{Esempi di rappresentazioni di gruppi finiti}

\begin{exmp}[Tabella dei caratteri di $S_3$]
La prima cosa da fare per costruire la tabella dei caratteri è vedere quanti elementi ha $S_3$,
suddividerli in classi di coniugio e poi cercare le rappresentazioni irriducibili.
Notiamo subito che $S_3$ ha esattamente 3 classi di coniugio. La prima è ovviamente quella banale, composta solo dall'identità $e$.
Poi c'è la classe delle trasposizioni $\{(1 2) ,(2 3), (1 3)\}$ che ha 3 elementi e poi ci sono i $3-$cicli, ovvero $(1 2 3)$ e $(1 3 2)$.
Possiamo cominciare a scrivere una tabella vuota $3\times 3$


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )  \\
 & 1 & 3 & 2 \\
\hline
 & &  & \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

Una rappresentazione irriducibile che c'è sempre è la rappresentazione banale di grado 1, ovvero quella che manda ogni elemento nell'identità. La tabella con questa informazione diventa

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

Un'altra rappresentazione che già conosciamo è il segno, $\epsilon$, che ricordiamo vale $(-1)^{n-1}$ dove $n$ è la lunghezza del ciclo. La tabella diventa

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

A questo punto ci sono due motivi per dire che l'ultima rappresentazione ha grado 2: il primo è che è l'unico modo di ottenere la relazione
\[ |G | = \dsum_i n_i^2 \]
Il secondo è che se fossero due rappresentazioni di grado 1 allora il gruppo avrebbe solo rappresentazioni irriducibili di grado 1 e un teorema che abbiamo fatto implicherebbe che $S_3$ è abeliano, cosa palesemente falsa.

Per trovare il carattere dell'ultima rappresentazione possiamo agire in più modi. Innanzitutto la tabella ora ha la forma

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 &  & \\
\hline
\end{tabular}
\end{table}

In generale ci saranno due numeri complessi $a, b$ nelle due caselle che mancano. Tuttavia noi sappiamo un sacco di teoremi che ci permettono di restringere il campo dei valori che possono avere. Per esempio noi sappiamo che
\[\langle \rho_i | \rho_j \rangle = \delta_{ij}\]
Per cui imponendo che il prodotto scalare con entrambe le precedenti faccia 0 abbiamo due equazioni e due incognite, ovvero un problema risolvibile. L'altro modo è dire che
\[ \mathcal{R} = 1 + \epsilon + 2\rho\]
E dato che il carattere si comporta bene con la somma di rappresentazioni,
\[\chi_{\mathcal{R}} = \chi_1 + \chi_\epsilon + 2 \chi_\rho  \]
Ma sappiamo anche che
\[ \chi_{\mathcal{R}}(s) =
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]
Per cui con agili conti riusciamo a completare la tabella

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_3$}
\label{tabella caratteri s3}
\end{table}

L'ultimo modo è cercare di scomporre un'altra rappresentazione a caso di $S_3$, cercando di trovare la rappresentazione che ci manca. Per esempio ricordiamo l'azione di $S_3$ sui vettori di base di $\C^3$
\[ \tau(s) e_i = e_{s(i)}\]
Ricordiamo che il sottospazio di dimensione $1$ fatto dallo $Span$ del vettore $v = e_1 + e_2 + e_3$ è un sottospazio invariante in cui $\tau(s)$ è sostanzialmente l'identità. Il suo ortogonale è un altro sottospazio invariante: proprio quello di dimensione $2$ che mancava. Di conseguenza potremo scrivere
\[ \tau = 1 + \rho\]
dove $\rho$ è irriducibile di grado $2$.
Dato che è facile calcolare il carattere di $\tau(s)$ in quanto è uguale a $\Fix(s)$, possiamo scrivere
\[ \Fix(s) = 1 + \chi_\rho\]
Da cui si ricava subito il carattere della rappresentazione $\rho$.
\end{exmp}






\begin{exmp}[Tabella dei caratteri di $S_4$]
Facciamo la prima cosa importante: dividiamo $S_4$ in classi di coniugio. Per i soliti teoremi sugli $S_n$, le classi di coniugio saranno
\[\{e\}, \{(a b)\}, \{(a b c)\}, \{(a b c d)\}, \{(a b)(c d)\}\]
E notiamo che sono 5. Possiamo quindi cominciare a compilare la tabella dei caratteri vuota

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

dove è già stata inserita la rappresentazione banale. Anche per $S_4$, essendo un gruppo simmetrico c'è la rappresentazione segno di grado 1.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

A questo punto bisogna fare cose a caso cercando le rappresentazioni irriducibili. Per esempio possiamo di nuovo considerare la rappresentazione per permutazioni
\[ \tau(s) e_i = e_{s(i)}\]
che si scompone anche questa come
\[ \tau = 1 + \rho\]
Vorremmo sapere se $\rho$ è irriducibile. Potremmo invocare qualche teorema ma lo faremo con le mani calcolando il carattere di $\rho$
\[ \chi_\rho(s) = \Fix(s) - 1 =
\begin{cases}
3 \quad \text{Se } s = e \\
1 \quad \text{Se } s = (a b) \\
0 \quad \text{Se } s = (a b c) \\
-1 \quad \text{Se } s = (a b c d ), (a b) (c d)\\
\end{cases}
\]
E andando a calcolare
\[\langle\chi_\rho |\chi_\rho\rangle = \dfrac{1}{24}\left(3^2  + 6 \cdot 1^2  + 0 + (-1)^2 \cdot (3 +6 )\right) = 1\]
scopriamo che è effettivamente irriducibile.  Aggiungiamola alla tabella.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

Abbiamo appena terminato le rappresentazioni che conoscevamo di $S_4$.\\
\emph{Ottimo consiglio:} Quando non vengono in mente altre rappresentazioni, si può provare a considerarne due già presenti nella tabella e fanne il prodotto. Risulta utile il seguente lemma.

\begin{lemma}
Se $\rho$ e $\sigma$ sono due rappresentazioni e $\deg(\rho)=1$ ( ovvero $\rho:G\rightarrow \mathbb{C}^*$), allora $\sigma$ è irriducibile se e solo se $\rho\sigma$ lo è. Inoltre hanno lo stesso grado.
\end{lemma}

\begin{proof}
Si considera il fatto che
\[ \sigma \text{ irriducibile } \Leftrightarrow 1=\langle\chi_{\sigma}|\chi_{\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\sigma(s)}|^2 \]
Quindi
\[ \langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho(s)}\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)|^2|\chi_{\sigma(s)}|^2 \]
ed essendo $\rho(s)$ una radice $n-$esima dell'unità dove $n$ è l'ordine di $G$ si ha che
\[
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}
|\chi_{\sigma(s)}|^2=\langle \chi_{\sigma}|\chi_{\sigma}\rangle
\]
Che abbiano lo stesso grado deriva per esempio dal fatto che
\[ \chi_{\rho\sigma}=\chi_{\rho}\chi_{\sigma}\Rightarrow \deg(\rho\sigma)=\chi_{\rho\sigma}(e)=\chi_{\rho}(e)\chi_{\sigma}(e)=\deg(\rho)\deg(\sigma)=\deg(\sigma) \]
\end{proof}

Ritornando all'esempio di $S_4$, essendo $\epsilon$ di grado 1 e $\rho$ irriducibile allora anche $\rho\epsilon$ è un'altra rappresentazione irriducibile.
\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}
A questo punto dato che $|S_4|=24$ e che $1+1+3^2+3^2=20$ si possono avere due situazioni: $S_4$ potrebbe avere ancora 4 rappresentazioni irriducibili di grado 1 oppure una di grado 2.
Ma sappiamo già che rimane posto solo per un'altra rappresentazione irriducibile, quindi siamo nel secondo caso.
Un altro modo per arrivare alla stessa conclusione è mostrare che $S_4$ non ammette altre rappresentazioni di grado $1$ oltre alla banale e alla rappresentazione segno.\\
Comunque, dato che manca solo una rappresentazione possiamo usare il trucco di prima (differenza dalla rappresentazione $\mathcal{R}$ ) e concludere.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
 $\sigma$& 2&  0 & -1& 0 & 2\\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_4$}
\label{tabella caratteri s4}
\end{table}


\begin{rem} Guardiamo la tabella, in particolare il "minore" ottenuto considerando le prime due e l'ultima riga e le prime 3 colonne.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 6 & 8 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\sigma$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\end{table}
Se la confrontiamo con la tabella dei caratteri di $S_3$ vediamo che sono analoghe. Intuitivamente $\rho$ in $S_3$ deriva dalla rappresentazione $\sigma$ di $S_4$ mediante un omomorfismo
\[S_4\rightarrow S_3\]
che corrisponde ad una azione di $S_4$ su un insieme di 3 elementi. Tale insieme è il sottogruppo di Klein privato dell'unità ovvero
$\{ (12)(34),(13)(24),(14)(23)\}$.
\end{rem}


In questo caso non è servito ma possono risultare utili i prossimi lemmi.
\begin{lemma}
$\rho^* $ è irriducibile $ \Leftrightarrow \rho$ è irriducibile.
\end{lemma}
\begin{proof} $\chi_{\rho^*}=\overline{\chi_\rho} $ e quindi si vede subito che
\[
1=\langle\chi_{\rho}|\chi_{\rho}\rangle \Leftrightarrow 1=\langle\chi_{\rho^*}|\chi_{\rho^*}\rangle
\]
\end{proof}

\begin{lemma}
Se $\rho$ è una rappresentazione di grado $d$ di $G$, come sempre gruppo finito, allora:\\
$(a)$ $|\chi_{\rho}(s)|\leq d$ \\
$(b)$ Direttamente dal punto $(a)$ si deduce che
\[
\chi_{\rho}(s)=d\Leftrightarrow \lambda_1,\ldots,\lambda_d=1\Leftrightarrow \rho(s)=\Id
\]
dove $\lambda_1,\ldots,\lambda_d$ sono gli autovalori di $\rho(s)$.
\end{lemma}

\begin{proof}
 Se $\lambda_1,\ldots,\lambda_d$ sono gli autovalori di $\rho(s)$ allora $\chi_{\rho}(s)=\sum_{i=1}^{d}\lambda_i$.
 Inoltre essendo $G$ finito $|\lambda_i|=1$ per ogni $i\in \{1,\ldots,d\}$. Se ne deduce che
\[
|\chi_{\rho}(s)|\leq \sum_{i=1}^d |\lambda_i|=d
\]
\end{proof}
\end{exmp}





\begin{exercise}
Trovare la tabella dei caratteri del gruppo diedrale $D_5$.
\end{exercise}


\subsection{Il problema della prima lezione visto con i nuovi strumenti}
Consideriamo un cubo. Scriviamo un numero su ciascuna delle facce e consideriamo l'operazione $T$ che per ogni faccia sostituisce al numero presente la media dei numeri presenti sulle 4 facce del cubo adiacenti. Vogliamo studiare il comportamento dei numeri del cubo quando questa iterazione viene compiuta molte volte.

Cerchiamo di formalizzare il problema usando la teoria delle rappresentazioni. Possiamo considerare l'insieme $F$ delle facce del cubo\footnote{che ha quindi 6 elementi}. Una generica configurazione del cubo sarà esprimibile come
\[ v = \dsum_{f \in F} a_f e_f \]
Dove $a_f \in \mathbb{C}$ e $\{e_f\}$ è una base di uno spazio vettoriale di dimensione $6$. L'operatore che sostituisce la media è lineare ma soprattuto commuta con le simmetrie del problema. Spieghiamo meglio questo concetto.
Consideriamo il gruppo $G$ delle rotazioni del cubo, ovvero
\[G = \{ g \in SO(3) \ |\ g(\text{Cubo}) \subseteq \text{Cubo} \} \]
\`E ovvio che il problema è invariante per simmetria, ovvero se $g \in G$, allora vale
\[ T v = g^{-1}T g v \]
che è la formula di un cambio di base. Questo si può scrivere come
\[gT = Tg \]
ovvero ci dice che $\forall g \in G$ le due operazioni commutano. \`E bene scrivere in modo formale che $\tau: G \to GL(V_\tau)$ è una rappresentazione del gruppo di rotazioni del cubo in $V_\tau = \mathbb{C}^6$ e questa rappresentazione commuta con un operatore $T$, ovvero
\[T\tau(g) = \tau(g) T \qquad \forall g \in G  \]
quindi $T$ è omomorfismo di rappresentazioni.
L'obiettivo che ci poniamo ora è quello di riuscire a scomporre $\tau$ come somma di rappresentazioni irriducibili in quanto una volta trovata una scomposizione
\[ V_\tau = \bigoplus_{i = i}^n V_{\rho_i} \]
potremo usare il lemma di Schur per dire che su ogni $V_{\rho_i}$ l'operatore $T$ si comporta come scalare (questo potremo dirlo se le irriducibili che troveremo avranno tutte molteplicità $1$). Per riuscire a capire qualcosa di come sono fatte le rappresentazioni è opportuno prima cercare di dare una struttura più chiara a questo gruppo.
In effetti è possibile mostrare che $G \iso S_4$ (suggerimento: vedere come agisce $G$ sulle quattro diagonali del cubo). A questo punto noi abbiamo una rappresentazione di grado 6 di $S_4$ che cerchiamo di scomporre come somma di rappresentazioni irriducibili. Ma avendo già calcolato la tabella dei caratteri di $S_4$ il lavoro sarà molto semplificato. Dato che
\[\tau = \dsum_i n_i\rho_i \Rightarrow \chi_\tau  = \dsum_i n_i\chi_{\rho_i}\]
è sufficiente calcolare i prodotti scalari dei caratteri delle rappresentazioni irriducibili di $S_4$ con il carattere di $\tau$ per trovare quali rappresentazioni compaiono.
Prima di tutto è necessario calcolare il carattere di $\tau$, ma questo si può fare con la tecnica dei punti fissi che abbiamo già usato alcune volte.
Con un po' di sforzo si ottiene
\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$G$ & (1) & (12) &  (123) & (1234) & (12)(34)\\
  & 1   & 6    & 8      & 6      & 3\\
\hline
$\tau$  & 6   & 0    & 0      & 2      & 2\\
\hline
\end{tabular}
\end{table}

Per cui con facili conti $\tau = 1 + \epsilon\rho + \sigma$,
ovvero possiamo scomporre $V_\tau = V_1 \oplus V_{\epsilon\rho} \oplus V_{\sigma}$ come somma di rappresentazioni irriducibili
che hanno rispettivamente dimensioni 1,3,2.

Concretamente è possibile rendersi conto che in realtà

\[V_1 =  \text{i numeri sulle facce sono tutti uguali} \]
\[V_{\epsilon\rho} = \text{le facce opposte hanno numeri opposti}\]
\[V_{\sigma} = \text{le facce opposte hanno numeri uguali e la somma di tutti è 0}\]
Su questi spazi è facile vedere che effettivamente $T$ è scalare e in particolare
\[
\begin{cases}
T|_{V_1} = 1 \\
T|_{V_{\epsilon\rho}} = 0 \\
T|_{V_\sigma} = -\frac{1}{2}\\
\end{cases}
\]
Quindi è evidente che eseguendo molte iterazioni su ogni faccia viene approssimata la media dei numeri che c'erano all'inizio.











\newpage
\section{Rappresentazioni reali, complesse e quaternioniche}

\subsection{Classificazione delle rappresentazioni su $\C$}
Sono di un certo interesse le rappresentazioni su $\C-$spazi vettoriali che possono essere espresse con matrici a coefficienti reali. Diamo una definizione precisa:

\begin{defn}
Diciamo che una rappresentazione $\rho:G\to GL(V_\rho)$ del gruppo $G$ è reale se esiste una base di $V_\rho$ secondo la quale
le matrici di $\rho(g)$, al variare di $g$ nel gruppo, sono tutte a coefficienti reali.
\label{def: rappr reale}
\end{defn}

Sia $V$ un $\C-$spazio vettoriale di dimensione $n$.
Denotiamo con $V^\R$ lo spazio vettoriale su $\R$ che ha per ``insieme di vettori''
lo stesso di $V$, con la stessa somma ma chiaramente con prodotto per scalari ristretto ad $\R$.
Osserviamo che se $\{v_1, \dots, v_n\}$ è una base di $V$ allora
$\{v_1, \imath v_1, v_2, \imath v_2,\dots, v_n, \imath v_n\}$ è una base di $V^\R$, il quale di conseguenza ha dimensione $2n$.
Se $\rho:G\to GL(V)$ è una rappresentazione del gruppo $G$, allora è definita
in modo naturale una rappresentazione $\rho^\R:G\to GL(V^\R)$, in cui il gruppo $G$
agisce sullo spazio vettoriale esattamente come nella rappresentazione $\rho$ (se $G$ agisce
in modo $\C-$lineare, a maggior ragione l'azione è $\R-$lineare).

Il fatto che $\rho$ sia una rappresentazione reale è equivalente a chiedere l'esistenza di un sottospazio reale $V_0 \subset V^\R$ che sia stabile sotto l'azione di $G$ e che abbia dimensione $n$. Cioè è possibile scomporre $V^\R$ come somma di sottorappresentazioni:
\[ V^\R = V_0 \oplus i V_0 \]

\begin{exmp}
Prendiamo come gruppo un gruppo ciclico, per esempio $\Z / 3\Z$\footnote{Che per i fisici è isomorfo a $C_3$}. Evidentemente tutte le rappresentazioni irriducibili non banali di $G$ non sono reali, in quanto sono di grado 1 e sono le radici dell'unità diverse da $1$.
\end{exmp}

Non è difficile osservare che se indichiamo con $\alpha_{i,j}$ i coefficienti della matrice $\rho(g)$
rispetto alla base $\{v_1, \dots, v_n\}$, allora la matrice di $\rho^\R(g)$ rispetto alla base
$\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$ si ottiene
ponendo al posto di $\alpha_{i,j}$ il blocchetto $2\times 2$:
\[\begin{pmatrix}
	\Re \alpha_{i,j} & -\Im \alpha_{i,j}\\
	\Im \alpha_{i,j} & \Re \alpha_{i,j}\\
\end{pmatrix}\]
dove $\Re$ e $\Im$ indicano parte reale e immaginaria.
Dunque abbiamo il seguente risultato:

\begin{prop}
	$\chi_{\rho^\R}(g) = \chi_\rho(g) + \chi_{\rho^*}(g)$
\end{prop}
\begin{proof}
	Basta ricordare che $\chi_{\rho^*}$ è il complesso coniugato di $\chi_\rho$, dunque
	$\chi_\rho(g) + \chi_{\rho^*}(g) = 2\Re \chi_\rho(g)$ è evidentemente la traccia di $\rho^\R(g)$ per
	quanto detto sopra.
\end{proof}


\begin{prop}
	Se $\rho:G\to GL(V)$ è irriducibile ($V$ è un $\C-$spazio vettoriale), allora $\rho$ è reale se e solo se $\rho^\R$ è riducibile.
\end{prop}
\begin{proof}
	Se $\rho$ è rappresentazione reale, sia $\{v_1,\dots v_n\}$ una
	base di $V$ tale che le matrici di $\rho(g)$ rispetto a tale base siano a coefficienti reali.
	Ora, $V^\R$ ha per base $\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$.
	Indichiamo con $V_0$ il sottospazio di $V^\R$ generato da $\{v_1,\dots v_n\}$, che ha dimensione $n$.
	Ora, $V_0$ è $G-$invariante, dunque $\rho^\R$ è riducibile.

	Viceversa, supponiamo che $V^\R = U \oplus W$ sia scomposizione di rappresentazioni
	con $U$ e $W$ diversi da $\{0\}$. A meno di scambiare $U$ e $W$ posso supporre $\dim U \le n$.
	Il sottospazio di $V$ generato dai vettori di $U$ (con combinazioni lineari complesse) è $G-$invariante,
	dunque essendo $\rho$ irriducibile tale sottospazio deve coincidere con $V$.
	Tutto ciò si traduce nel fatto che $V^\R = U + \imath U$, e siccome $\dim\imath U = \dim U \le n$
	deve essere necessariamente che $U$ e $\imath U$ sono in somma diretta e hanno
	entrambi dimensione $n$. Ora basta prendere una qualsiasi base di $U$,
	la quale è anche base di $V$ (essendo costituita da $n$ vettori che generano $V$) ed è tale che la matrice di $\rho(g)$ è
	a coefficienti reali per ogni $g\in G$, ovvero $\rho$ è reale secondo la definizione \ref{def: rappr reale}.
\end{proof}

Ora andremo a fare una classificazione delle rappresentazioni in 3 tipi diversi:
\begin{itemize}
\item Reali
\item Complesse
\item Quaternioniche
\end{itemize}
La classificazione verrà fatta in base all'esistenza o meno di forme bilineari di un certo tipo invarianti sotto $G$.
Iniziamo con alcune proprietà soddisfatte dalle rappresentazioni reali:

\begin{thm}
Prendiamo una $\rho:G\to GL(V)$ rappresentazione $/\C$ che sia reale. Allora:
\begin{enumerate}
\item $\chi_\rho(g)\in \R$ $\forall g\in G$
\item $V$ possiede una forma bilineare simmetrica $G-$invariante non nulla. Ovvero lo spazio delle forme bilineari simmetriche $S^2V^*\subset V^*\otimes V^*$ è tale che $(S^2V^*)^G\neq 0$.
\end{enumerate}
\end{thm}

\begin{proof}
Abbiamo supposto la rappresentazione reale. Quindi la matrice è reale ed in particolare lo sarà anche la sua traccia. Quindi il primo punto è vero. Ora veniamo al secondo punto: esisterà un $V_0$ spazio vettoriale reale $G-$invariante che tensorizzato con $\C$ dia $V^\R$. Consideriamo ora una forma bilineare simmetrica $B_0$ non degenere $/\R$
\[ B_0 \in S^2 V_0^*\]
Possiamo ora renderla invariante sotto l'azione di $G$ con il solito metodo del fare la media. Consideriamo quindi $\tilde B_0$ definito come
\[ \tilde B_0(v_1, v_2) = \dfrac{1}{|G|} \dsum_{g\in G} B_0(\rho(g) v_1, \rho(g) v_2)\]
Questo ha le caratteristiche precedenti ed è anche invariante sotto $G$, ovvero $\tilde B_0\in (S^2V_0^*)^G$. Possiamo a questo punto estenderla a forma bilineare su $V$ complessificandola in modo ovvio. Costruiamo quindi $B \in (S^2V^*)^G$ definendola nel seguente modo:
\[B(v_1 + i v_1', v_2 + i v_2') = \left( \tilde B_0(v_1, v_2) - \tilde B_0(v_1', v_2')\right)  + i \left( \tilde B_0(v_1', v_2) + B_0(v_1, v_2')\right)\]
\`E una banale verifica controllare che rispetta le caratteristiche richieste.
\end{proof}

\begin{rem}
Supponiamo che $\rho:G\to GL(V)$ sia una rappresentazione irriducibile. Allora il fatto che esiste una forma bilineare $G-$invariante non nulla
equivale a $(V^*\otimes V^*)^G\neq 0$. Ma ricordando che $(V^*\otimes V^*)^G \iso \Hom(V,V^*)^G$ (che è l'insieme degli omomorfismi di rappresentazioni) ciò vuol dire che $\rho$ e $\rho^*$ sono isomorfi, il che si traduce in $\chi_\rho = \chi_{\rho^*}$. Ma questo avviene esattamente quando $\chi_\rho$ assume solo valori reali. Ciò significa che la seconda condizione trovata (che prevede l'esistenza di una forma bilineare simmetrica invariante) è strettamente più forte della prima (che è soddisfatta anche nel caso in cui esista una forma bilineare invariante non necessariamente simmetrica).
\end{rem}

Vediamo ora un lemma che ci servirà per la classificazione.
\begin{lemma}
Sia $V$ una $G-$rappresentazione irriducibile $/\C$. Allora ogni forma bilineare non nulla $G-$invariante è non degenere. Inoltre, se esiste, è unica a meno di scalari, ovvero $\dim (V^*\otimes V^*)^G\le1$.
\end{lemma}
\begin{proof}
Prendiamo un elemento $B \in \left( V^* \otimes V^*\right) ^G$.

Ricordiamo che $V^* \otimes V^*$ e $\Hom(V, V^*)$ sono $G-$rappresentazioni isomorfe. In particolare:
\[\left(V^* \otimes V^*\right)^G \iso \Hom(V, V^*)^G \]
A questo punto, se esiste una forma bilineare non nulla invariante, questa corrisponde ad un omomorfismo di rappresentazioni $\phi: V\to V^*$.
Per il lemma di Schur o $\phi$ è nullo o $\phi$ è un isomorfismo. Dato che la forma è non nulla, allora $\phi = \lambda \Id$ con $\lambda \in \C\setminus\{0\}$.
\end{proof}


\begin{thm}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile $/\C$.\\
Definiamo $m_\rho = \dfrac{1}{|G|} \dsum_{g \in G} \chi_{\rho}(g^2)$ detto ``indicatore di Frobenius-Schur''. Allora
\begin{enumerate}
\item Se $B \in (V^* \otimes V^*)^G$ allora $B \in S^2V^*$ oppure $B \in \bigwedge ^2 V^*$
\item  $m_\rho \in \{-1, 0, 1 \}$
\item{ \begin{itemize}
   \item se $m_\rho = 0$ allora $(V^*\otimes V^*)^G = 0$
   \item se $m_\rho = 1$ allora $(S^2 V^*)^G \neq 0$
   \item se $m_\rho = -1$ allora $(\bigwedge^2 V^*)^G \neq 0$
\end{itemize}
}
\end{enumerate}
\end{thm}

\begin{proof}
Indichiamo $W = (V^*\otimes V^*)^G$ lo spazio delle forme bilineari $G-$invarianti, che è sottorappresentazione di $V^*\otimes V^*$.
Vogliamo dimostrare che $W\subseteq S^2V^*$ oppure $W\subseteq \bigwedge^2V^*$.
Dal lemma precedente sappiamo che $\dim W \le 1$. Se $\dim W = 0$ allora si ha automaticamente la tesi.
Supponiamo ora $\dim W = 1$.

Ricordando che $V^*\otimes V^* \iso S^2V^* \oplus \bigwedge^2V^*$ è scomposizione in somma di rappresentazioni,
consideriamo le proiezioni $p_{sym}:V^*\otimes V^* \to S^2V^*$ e $p_{alt}:V^*\otimes V^* \to \bigwedge^2V^*$
che in particolare sono omomorfismi di rappresentazioni (la verifica è facile, comunque questo accade ogni volta che si ha una somma di rappresentazioni).

Prendiamo ora $B\in W$ forma bilineare invariante non nulla. Allora almeno uno tra $p_{sym}(B)$ e $p_{alt}(B)$ è non nullo. Se fossero entrambi non nulli allora avremmo $\dim (V^*\otimes V^*)^G = \dim(S^2V^*)^G + \dim(\bigwedge^2V^*)^G \ge 2$, assurdo.
Dunque $B$ deve essere simmetrica oppure alternante, e ciò dimostra il punto 1.

Passiamo ora agli altri due punti. Ricordando il teorema \ref{thm:tracciasymalt} abbiamo che:
\begin{itemize}
\item $\dim(S^2V^*)^G = \langle 1|\chi_{S^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 + \chi_\rho(g^2)}{2}$
\item $\dim(\bigwedge^2V^*)^G = \langle 1|\chi_{\bigwedge^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 - \chi_\rho(g^2)}{2}$
\item $\dim(V^*\otimes V^*)^G = \langle 1|\chi_{\rho^*\otimes\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\chi_\rho(g)^2$
\end{itemize}
dunque possiamo scrivere
$\dim(S^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G + m_\rho}{2}$;\ \
$\dim(\bigwedge^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G - m_\rho}{2}$.

Ora, usando quanto ottenuto al punto 1 e distinguendo i casi in cui $V$ ammetta una forma simmetrica invariante, una forma antisimmetrica invariante, oppure non ammetta forme invarianti, si ottiene facilmente quanto affermato ai punti 2 e 3.
\end{proof}

\begin{defn}[Classificazione tramite l'indice di Frobenius]
Sia $\rho:G\rightarrow GL(V)$ una rappresentazione irriducibile $/\C$: essa è detta
\begin{itemize}
\item \textbf{reale} se $m_\rho = 1$
\item \textbf{complessa} se $m_\rho = 0$
\item \textbf{quaternionica} se $m_\rho = -1$
\end{itemize}
\end{defn}

Ma la nuova definizione che abbiamo dato è compatibile con la \ref{def: rappr reale} data inizialmente?
La rassicurante risposta è data dal seguente lemma.
\begin{lemma}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile su $\C$. Allora $\rho$ è reale secondo la definizione \ref{def: rappr reale} se e solo se $m_\rho = 1$.
\end{lemma}
\begin{proof}
Sappiamo già che se $\rho$ è reale nel senso \ref{def: rappr reale} allora $m_\rho = 1$, in quanto $V_\rho$ ammette una forma simmetrica invariante non nulla. Mostriamo il viceversa.

Sia $B \in (S^2V_\rho^*)^G$, con $\dim_\C V_\rho = n$.
Noi stiamo cercando un certo $V_0 \subset V_\rho$ spazio vettoriale su $\R$ tale che $V_\rho^\R = V_0 \oplus i V_0$ .

Prendiamo ora una certa forma $h : V_\rho \times V_\rho \to \C$ hermitiana, definita positiva e $G-$invariante. Questa sicuramente esiste, è stato dimostrato nel teorema \ref{thm:esistenza hermitiana}.
Se si fissa un vettore $y\in V$ allora la funzione che manda $x$ in $B(x,y)$ è $\C-$lineare. Quindi per il teorema di Riesz esiste un vettore $\phi(y)$
tale che per ogni $x\in V$ si abbia $B(x, y) = h(x, \phi(y))$.

Che proprietà ha $\phi$? Possiamo notare che $\phi$ è $G-$equivariante, ovvero vale $\phi(\rho(g)y) = \rho(g) \phi(y)$.
Mostriamolo rapidamente, usando il fatto che $h$ è non degenere:
\[ h(x, \phi(\rho(g)y)) = B(x, \rho(g)y) = B(\rho(g^{-1} ) x, y) = h (\rho(g^{-1} ) x, \phi(y)) = h(x, \rho(g)y)\]
Tuttavia $\phi$ non è $\C-$lineare, ma piuttosto antilineare:
\[ \phi(z_1 x_1 + z_2 x_2) = \overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho\]
L'antilinearità rispetto agli scalari deriva essenzialmente dal fatto che stiamo usando un prodotto hermitiano invece di un prodotto scalare. Infatti
\[ h( x, \phi(zy) ) = B(x, zy) = z B(x,y) = z h(x, \phi(y)) = h(x, \overline{z} \phi(y)) \]
E quindi purtroppo $\phi$ non è davvero lineare come applicazione tra spazi vettoriali sul campo $\C$. Vediamo però che $\phi^2$ si comporta meglio:
\[\phi^2(z_1 x_1 + z_2 x_2) = \phi(\overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2))  = z_1 \phi^2(x_1) + z_2 \phi^2(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho \]
E quindi effettivamente $\phi^2$ è un omomorfismo di rappresentazioni irriducibili. Per questo motivo possiamo applicare Schur e concludere che
$\phi^ 2 = \lambda \Id_{V_\rho}$ per un certo $\lambda \in \C$.

Cosa possiamo dire su $\lambda$? Il claim è che sia $\lambda \in \R$ e $\lambda > 0$. Prima di tutto osserviamo che
\[h(x, \phi(y)) = B(x, y) = B(y, x) = h(y, \phi(x)) = \overline{h(\phi(x), y)} \]
e usando questo fatto possiamo considerare
\[ \lambda h(x, y) = h(\phi^2(x), y) = \overline{h(\phi(x), \phi(y))} = h(x, \phi^2(y)) = \overline{\lambda} h(x, y) \qquad \forall x, y \in V_\rho\]
E questo ci dice ovviamente che $\lambda \in \R$. Per mostrare ora che $\lambda > 0$ dobbiamo sfruttare il fatto che la nostra forma hermitiana sia definita positiva. Per questo motivo andiamo a considerare
\[\lambda h(x, x) = h (\phi^2(x), x) = \overline{h(\phi(x), \phi(x))} \qquad\Longrightarrow\qquad \lambda = \dfrac{\overline{h(\phi(x), \phi(x))}}{ h(x,x)} \qquad \forall x \in V_\rho\setminus\{0\}\]
E dato che $h $ è definita positiva si ha anche $\lambda > 0$

A questo punto possiamo (a meno di riscalare) scegliere $\lambda = 1$, ovvero $\phi^2 = \Id$. A questo punto ci piacerebbe tornare a considerare $\phi$ e non $\phi^2$. Notiamo che se ci restringiamo a spazi vettoriali su $\R$, allora anche $\phi$ è lineare in quanto il coniugio non ci dà fastidio. Dato che quindi $\phi$ è un endomorfismo di uno spazio vettoriale reale tale che $\phi^2=1$, allora $\phi$ è diagonalizzabile e ha solo gli autovalori $\pm 1$.  Per questo motivo possiamo scomporre lo spazio di partenza $V_\rho = V_+ \oplus V_-$, con ovvia notazione per gli autospazi.

A questo punto ci manca poco. $V_+$ e $V_-$ sono sottospazi reali del nostro spazio di partenza. Se mostriamo che sono isomorfi, o ancora meglio che vale $i V_+ = V_-$, abbiamo trovato la nostra scomposizione dello spazio $V_\rho$ in due spazi $V_\rho = V_0 \oplus iV_0$.
Prendiamo per esempio $x \in V_+$. Allora
\[ \phi(ix) = - i \phi(x) = -i x \]
Ovvero il vettore $ix$ è autovettore di $\phi$ con autovalore $-1$. Applicando due volte questo ragionamento si ottiene facilmente
\[ V_+ \iso V_- \qquad (i V_+ = V_-)\]
Per cui a questo punto abbiamo finito.
\end{proof}


\begin{lemma} $\rho:G\rightarrow GL(V)$ rappresentazione irriducibile $/_\C$: allora $\chi_\rho$ è reale se e solo se $\rho$ è una rappresentazione reale o quaternionica.
\end{lemma}
\begin{proof} Abbiamo visto prima che $(V^*\otimes V^*)^G\neq 0\Rightarrow \chi_\rho=\chi_{\rho^*}\Rightarrow \chi_\rho$ è reale, e se $\rho$ è reale o quaternionica ha proprio $(V^*\otimes V^*)^G$ diverso da $0$. Viceversa $\chi_\rho$ reale implica $\chi_\rho=\chi_{\rho^*}$. D'altra parte vale che $\rho\iso\rho^*\Leftrightarrow \chi_\rho =\chi_{\rho^*}$. Ciò significa che $\Hom_G(V,V^*)\neq 0\Rightarrow \exists B\in (V^*\otimes V^*)^G\Rightarrow \rho$ può essere solo reale o quaternionica.
\end{proof}

\begin{exercise}
	Mostrare che tutte le rappresentazioni irriducibili $/_\C$ di $S_4$ sono reali.
\end{exercise}
\begin{exercise}
	Sia $G$ gruppo ciclico di ordine dispari. Allora le rappresentazioni non banali sono complesse.
\end{exercise}




\begin{thm}
Sia $\rho:G\rightarrow GL(V)$ rappresentazione irriducibile $/_\R$: allora $\exists \sigma:G\rightarrow GL(V_{\sigma})$ rappresentazione irriducibile $/_\C$ tale che è vera una delle seguenti affermazioni:
\begin{enumerate}
\item $\chi_\rho=\chi_\sigma$;
\item $\chi_\rho=\chi_\sigma+\chi_{\sigma^*}$;
\item $\chi_\rho= 2\chi_\sigma$.
\end{enumerate}
\end{thm}

\begin{proof} Definiamo $V^\C=\C\otimes V$ la complessificazione di $V$ (è semplicemente $V$ visto però come spazio vettoriale su $\C$). Vi sono due casi:
\begin{enumerate}
\item $\rho:G\rightarrow GL(V^\C)$ (che chiamiamo $\rho^\C$ solo per ricordarci che $V$ è visto come spazio vettoriale sui complessi) è una rappresentazione irriducibile $/_\C$;
\item $\rho^\C:G\rightarrow GL(V^\C)$ è una rappresentazione riducibile $/_\C$.
\end{enumerate}
Se siamo nel primo caso $\sigma=\rho^\C$ è irriducibile $/_\C$ e ovviamente $\chi_\rho=\chi_{\rho^\C}$.\\
Se siamo nel secondo caso allora $V^\C=U\oplus W$ con $U$ e $W$ sottospazi $G-$invarianti. Quindi $\rho^\C=\rho_U+\rho_W$. Passiamo ora alla loro realificazione: $(\rho^\C)^\R=(\rho_U)^\R+(\rho_W)^\R$. Tuttavia sapendo che $\rho^\C$ derivava da una rappresentazione reale allora $(\rho^\C)^\R=\rho+\rho$ derivante dalla scomposizione $V^\R=V+iV$ dunque si ha
\[\rho+\rho=\rho_U^\R+\rho_W^\R\qquad\Longrightarrow\qquad \rho\iso\rho_U^\R\]
poichè le $\rho$ sono irriducibili. Inoltre se $\rho_U$ fosse ancora riducibile avrei una scrittura di $\rho$ come somma di due rappresentazioni più piccole, il che è assurdo. Dunque $\dim(U)=\dim(W)=\frac{n}{2}$ e quindi deduciamo, tra parentesi, che se la dimensione di $V$ è dispari allora $\rho$ su $\C$ è irriducibile.
Quindi la $\sigma$ irriducibile su $\C$ cercata è $\rho_U$:
dato che $\sigma^\R=\rho$ si ha che $\chi_\rho=\chi_\sigma+\chi_{\sigma^*}$. Se $\sigma\iso \sigma^*$ allora si è nel terzo caso (ciò avviene quando il carattere di $\sigma$ è reale).
\end{proof}



\subsection{Quaternioni}
\label{sec:quaternioni}
Ovviamente le rappresentazioni quaternioniche hanno a che fare con il corpo dei quaternioni. Vediamo un po' di caratteristiche interessanti di questo oggetto.

Il corpo $\HH$ si può vedere come
\[\HH = \R \oplus i \R \oplus j \R \oplus k \R \]
Con $i,j,k$ unità immaginarie che rispettano le seguenti regole
\[
\begin{cases}
i^2 = j^2 = k^2 = -1 \\
ij = - ji = k \\
jk = -kj = i \\
ki = - ik = j \\
\end{cases}
\]

Vediamo un po' di proprietà interessanti. Per esempio se consideriamo
\[ Q_8 =  \{\pm 1, \pm i, \pm j, \pm k \}\]
allora questo insieme è un gruppo se munito della moltiplicazione. Possiamo andare a vedere la tabella dei caratteri di questo gruppo.
Allo scopo possiamo usare la tecnica di andare ad esaminare i quozienti ciclici del gruppo.
Consideriamo i seguenti sottogruppi di $Q_8$
\begin{align*}
	H_1=\{ \pm1, \pm i \}\\
	H_2=\{ \pm1, \pm j \}\\
	H_3=\{ \pm1, \pm k \}
\end{align*}
\'E semplice verificare che $H_i$ è normale in $Q_8$ per $i=1,2,3$ e che $Q_8/H_i\iso C_2$.\footnote{Visto che gli $H_i$ sono normali, $Q_8/H_i$ è un gruppo, avendo solo due elementi è necessariamente $C_2$.} Di $C_2$ conosciamo le rappresentazioni di grado $1$: c'è la rappresentazione banale (che induce su $G$ la rappresentazione banale appunto) e $\rho:C_2\to \C^*$ che manda l'elemento non unità in $-1$. Da quest'ultima otteniamo le rappresentazioni che indicheremo con $\rho_i, \rho_j, \rho_k$ rispettivamente nel caso in cui quozientiamo per $H_1, H_2, H_3$; vediamo ora come calcolare una di queste, per esempio $\rho_i$, riportando il diagramma degli omomorfismi.
\[\tridiag {Q_8} {\pi_1} {Q_8/H_1} \rho {\C^*} {\rho_i}\]
Sia $g\in Q_8$, vogliamo capire quanto fa $\rho_i(g)$, ci sono due casi da considerare:
\begin{enumerate}
	\item se $g\in H_1$, allora $\pi(g) = eH_1$ dunque, visto che $eH_1$ è l'identità nel quoziente, $\rho_i(g)=1$
	\item se $g\not\in H_1$, allora $\pi(g) =gH_1\neq eH_1$, quindi, per la definizione di $\rho$, $\rho_i(g)=-1$
\end{enumerate}

L'ultima rappresentazione che manca ancora deve avere dimensione $2$ perché abbiamo già trovato $4$ rappresentazioni di grado $1$ (quella banale più le tre $\rho_i, \rho_j, \rho_k$ essendo tutte non isomorfe\footnote{La verifica è immediata}), abbiamo inoltre esaurito le rappresentazioni di grado $1$: potremmo ancora quozientare per $H=\{ 1, -1 \}$, ma si vede abbastanza facilmente che $Q_8/H\iso C_2\times C_2$, metre cercavamo quozienti ciclici; dalla formula $\sum n_i^2=|Q_8| = 8$ si deduce quindi che l'ultima rappresentazione deve avere grado $2$. Quest'ultima si ottiene dalla rappresentazione matriciale dei quaternioni: infatti i quaternioni possono essere presentati come un sottoinsieme delle matrici complesse $2\times 2$.
\[
	\HH = \left\{
		\begin{pmatrix}
	              	z & w\\
	              	-\overline{w} & \overline{z}\\
	              \end{pmatrix}
	              \middle|\ z,w\in \C
		\right\}
\]
La rappresentazione è determinata dall'omomorfismo $\rho_{\HH}:Q_8\to GL(\C^2)$ definito sui generatori:
\[
	\rho_{\HH}(i) = \begin{pmatrix}
	          	i & 0\\
	          	0 & -i\\
	          \end{pmatrix}
	\qquad
	\rho_{\HH}(j) = \begin{pmatrix}
	          	0 & 1\\
	          	-1& 0\\
	          \end{pmatrix}
\]
Quindi vediamo che gli elementi di $Q_8$ possono essere visti come matrici $2\times2$ complesse:
\[
1_\HH =
\left(
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right)
\qquad
i_\HH =
\left(
\begin{array}{cc}
i & 0 \\
0 & -i \\
\end{array}
\right)
\qquad
j_\HH =
\left(
\begin{array}{cc}
0 & 1 \\
-1 & 0 \\
\end{array}
\right)
\qquad
k_\HH =
\left(
\begin{array}{cc}
0 & i \\
i & 0 \\
\end{array}
\right)
\qquad
\]

Queste 4 matrici prendono il nome di matrici di spin di Pauli per le loro applicazioni in Fisica. Possiamo infine ricostruire la tabella dei caratteri di $Q_8$

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 1 & 2 & 2 & 2 \\
$Q_8$ & 1 & -1 & $\pm i$ & $\pm j$ & $\pm k$ \\
\hline
$\rho_1$ & 1 & 1 & 1 & 1 & 1 \\
\hline
$\rho_i$ & 1 & 1 & 1 & -1 & -1 \\
\hline
$\rho_j$ & 1 & 1 & -1 & 1 & -1 \\
\hline
$\rho_k$ & 1 & 1 & -1 & -1 & 1 \\
\hline
$\rho_\HH$ & 2 & -2 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $Q_8$}
\label{tab: caratteri q8}
\end{table}


\`E interessante notare che la tabella dei caratteri di $Q_8$ è uguale a quella di $D_4$, ma i due gruppi non sono isomorfi. Questo ci ricorda che la tabella dei caratteri dice tanto di un gruppo ma non tutto.



Ora vorremmo effettivamente capire il perché dei nomi dati nella classificazione delle rappresentazioni come reali, complesse e quaternioniche.
Per questo motivo ci servono un paio di concetti di algebra.


\begin{defn}[Algebra]
Un'algebra su $\R$ è uno spazio vettoriale reale $A$ dotato di una moltiplicazione $\cdot : A \times A \to A$ che sia associativa e bilineare.
Inoltre imponiamo che vi sia un elemento neutro rispetto a questa moltiplicazione. Quest'ultima richiesta non fa parte della
più generale definizione di algebra (le algebre che la soddisfano si dicono \emph{unitarie}), ma noi la inseriamo nella definizione
poiché in questo corso non tratteremo mai algebre non unitarie.
\end{defn}


\begin{defn}[Algebra di divisione]
Un algebra di divisione è un'algebra in cui ogni elemento escluso lo $0$ possiede un inverso moltiplicativo.
\end{defn}

\begin{exmp}
Gli esempi più standard di algebra di divisione su $\R$ di dimensione finita sono i campi $\R$ e $\C$ come spazi vettoriali reali.
Un esempio più sofisticato è dato dal corpo dei quaternioni $\HH$ (ovviamente visto come spazio vettoriale su $\R$).
\end{exmp}


\begin{rem}
Consideriamo una rappresentazione irriducibile $\rho:G\to GL(V_\rho)$ con $V_\rho$ spazo vettoriale su $\R$. Allora l'insieme degli endomorfismi
di $\rho$
\[ \End_G(V_\rho)\]
è un'algebra di divisione su $\R$ se dotato della composizione.
Infatti per lemma di Schur (in particolare la prima parte dell'enunciato, che vale su ogni campo) ogni elemento di $\End_G(V_\rho)$ o è la funzione nulla oppure è un isomorfismo, quindi ammette inverso.
Se $\dim V_\rho = n$ allora $\End_G(V_\rho)$, essendo contenuto in $\End(V_\rho)$, ha dimensione finita, in particolare $\dim \End_G(V_\rho)\le n^2$.
Vedremo tuttavia che vale una limitazione molto più forte.
\end{rem}


Presentiamo ora un sorprendente teorema che afferma che non ci sono altre algebre di divisione di dimensione finita su $\R$ oltre a quelle che abbiamo elencato come esempi, ovvero $\R, \C, \HH$.
\begin{thm}[di Frobenius]
\label{thm: frobenius}
Sia $A$ un'algebra di divisione su $\R$ di dimensione finita. Allora si ha
\[A \iso \R \quad \vee \quad  A \iso \C \quad \vee \quad A  \iso \HH \]
\end{thm}

\begin{proof}
Indichiamo con $\bm{1}\in A$ l'elemento neutro rispetto alla moltiplicazione di $A$ (occhio: è un vettore di $A$, non un numero reale).
Il sottospazio vettoriale generato da $\bm{1}$ è chiaramente isomorfo ad $\R$ come $\R-$spazio vettoriale, ma in
realtà lo è anche come sottoalgebra: infatti se $a,b\in\R$ allora
\[a\bm{1} \cdot b\bm{1} = ab (\bm{1}\cdot \bm{1}) = ab\bm{1} \]
dove nel primo passaggio abbiamo usato la bilinearità e nel secondo il fatto che $\bm{1}$ è elemento neutro.
Dunque, con un lieve abuso di notazione, possiamo scrivere $\R \subseteq A$, intendendo per $\R$ proprio il sottospazio generato da $\bm{1}$.
Se $\dim A = 1$ allora sarebbe $A=\R$. D'ora in poi supponiamo $\dim A > 1$.

Vogliamo ora mostrare che esiste una sottoalgebra di $A$ isomorfa a $\C$.
Sia $\alpha\in A \setminus\R$ e indichiamo con $A[\alpha]$ la sottoalgebra generata da $\alpha$, ovvero consideriamo l'insieme
\[A[\alpha] = \left\{ \dsum_{n = 0} ^N a_n \alpha^n |\ N\in\mathbb{N}, \ a_i \in \R,\ \alpha \in A   \right\} \]
L'algebra $A[\alpha]$, essendo contenuta in $A$, ha necessariamente dimensione finita, quindi esisterà un certo $N$ per cui gli elementi
$\alpha^0, \alpha^1, \dots \alpha^N$ sono linearmente dipendenti. Più precisamente prendiamo il minimo $N$ per cui questo avviene.
Allora esistono dei coefficienti $a_n$ reali tali che
\[ \dsum_{n=0}^N a_n \alpha^n = 0 \]
Ovvero il polinomio a coefficienti reali
\[ p(x) = \dsum_{n = 0}^N a_n x^n \]
è annullato da $\alpha$.
Se potessimo scomporre in modo non banale $p(x) = p_1(x)p_2(x)$ allora avremmo che $0 = p_1(\alpha)p_2(\alpha)$, dunque $\alpha$ annullerebbe uno dei due fattori (qui
stiamo usando che $A$ è un'algebra unitaria), il che sarebbe assurdo vista l'ipotesi di minimalità di $N$.
Dunque $p(x)$ deve essere irriducibile. Ma su $\R$ i polinomi irriducibili possono solo avere grado $1$ o $2$.
Vediamo rapidamente perché non può avere grado $1$.
Supponiamo per assurdo che sia
\[p(x) = a_0 + a_1 x\]
Ma ciò vorrebbe dire che $a_0+a_1\cdot\alpha=0$, ovvero $\alpha=-a_0/a_1 \in \R$, ma noi avevamo assunto $\alpha\not\in\R$.
Di conseguenza $p(x)$ ha grado esattamente $2$.
Non è difficile mostrare che $A[\alpha]$ ha esattamente dimensione $2$: se $\alpha^2$ si scrive in termini di potenze inferiori lo faranno anche tutte le potenze successive,
dunque ogni elemento di $A[\alpha]$ si può scrivere nella forma $x + \alpha y$ con $x,y$ reali e $\{1,\alpha\}$ è base di $A[\alpha]$, essendo i due elementi indipendenti.
A meno di moltiplicare $p(x)$ per una costante per renderlo monico, possiamo scrivere
\[ p(x) = (x-s)^2+t^2\]
con $s,t$ reali, $t\neq 0$. Se definiamo
\[i = \frac{\alpha-s}{t}\]
è facile osservare che $i^2 = -1$. Ora $\{1, i\}$ è base di $A[\alpha]$ (essendo $1$ e $i$ linearmente indipendenti) e il nostro $i$
gioca esattamente lo stesso ruolo dell'unità immaginaria in $\C$. A questo punto è immediato esibire un isomorfismo
\[ A[\alpha] \iso \C \]
Se $\dim A =2 $ abbiamo finito. Supponiamo d'ora in poi $\dim A > 2$ e scriviamo $\C\subset A$ identificando $\C$ con la
sottoalgebra $A[\alpha]$. Definiamo ora un'applicazione $\varphi:A\to A$ nel modo seguente:
\[ \varphi(x) = -ixi = ixi^{-1} \]
Osserviamo che $\varphi$ è $\R-$lineare, infatti presi $a,b\in\R$ e $x,y\in A$ si ha:
\[ \varphi(ax+by) = i(ax+by)i^{-1} = i(ax)i^{-1} +i(by)i^{-1} = a\varphi(x) + b\varphi(y)\]
Inoltre osserviamo che $\varphi^2$ è l'identità. Questo implica che possiamo decomporre
\[ A = A_+ \oplus A_-\] dove $A_+$ e $A_-$ sono gli autospazi relativi rispettivamente agli autovalori $1$ e $-1$.
In particolare gli elementi di $A_+$ sono esattamente quelli che commutano con $i$.
Vogliamo ora mostrare che $A_+ = \C$.
Sia $\beta\in A_+$. Imitando il ragionamento fatto prima con $\alpha$, possiamo considerare il polinomio a coefficienti complessi di minimo grado
che si annulla in $\beta$. Come fatto prima osserviamo che deve essere irriducibile (questo passaggio richiede in realtà qualche cautela,
ma tutto funziona come deve poiché $\beta$ commuta con tutti gli elementi di $\C$. Se avessimo preso $\beta\in A_-$ il ragionamento sarebbe errato).
Ma gli unici polinomi irriducibili a coefficienti in $\C$ sono quelli di grado $1$, quindi $\beta\in\C$.

Abbiamo dunque stabilito che $A_+ = \C$. Dato che $\dim A > 2$ possiamo prendere $z\in A_-$ non nullo.
Consideriamo l'applicazione $\psi_z:A\to A$ definita da $\psi_z(x) = zx$.
Osserviamo che valgono le seguenti implicazioni:
\[ x\in A_+ \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = -zx = -\psi_z(x)\]
\[ x\in A_- \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = (-z)(-x) = \psi_z(x)\]
ovvero $\psi_z$ scambia $A_+$ e $A_-$. Inoltre $\psi_z$ è bigettiva e lineare, dunque concludiamo che $A_+\iso A_-$ come $\R-$spazi vettoriali e in particolare $\dim A = 4$.

Con un ragionamento simile a quello che avevamo fatto per $\alpha$, osserviamo che $z^2$ è nel sottospazio generato da $1$ e $z$. Inoltre $z^2=\psi_z(z)\in A_+$.
Visto che $Span(1,z) \cap A_+ = \R$ deve essere per forza $z^2\in\R$.

Se fosse $z^2 \ge 0$ allora potremmo scrivere $z^2=r^2$ per qualche $r\in\R$.
Visto che $r$ e $z$ commutano sarebbe allora $(z-r)(z+r)=0$, dunque uno dei due fattori sarebbe $0$, assurdo perché $z\not\in\R$. Pertanto $z^2 < 0$ e possiamo definire
\[j = \frac{z}{\sqrt{-z^2}}\]
così $j^2 = -1$. Infine definiamo $k = ij$.

Ora $k$ e $j$ sono indipendenti e sono in $A_-$, dunque formano una base di $A_-$. Allora $\{1,i,j,k\}$ è base per $A$ ed è facile
verificare che questi quattro elementi rispettano le stesse regole di moltiplicazione dei quaternioni, pertanto si conclude che $A\iso\HH$ e la tesi è dimostrata.


\end{proof}




Dal teorema precedente si ha immediatamente che se $G\to GL(V)$ è una rappresentazione irriducibile su $\R$ allora $\End_G(V)$ è isomorfo a uno
tra $\R, \C, \HH$.

\begin{prop}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora:
\begin{itemize}
\item $\End_G(V) \iso \R$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 1$
\item $\End_G(V) \iso \C$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 2$
\item $\End_G(V) \iso \HH$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 4$
\end{itemize}
\end{prop}
\begin{proof}
Basta ricordare che $\dim \End_G(V) = \langle\chi_\rho|\chi_\rho\rangle$ (vedere il lemma \ref{lemma:dim_hom}, la cui dimostrazione funziona
anche se il campo degli scalari è $\R$) e la tesi segue subito dal teorema di Frobenius.
\end{proof}










\newpage
\section{Rappresentazioni indotte}
Supponiamo di avere $\rho:H\rightarrow GL(V)$ una rappresentazione di $H<G$ con $G$ gruppo.
Il nostro obiettivo è quello di definire una rappresentazione $\widetilde{\rho}$ del gruppo $G$ che in un certo senso estenda $\rho$.

\begin{defn}Indichiamo con \emph{induzione da $H$ in $G$ di $W$} il seguente spazio vettoriale:
\[\Ind_H^G(W)=\{f:G\rightarrow W: \rho(h)\circ f(gh)=f(g)\ \forall (g,h)\in G\times H\}   \]
\end{defn}

\begin{rem} $\Ind_H^G(W)$ è lo spazio vettoriale di una rappresentazione di $G$.
Si consideri $N=\{f:G\rightarrow W\}$: si definisce $\rho_N:G\rightarrow GL(N)$ definita da
\[ (\rho_N(g)f)(g')=f(g^{-1}g')\]
Si verifica che $\rho_N$ è una rappresentazione di $G$ su $N$. Notiamo innanzitutto che $\Ind_H^G(W)\subseteq N$ e che tale sottospazio è $\rho_N-$invariante poichè detta $f\in \Ind_H^G(W)$ si ha che
\[ (\rho_N(g)f)(g')=f(g^{-1}g')=\rho(h)f(g^{-1}g'h)=\rho(h)\rho_N(g)(f)(g'h)\]
ovvero $\rho_N(g)f\in \Ind_H^G(W)$. Questo ci dice che $\Ind_H^G(W)$ è una sottorappresentazione di $\rho_N$ che chiamiamo $\rho_{ind}:G\rightarrow GL(\Ind_H^G(W))$.
\end{rem}

\begin{exmp}
$H=\{e\}$ e $W=\C$ con $\rho:H\rightarrow GL(\C)$: allora
\[\Ind_H^G(W)=\{f:G\rightarrow \C:\rho(e)f(ge)=f(g)\}=\{f:G\rightarrow \C\}\]
\end{exmp}

Spesso indicheremo lo spazio vettoriale $\{f:G\rightarrow \C\}$ con $\C[G]=\bigoplus_{g\in G}\C e_g$ dove gli $\{e_g\}_{g\in G}$ sono una base di $\{f:G\rightarrow \C\}$
in cui gli $e_g$ sono definiti da $e_g(s) = \delta_{g,s}$.
In tal modo risulta che
\[\{f:G\rightarrow W\}=\C[G]\otimes W\]
e interpretando secondo tale isomorfismo gli elementi del prodotto tensoriale come funzioni da $G$ in $W$ si ha
\[e_g\otimes w:G\rightarrow W \textup{ tale che } g\mapsto w \textup{ e } g'\mapsto 0\ \forall g'\neq g\]

Ricapitolando $\rho_{ind}:G\to GL(\Ind_H^G)$ tale che $g\mapsto \rho_{ind}(g)$ dove $(\rho_{ind}(g)f)(g')=f(g^{-1}g')$. Vediamo ora un po' delle sue caratteristiche. Poniamo per snellire la scrittura $V=\Ind_H^G(W)$. Ricordandoci il nostro aim, di certo vogliamo che $W$ sia un sottospazio di $V$.

\begin{defn}
Data $f\in \{f:G\rightarrow W\}$ definiamo il supporto di $f$
\[\Supp(f)=\{g\in G:f(g)\neq 0\}.\]
\end{defn}

\begin{defn}
Dato $g\in G$ definiamo $V_g=\{f\in V : \Supp(f)\subseteq gH \}$.
\end{defn}

\begin{rem} Se $g'\in gH$, allora $g'H=gH\Rightarrow V_g=V_{g'}$. Quindi più che dipendere dagli elementi di $G$, gli insiemi sopra definiti dipendono dalle classi laterali di $H$ in $G$. Perciò ora parleremo non più di $V_g$ bensì di $V_{gH}$.
\end{rem}

\begin{rem} Dato che le classi laterali di $H$ sono una partizione di $G$, dette $\{g_iH\}_{i\in |G/H|}$ i rappresentanti delle classi laterali $gH$ al variare di $g$ in $G$ allora
\[G=\bigsqcup_{i=1}^{|G/H|} g_iH\]
e si vede facilmente che ogni $f\in V$ si può scrivere come combinazione lineare di $f_i\in V_{g_iH}$, definendo
  \[f_i(g)=\begin{cases}
    f(g)& \textup{se } g\in g_iH\\
    0& \textup{altrimenti}
  \end{cases}\]
Dunque si può scrivere
\[V=\bigoplus_{i=1}^{|G/H|} V_{g_iH}\]
\end{rem}
Siamo ora interessati a trovare un sottospazio isomorfo a $W$ dentro $V$. Notiamo innanzitutto che la scrittura precedente non è detto che sia una decomposizione di rappresentazioni, ma possiamo vedere subito che si comporta bene sotto l'azione di $\rho_{ind}$.

\begin{lemma}
$\rho_{ind}$ permuta le varie $V_{gH}$ nel senso che $\rho_{ind}(g)V_{g'H}=V_{gg'H}$.
\end{lemma}

\begin{proof}
  Sia $f\in V_{g'H}$ e fissiamo un $g\in G$. Allora $(\rho_{ind}(g)f)(x)=f(g^{-1}x)$, dunque
  \[(\rho_{ind}(g)f)(x)\neq 0 \text{ se e solo se } g^{-1}x\in \Supp(f)\subseteq g'H\]
  Quest'ultima condizione si riscrive: $x\in g\Supp(f)\subseteq gg'H$.\newline
  Ovvero $x\in \Supp(\rho_{ind}(g)(f))\Leftrightarrow x\in g\Supp(f)$. Ciò implica che
  \[ V_{g'H} \begin{matrix}
    \overset{\rho_{ind}(g)}{\longrightarrow}\\
    \underset{\rho_{ind}(g^{-1})}{\longleftarrow}
  \end{matrix} V_{gg'H} \]
  Essendo $\rho_{ind}(g),\rho_{ind}(g^{-1})\in GL(V)$ allora seguono iseguenti fatti:
  \begin{enumerate}
  \item $\rho_{ind}(g)V_{g'H}\subseteq V_{gg'H}$;
  \item $\rho_{ind}(g^{-1})V_{gg'H}\subseteq V_{g'H}$;
  \item $V_{g'H}\iso V_{gg'H}$, ovvero sono due spazi vettoriali isomorfi;
  \item $\rho_{ind}(g)^{-1}=\rho_{ind}(g^{-1})$;
  \end{enumerate}
  Quindi $\rho_{ind}(g)V_{g'H}=V_{gg'H}$.
\end{proof}

\begin{cor}
$\forall i,j\in |G/H|\ \ \ V_{g_iH}\iso V_{g_jH}$.
In particolare hanno tutti la stessa dimensione e quindi
\[\dim(V)=|G/H|\dim(V_{eH})=\dim(\Ind_H^G(W))\]
\end{cor}

Concentriamoci ora su $V_{eH}=\{f\in V:supp(f)\subseteq H\}$. Sia $h\in H$: allora $\rho_{ind}(h):V_{eH}\rightarrow V_{eH}$ in quanto $eH=hH$. Ciò significa che $\rho_{ind}|_{H}$ definisce una rappresentazione di $H$ in $V_{eH}$.
\begin{lemma}
La rappresentazione appena trovata di $H$ su $V_{eH}$ è naturalmente isomorfa a\\ $\rho:H\rightarrow GL(W)$.
\end{lemma}

\begin{proof} Consideriamo la seguente funzione:
\[\Phi:V_{eH}\rightarrow W\ \textup{tale che } f\mapsto f(e)\]
ovvero la funzione che associa ad un $f$ la valutazione della stessa nell'elemento neutro. Verifichiamo innanzitutto che si tratta di un omomorfismo di rappresentazione. Ricordiamo chi è $V_{eH}$:
\[V_{eH}=\{f:G\rightarrow W: \rho(h)f(gh)=f(g)\ \forall h,g\ \ \textup{e } \Supp(f)\subseteq H\}\]
$\Phi$ è un omomorfismo di rappresentazioni se e solo se $\Phi(\rho_{ind}(h)f)=\rho(h)\Phi(f)\ \forall h\in H$.
\[\Phi(\rho_{ind}(h)f)=\rho(h)\Phi(f) \Leftrightarrow (\rho_{ind}(h)f)(e)=\rho(h)(f(e))\Leftrightarrow f(h^{-1})=\rho(h)f(e)\]
ma ciò è vero in quanto $f\in \Ind_H^G(W)$. \\
Verifichiamo che $\Phi$ è iniettiva. $\Supp(f)\subseteq H$ e $\forall h\in H\ f(h)=\rho(h^{-1}f(e))$: questo ci dice che una volta che si è fissato il valore di $f(e)$ allora la funzione è univocamente determinata su $H$. D'altra parte in $G\setminus H$ la funzione è identicamente nulla, quindi è univacamente determinata su tutto $G$. Pertanto $\Phi$ è iniettiva.\\
Vediamo ora che è anche surgettiva. Sia $w\in W$: vogliamo $f\in V_{eH}$ che valutata in $e$ dia $w$. Definiamo $f_w:G\rightarrow W$ nel seguente modo:
\[f_w:=\begin{cases}
\rho(h^{-1})(w)& \textup{ se } h\in H\\
0& \forall g\in G\setminus H
\end{cases}\]
Si verifica che tale funzione soddisfa le proprietà richieste.
\end{proof}


Da quanto mostrato segue subito il seguente teorema
\begin{thm} Detti $g_1H,...,g_{|G/H|}H$ i rappresentanti delle classi laterali allora
\[\Ind_H^G(W)= \bigoplus_{i=1}^{|G/H|} V_{g_iH} = \bigoplus_{i=1}^{|G/H|}\rho_{ind}(g_i)V_{eH} \]
Inoltre $\rho_{ind}|_H$ definisce una rappresentazione di $H$ su $V_{eH}$ isomorfa a $\rho$. In particolare quindi
\[\dim(\Ind_H^G(W))=|G/H|\dim(W)\]
\end{thm}

\begin{exmp}
Vediamo ora degli esempi molto semplici.
\begin{enumerate}
\item Abbiamo visto prima che se $H=\{e\}$ e $W=\C$ allora $\Ind_H^G(W)=\C[G]$ è la rappresentazione regolare di $G$.
\item $H\neq \{e\}$ sottogruppo di $G$ e $\rho:H\rightarrow GL(W)$ la rappresentazione banale con $W=\C$. Allora
\[\Ind_H^G(\C)=\{f:G\to\C: f(gh)=f(g)\ \forall (g,h)\in G\times H\} \]
Allora $f \in \Ind_H^G(\C)$ se e solo se è costante sulle classi laterali di $H$,
quindi $\Ind_H^G(\C)=\C[G/H]$.
\[
\begin{diagram}
G          & \rTo^{f}            & \C \\
\dTo<{\pi} & \ruTo>\  &    \\
G/H        &                     &    \\
\end{diagram}
\]
$G/H$ è un insieme finito su cui $G$ agisce per moltiplicazione a sinistra\footnote{$g(g'H)=gg'H$}. Si trova che $\Ind_H^G(W)$ è la rappresentazione per permutazione associata all'azione di $G$ su $G/H$.

\textbf{Recall:} $X$ è un $G-$insieme, sia $X= \bigsqcup X_i$ la decomposizione in $G-$orbite: allora
\[\C[X]=\bigoplus_{i=1}^{\# G-orbite} \C[X_i]\]
\item $H\subseteq G$ e $\rho:H\rightarrow GL(W)$ rappresentazione banale. Nel caso in cui $W$ avesse dimensione 1 si torna all'esempio 1. Tuttavia anche se $\dim(W)>1$, $\Ind_H^G(W)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di } H\}=\C[G/H]\otimes W$
\end{enumerate}
\end{exmp}

\begin{exercise} Supponiamo di avere una somma di rappresentazioni $W=W_1\bigoplus W_2$ del sottogrupppo $H$. Allora
\[\Ind_H^G(W)=\Ind_H^G(W_1)\bigoplus \Ind_H^G(W_2)\]
\end{exercise}

\subsection{Formula di aggiunzione}
Vediamo ora una proprietà universale di queste rappresentazioni indotte. Prima abbiamo definito passando da $\rho$ a $\rho_{ind}$ sostanzialmente una applicazione
\[\Ind:Rappr(H)\rightarrow Rappr(G)\]
e abbiamo anche ``l'inversa'' ovvero la restrizione della rappresentazione ad H
\[\Res:Rappr(G)\rightarrow Rappr(H)\]
e risulta che $\Res_H^G(V)=V$ visto però come una rappresentazione di $H$ ($\rho_{res}=\rho|_H$). Che legame c'è tra queste due applicazioni?
\begin{thm}
Sia $\rho:H\rightarrow GL(W)$ una rappresentazione di $H$ e sia $\sigma:G\rightarrow GL(U)$ una rappresentazione di $G$. Allora
\[\Hom_H(W,\Res_H^G(U))\iso \Hom_G(\Ind_H^G(W),U)\]
ovvero ogni $\phi:W\rightarrow \Res_H^G(U)$ omomorfismo di $H-$rappresentazioni si estende in modo unico a un $\hat{\phi}:\Ind_H^G(W)\rightarrow U$ omomorfismo di $G-$rappresentazioni.
\end{thm}
\begin{proof} Sia $\phi:W\rightarrow \Res_H^G(U)$ un omomorfismo di $H-$rappresentazioni e sia $V:=\Ind_H^G(W)=\bigoplus_{i=1}^{|\faktor GH|} \rho_{ind}(g_i)V_{eH}$. Essendo $V_{eH}\iso W$ come $H-$rappresentazioni, allora possiamo pensare a $\phi$ come un'applicazione $\phi:V_{eH}\rightarrow \Res_H^G(U)$ che vogliamo estendere a tutto $V$. Essendo $V$ definito come una somma diretta è sufficiente definire l'omomorfismo sui blocchi.
Consideriamo la catena di applicazioni lineari
\[V_{g_iH}\overset{\rho(g_i^{-1})}{\longrightarrow} V_{eH}\overset{\phi}{\longrightarrow} \Res_H^G(U)\overset{\sigma(g_i)}{\longrightarrow} U\]
Quindi $\sigma(g_i)\phi\rho_{ind}(g_i^{-1})$ è un'applicazione lineare da $V_{g_iH}$ a $U$.\newline
Incollando tutte queste applicazioni otteniamo una $\hat{\phi}:\Ind_H^G(W)\rightarrow U$ dove
$\hat{\phi}(v)=\sigma_{g}\circ \phi\circ \rho_{ind}(g^{-1})(v)$ con $v\in V_{gH}$. Si verifica che $\hat{\phi}\in \Hom_G(\Ind_H^G(W),U)$. \\
Vediamo ora che questa estensione è unica. Siano $\hat{\phi_1}$ e $\hat{\phi_2}$ due estensioni di $\phi$: valutiamole nello stesso elemento $v\in V_{gH}$.
\[\hat{\phi_1}(\rho_{ind}(g^{-1})v)=\phi(\rho_{ind}(g^{-1})(v))=\hat{\phi_2}(\rho_{ind}(g^{-1})v)\]
dove l'uguaglianza è vera perchè $\rho_{ind}(g^{-1})(v)\in V_{eH}$. Quindi coincidendo sui singoli blocchi le due estensioni coincidono anche su $V$. E quindi l'estensione è unica.
Per finire si osserva che l'applicazione che manda $\phi$ in $\hat\phi$ è lineare.
\end{proof}

\begin{cor}[Reciprocità di Frobenius]
Sia $H$ un sottogruppo di $G$. Sia $\rho$ una rappresentazione di $G$ e $\sigma$ una rappresentazione di $H$.
Allora $\langle \chi_\rho, \chi_{\Ind\sigma}\rangle_G = \langle \chi_{\Res \rho}, \chi_\sigma\rangle_H$.
In particolare se $\rho$ e $\sigma$ sono irriducibili la molteplicità di $\rho$ in $\Ind\sigma$ coincide con la molteplicità di $\sigma$ in $\Res\rho$.
\end{cor}
\begin{proof}
Abbiamo la seguente catena di uguaglianze:
\[ \langle \chi_\rho, \chi_{\Ind\sigma}\rangle_G = \dim\Hom(\rho, \Ind\sigma) = \dim\Hom(\Res\rho, \sigma) = \langle \chi_{\Res \rho}, \chi_\sigma\rangle_H\]
che dà subito la tesi.
\end{proof}

\subsection{Le rappresentazioni dei gruppi diedrali $D_{n}$}
Per fare un esempio di come si possano usare le rappresentazioni indotte, studiamo le rappresentazioni dei gruppi diedrali.
Ricordiamo che i gruppi diedrali hanno due generatori, che chiameremo $\rho$ e $\sigma$. Il gruppo $G = D_{n}$ si scrive quindi
\[ D_{n} = \{ 1, \rho, \rho^2, ... \rho^{n-1}, \sigma, \sigma\rho, ... \sigma\rho^{n-1} | \ \sigma^2 = \rho^n = 1, \quad, \rho\sigma = \sigma\rho^{-1} \}\]
Consideriamo l'ovvio sottogruppo $H$ generato da $\rho$, che è evidentemente un gruppo ciclico, isomorfo a $\Z / n \Z$.
Le rappresentazioni di questo gruppo, dato che è abeliano, sono di grado 1 e sono le radici ennesime dell'unità.
In particolare sono univocamente determinate dai valori che assumono sul generatore. Chiamiamo con $\tau_k$ queste rappresentazioni, con $k =0, 1,\dots, n-1$, dove
\[\tau_k(\rho) = \omega^k  \qquad \omega = e^{\frac{2\pi i}{n}}\]
Studiamo ora la rappresentazione indotta da $\tau_k$, ovvero studiamo $\Ind_G^H \tau_k$, che per amore di brevità indicheremo con $\Ind \tau_k$. \`E abbastanza utile notare che noi conosciamo la dimensione della rappresentazione indotta, in quanto conosciamo la formula
\[ \dim \Ind^H_G \tau_k = |G/H| \deg \tau_k = 2 \cdot 1 = 2\]
Per andare avanti è intelligente ricordare la definizione della rappresentazione indotta. Indicheremo con $W$ lo spazio su cui agisce $\Ind \tau_k$ (che è ovviamente isomorfo a $\C^2$)
\[ W = \{ f : G \to W\ |\ f(gh) = \tau_k(h)^{-1} f(g) \quad \forall g \in G, \forall h \in H \}\]
Possiamo prendere una base di questo spazio per scrivere le matrici associate agli elementi di $G$ e calcolarne poi il carattere, per esempio. Scegliamo questa base: prendiamo le funzioni $f_H, f_{\sigma H}$ definite da
\[
\begin{cases}
f_H (e) = 1 \\
f_H (\sigma) = 0 \\
\end{cases}
\qquad
\begin{cases}
f_{\sigma H} (e) = 0 \\
f_{\sigma H } (\sigma) = 1 \\
\end{cases}
\]
Evidentemente questa è una base e, data la definizione di $W$ è anche sufficiente a determinare il comportamento delle funzioni su tutto il gruppo $G$.

In particolare una generica funzione $f \in W$ si potrà scrivere come $\C-$combinazione lineare delle due funzioni appena definite, ovvero
\[ f = a f_H + b f_{\sigma H} \qquad a,b \in \C\]
Ma possiamo ovviamente trovare i coefficienti $a,b$ esplicitamente:
\[ f(g) = f(1) f_H(g) + f(\sigma) f_{\sigma H} (g) \]
Ora che abbiamo lo spazio dove agiscono le rappresentazioni indotte, dobbiamo trovare esplicitamente le rappresentazioni. Per ogni $\tau_k$, indicheremo con $\hat \tau_k$ la sua indotta. Per vedere come è fatta, facciamola agire sulla base dello spazio.
In particolare vogliamo capire come agiscono i generatori $\rho, \sigma$ e calcolare le matrici a loro associate.

Bisogna quindi calcolare
\[
\begin{cases}
\hat \tau_k (\rho) f_H \\
\hat \tau_k (\rho) f_{\sigma H} \\
\end{cases}
\qquad
\begin{cases}
\hat \tau_k (\sigma) f_H \\
\hat \tau_k (\sigma) f_{\sigma H}\\
\end{cases}
\]
A titolo di esempio riportiamo il primo calcolo:
\[
\hat \tau_k (\rho) f_H(1) = f_H(\rho^{-1}) = \tau_k(\rho) f_H(1) = \omega^k f_H(1)
\]
Con conti analoghi si ottiene infine
\[
\hat \tau_k (\rho) =
\left(
\begin{array}{cc}
\omega^k & 0 \\
0 & \omega^{-k} \\
\end{array}
\right)
\qquad
\hat \tau_k (\sigma) =
\left(
\begin{array}{cc}
0 & 1 \\
1 & 0 \\
\end{array}
\right)
\]
A questo punto abbiamo un sacco di rappresentazioni di $D_{n}$. 
Dovremmo però capire se sono irriducibili e se alcune sono isomorfe tra loro.
Alcuni casi particolari si vedono abbastanza facilmente.
Per esempio, per $k=0$, $\hat \tau_k(\rho) = \Id$.
Di conseguenza, tutte le $\hat \tau_0(g)$ sono simultaneamente diagonalizzabili e quindi in realtà la rappresentazione non sarà irriducibile.
In particolare si vede (diagonalizzando le matrici) che $\hat\tau_0$ è somma della rappresentazione banale di grado $1$ e di un'altra irriducibile di grado $1$.

Con un ragionamento analogo si vede che se $n$ è pari allora $\hat\tau_{n/2}$ è somma di due rappresentazioni di grado $1$,
diverse da quelle che compongono $\hat\tau_0$.

Quindi abbiamo individuato $4$ rappresentazioni di grado $1$ se $n$ è pari (solo $2$ se $n$ è dispari).

Si vede facilmente che se $0<i<\frac{n}{2}$ allora $\hat\tau_i$ è irriducibile (di grado 2) e che $\hat\tau_i\iso\hat\tau_{n-i}$.
A questo punto è facile controllare che abbiamo trovato tutte le rappresentazioni irriducibili di $D_n$.
Infatti se $n$ è pari abbiamo $4$ rappresentazioni di grado $1$, $\frac{n}{2}-1$ rappresentazioni di grado 2 e 
\[ 4\cdot 1^2 + \left(\frac{n}{2}-1\right)\cdot 2^2 = 2n\]
Mentre se $n$ è dispari abbiamo $2$ rappresentazioni di grado $1$, $\frac{n-1}{2}$ rappresentazioni di grado 2 e 
\[ 2\cdot 1^2 + \left(\frac{n-1}{2}\right)\cdot 2^2 = 2n\]






\newpage
\section{Rappresentazioni di gruppi compatti}

Ci piacerebbe in qualche modo estendere quello che abbiamo fatto per i gruppi finiti ad una categoria particolare di gruppi infiniti, in particolare i gruppi compatti. Per farlo, abbiamo bisogno di un po' di definizioni che generalizzino quello che abbiamo fatto.


\begin{defn}[Gruppo topologico] Dato un insieme $G$, una funzione $\cdot: G\times G \to G$ e un sottoinsieme $\tau$ delle parti di $G$, la terna $(G, \cdot, \tau)$ si dice gruppo topologico se valgono le seguenti proprietà:

\begin{itemize}
\item $(G, \cdot)$ è un gruppo.
\item $(G, \tau)$ è uno spazio topologico.
\item Le due operazioni $\cdot : G \times G \to G$ e $i: G \to G$, la seconda definita come la mappa che manda $g $ in $g^{-1}$,  sono continue secondo la topologia indotta da $\tau$.
\end{itemize}

\end{defn}


La definizione che abbiamo dato sopra è in un certo senso l'unica che si poteva dare per mettere insieme topologia e gruppi, in quanto le prime due sono obbligate e la terza di dice che in qualche modo vogliamo che le due operazioni di gruppo e di topologia si parlino fra di loro e che non siano indipendenti. Si possono fare diversi esempi, come


\begin{itemize}
\item $G$ finito, con la topologia discreta
\item $G$ qualsiasi, con la topologia discreta
\item Il gruppo additivo dei numeri reali $(\R, +)$ con la topologia euclidea
\item Il gruppo $GL_n (\R)$ (o anche su $\C$), con la topologia indotta da quella di $M_n(\R)$

\end{itemize}


In questo corso non parleremo di rappresentazioni di gruppi topologici in generale, ma solo di alcuni, ovvero quelli compatti. Per cui diamo la definizione

\begin{defn}
$(G, \cdot, \tau)$ si dice compatto se $(G, \tau)$ è compatto.
\end{defn}


Facciamo un po' di esempi di gruppi compatti che andremo a trattare
\begin{itemize}
\item I gruppi finiti, con la topologia discreta.
\item I gruppi $O_n, SO_n, U_n, SU_n$: dimostriamo che effettivamente lo sono (lo dimostriamo per $O_n$ perchè per $U_n$ la dimostrazione è analoga e gli altri sono sottogruppi).
Se $G\subseteq \R^m$ dotato della topologia euclidea indotta allora
\[G\ \text{compatto}\ \Leftrightarrow G\ \text{è chiuso in}\ \R^m\ \text{e limitato}\]
Considero $O_n(\R)\subseteq \R^{n^2}$: esso è così descritto
\[O_n(\R)=\{A\in M_n(\R):\ A^tA=I\}=\{A\in M_n(\R):\ \sum_{h=1}^{n}a_{ih}a_{jh}=\delta_{ij}\ \forall i,j=1,..,n\}\]
$\Rightarrow O_n$ è chiuso poichè descritto da equazioni (risulta essere l'intersezione finita di cotroimmagini di chiusi $\{1\},\{0\}$). Inoltre per $i=j\Rightarrow \sum_{h=1}^n a_{ih}^2=1\Rightarrow |a_{ij}|\leq 1\ \forall i,j\Rightarrow O_n$ è limitato.
\end{itemize}

Può essere utile ricordare alcuni isomorfismi di gruppi topologici importanti come:
\begin{align*}
SO_2(\R) \iso \mathbb{S}^1 \\
SU_2(\C) \iso \mathbb{S}^3
\end{align*}

A questo punto sarà opportuno dare una nozione di rappresentazione in cui entra in gioco la topologia di $G$, oltre alla sua struttura di gruppo.
\begin{defn}[Rappresentazione continua]
Consideriamo una rappresentazione del gruppo topologico $G$, $\rho: G \to GL(V_\rho)$. Diciamo che la rappresentazione è continua se la mappa
\[ \rho: G \to GL(V_\rho)\]
è continua. Per il primo termine bisogna considerare la topologia su $G$, che è definita in quanto il gruppo è topologico, mentre su $GL(V_\rho)$ si usa la topologia euclidea.
\end{defn}

Facciamo alcuni esempi
\begin{itemize}
\item Se $G$ è finito, ovviamente con la topologia discreta ogni rappresentazione è continua. In un certo senso questo non è così banale, in quanto ci dice che quello che andremo a fare sarà una generalizzazione di quello che abbiamo fatto per gruppi finiti.
\item Un'altra rappresentazione del tutto ovvia: la rappresentazione del gruppo $GL(V)$ in $GL(V)$ tramite l'identità, che è evidentemente continua.
\item La mappa esponenziale $e^x: (\R, +) \to (\R^+, \cdot )$ che associa $x\mapsto e^x$.
\end{itemize}
Nel seguito di questa sezione, parlando di rappresentazioni, intenderemo sempre rappresentazioni continue.

\subsection{Le rappresentazioni di $\mathbb{S}^1$}
Per chiarirci le idee, andremo a studiare nel dettaglio un gruppo particolare, ovvero $\mathbb{S}^1$, che è ovviamente isomorfo a $SO_2(\R)$. Cerchiamo quindi innanzitutto le sue rappresentazioni irriducibili su spazi di dimensione finita sui complessi.

\begin{lemma}
Tutte le rappresentazioni irriducibili $/_\C$ di un gruppo compatto abeliano $G$ hanno dimensione 1. Di conseguenza, essendo $\mathbb{S}^1$ un gruppo compatto abeliano, tutte le rappresentazioni di $\mathbb{S}^1$ irriducibili su uno spazio vettoriale complesso hanno dimensione 1.
\end{lemma}
\begin{proof}
  $G$ è un gruppo compatto abeliano e sia $\rho:G\rightarrow GL(V)$ rappresentazione.
  \[ gh = hg \qquad \forall g,h \in G \ \Rightarrow\ \rho(g)\rho(h) = \rho(h)\rho(g) \qquad \forall g, h \in G\]
  Il che vuol dire che, fissato $h$, $\rho(h)$ è un'omomorfismo di rappresentazioni irriducibili su $\C$, ovvero appartiene a $\End_{\mathbb{S}^1}(V)$ proprio perchè commuta con $\rho(g)$ per tutte le $g$. Sia $\lambda$ un autovalore di $\rho(h)$ (esiste perchè siamo su $\C$): allora $\rho(g)-\lambda \Id\in \End_{\mathbb{S}^1}(V)$ e ha un nucleo: questo coincide con $V$ essendo $V$ irriducibile. Allora $\rho(h) \equiv \lambda(h) \Id$. Quello che abbiamo fatto vale per ogni $h \in G$, per cui tutti i $\rho(g) $ sono in realtà scalari. Questo ci dice che ogni base di $V$ è una base di autovettori per $\rho(g)\ \forall g\in G$ e quindi ogni autospazio è $G-$invariante. Dunque le rappresentazioni irriducibili di $G$ sono tutte di grado 1.
\end{proof}


\begin{prop}
  Le rappresentazioni irriducibili di $\mathbb{S}^1$ sono tutte e sole le $\rho_n:\mathbb{S}^1\to\C^*$ definite, al variare di $n$ in $\Z$, nel seguente modo:
  \[ \rho_n(z) = z^n \]
\end{prop}


\begin{proof}
  Innanzitutto notiamo che le $\rho_n$ definite nell'enunciato sono effettivamente delle rappresentazioni irriducibili di $\mathbb{S}^1$, quindi bisogna solo verificare che non ce ne sono altre.

  Sappiamo che le rappresentazioni irriducibili dovranno avere dimensione $1$, per cui saranno omomorfismi $\rho:\mathbb{S}^1\to\C^*$. Inoltre dovrà per forza essere $|\rho(z)| = 1 \quad \forall z \in \mathbb{S}^1$. Questo si dimostra abbastanza facilmente. Supponiamo per assurdo infatti che si abbia
  \[ |\rho(x)| > 1\]
  per qualche $x$. Allora anche $\rho(x)^n$ farebbe parte di $\rho(G)$. Tuttavia qui c'è un assurdo in quanto $\rho(G)$ è compatto perché la rappresentazione è continua (compatti vanno in compatti), mentre $\rho(x)^n$ non è limitato. Se invece per assurdo fosse $|\rho(x)| < 1$ si potrebbe usare lo stesso argomento notando che $|\rho(x^{-1})| > 1$.
  Di conseguenza sappiamo che $\rho(\mathbb{S}^1) \subseteq \mathbb{S}^1$.
  Consideriamo ora la mappa
  \begin{align*}
    \phi : (\R, +) &\to \mathbb{S}^1 \\
    x &\to  e^{ix}
  \end{align*}
  che è evidentemente continua ed è omomorfismo di gruppi (è un rivestimento).
  Sia $\hat\rho$ l'applicazione che fa commutare il seguente diagramma:
  \[\tridiag \R \phi {\mathbb{S}^1} \rho {\mathbb{S}^1} {\hat\rho}\]
  Essendo composizione di omomorfismi continui, anche $\hat\rho$ è omomorfismo continuo. Si può dimostrare (ma non lo facciamo in questa sede) che esiste un'applicazione continua $\theta:\R\to\R$ che fa commutare il diagramma:
  % ~ Si può dimostrare che questa mappa è l'unica che soddisfi le proprietà del rivestimento universale da $\R$ a $\mathbb{S}^1$. Ci è utile perché in qualche modo scarica il problema di trovare le rappresentazioni da e verso $\mathbb{S}^1$ a cercare delle rappresentazioni da e verso $(\R, +)$. Per l'unicità del rivestimento concluderemo che sono anche tutte.
  \[
  \begin{diagram}
    \R          & \rTo^{ \phi }         & \mathbb{S}^1           \\
    \dTo<{\theta}   & \rdTo^{\hat \rho}     & \dTo>{\rho}            \\
    \R          &  \rTo_{\phi}          & \mathbb{S}^1           \\
  \end{diagram}
  \]
Ovvero che fattorizza $\hat\rho=\phi\theta$. A questo punto $\theta(0)$ deve essere un multiplo intero di $2\pi$, visto che deve risultare $e^{i\theta(0)} = \hat\rho(0) = 1$.
Quindi, a meno di traslare $\theta$ di un multiplo intero di $2\pi$ (il che non interferisce con la commutatività del diagramma), possiamo assumere $\theta(0) = 0$. Dati $x,y$ reali deve valere
\[ e^{i\theta(x+y)} = \hat \rho(x+y) = \hat \rho(x )\cdot \hat \rho(y) = e^{i\theta(x)+i \theta(y)}\]
per cui deve essere
\[ \theta(x+y) - \theta(x) - \theta(y) \in 2\pi \Z \qquad \forall x, y \in \R \]
Ma visto che $\theta$ è continua l'espressione sopra deve essere necessariamente costante al variare di $x$ e $y$. Ponendo $x=y=0$ ricaviamo che tale costante è $-\theta(0)=0$, dunque
\[ \theta(x + y) = \theta(x ) + \theta (y) \qquad \forall x,y\in \R\]
A questo punto si osserva (sfruttando la continuità di $\theta$) che deve essere $\theta(x) = \theta(1) x$, con $\alpha \in \R$. Non solo, possiamo dare delle informazioni a riguardo di $\theta(1)$. Infatti, dato che $\hat\rho$ è ``periodico'', deve essere
\[ 1 = \hat\rho(0) = \hat\rho(2\pi) = e^{i\theta(1) 2\pi}\]
per cui in realtà si ha $\theta(1) \in \Z$.
Ricapitolando, abbiamo mostrato che per ogni $x\in\R$ vale che
\[e^{i \theta(1)x} = \rho(e^{ix}) \]
dove $\theta(1)$ è un intero fissato. Ovvero per ogni $z\in\mathbb{S}^1$ vale
\[ \rho_{\theta(1)}(z) = z^{\theta(1)} = \rho(z) \]
che è quello che volevamo mostrare.

\end{proof}

A questo punto il nostro obiettivo sarebbe quello di ripetere la stessa cosa che abbiamo fatto per i gruppi finiti, ovvero scomporre una rappresentazione generica come somme di rappresentazioni irriducibili. Per farlo in sostanza abbiamo inventato quel prodotto hermitiano invariante che da solo ci ha permesso praticamente di fare tutto. Sarebbe molto bello avere una cosa simile anche per questi gruppi topologici. In particolare per i gruppi compatti possiamo definire qualcosa di molto simile.


\begin{defn}[Integrazione su un gruppo]
Sia $G$ un gruppo topologico. Consideriamo un'applicazione $I: C_{\R}(G) \to \R$
dove con $C_\R$ si intendono le funzioni continue da $G$ in $\R$\footnote{Tutto funziona allo stesso modo se al posto di $\R$ mettiamo $\C$. In futuro non si baderà a questa distinzione.}. La mappa $I$ si dice integrazione se rispetta le seguenti proprietà.
\begin{itemize}
\item \`E lineare: $I(a f + b g) = aI(f) + bI(g)$.
\item Se la funzione $f$ è positiva ($ > 0$), allora anche l'integrale deve essere positivo ($> 0$).
\end{itemize}
\end{defn}


A questo punto, in analogia con quello che abbiamo fatto con i prodotti finiti, cerchiamo di trovare quello invariante.

%\begin{defn} Sia $G$ un gruppo compatto e $\rho: G \to GL(V_\rho)$ una sua rappresentazione continua. Sia $I$ una %ntegrazione su $G$. Si dice che $I$ è invariante sotto $\rho$ se
%\[ I(f(x)) = I(f(\rho(g) x)) \qquad \forall g \in G\]
%\end{defn}
\begin{defn} Sia $G$ un gruppo compatto e $I$ una integrazione su $G$. Si dice che $I$ è $G$-\textit{invariante} se $\forall f\in C_{\R}(G), \forall g\in G$ vale che
\[ I(f) = I(L_g(f)) = I(R_g(f))\]
dove $(L_g(f))(x) = f(g^{-1}x)$ e $(R_g(f))(x) = f(xg)$.
\end{defn}
Se questa integrazione esiste, allora in analogia a quanto fatto per i gruppi finiti possiamo sperare che le rappresentazioni di un gruppo compatto siano completamente irriducibili.


\begin{exmp}Alcuni esempi di integrazioni invarianti sono:
	\begin{enumerate}
		\item Nel caso dei gruppi finiti $I(f)=\frac{1}{|G|}\sum_{g\in G}f(g)$ è una integrazione invariante.
		\item Definiamone esplicitamente una su $\mathbb{S}^1$
		\[\dint_{\mathbb{S}^1}f(g)\ dg:=\frac{1}{2\pi}\dint_0^{2\pi}f(e^{i\theta})\ d\theta\]
		Le verifiche che sia una integrazione a tutti gli effetti sono banali: infatti che sia lineare e la positività derivano dalle proprietà dell'integrazione su $\R$ e l'invarianza deriva dal fatto che far agire $\mathbb{S}^1$ significa solo ruotare di un angolo ovvero riparametrizzare la circonferenza.
		Il diviso $2\pi$ è dovuto al fatto che vogliamo in generale avere una integrazione invariante normalizzata ovvero
		\[\dint_{\mathbb{S}^1}1\ dg=1\]
		Nel caso dei gruppi finiti dividevamo per la cardinalità del gruppo: ora che si parla di gruppi compatti possiamo immaginare che si divida per la ``misura'' del gruppo.
	\end{enumerate}
\end{exmp}

\begin{lemma}
  Se esiste l'integrazione invariante su un gruppo, allora esiste una forma hermitiana definita positiva invariante sotto l'azione del gruppo.
\end{lemma}
\begin{proof}
  Sia $h$ una forma hermitiana definita positiva su $V_\rho$. Consideriamo la quantità
  \[ h_G(v,w) = I( h(\rho(g) v, \rho(g) w)) \]

  Evidentemente è di nuovo una forma hermitiana definita positiva ed è invariante sotto $G$.


\end{proof}




\begin{thm} Sia $G$ un gruppo compatto e $\rho : G \to GL(V_\rho)$ una sua rappresentazione continua, con $V_\rho$ di dimensione finita. Se esiste una integrazione invariante $I$, allora la rappresentazione è completamente riducibile.

\end{thm}

\begin{proof}
  La dimostrazione è identica alla dimostrazione \ref{thm:gruppo finito completamente riducibile}, usando la forma hermitiana invariante appena definita.
\end{proof}

Ma esiste sempre un'integrazione invariante? Il seguente teorema, che non dimostriamo, dà la risposta affermativa almeno per i gruppi compatti.
\begin{thm}[Teorema di Haar]
Per ogni gruppo compatto esiste l'integrazione invariante.
\end{thm}

Definiamo ora in $C(\mathbb{S}^1)$ un prodotto hermitiano come segue
\[(f_1,f_2):=\dint_{\mathbb{S}^1}f_1(g)\overline{f_2(g)}\ dg\]
\`E una buona definizione ed è $\mathbb{S}^1$ invariante.
Osserviamo che vale per $\mathbb{S}^1$ un fenomeno analogo a quello che sappiamo valere per i gruppi finiti:
\[(\rho_n,\rho_m)=\dint_{\mathbb{S}^1}\rho_n(g)\overline{\rho_m(g)}\ dg=\frac{1}{2\pi}\dint_0^{2\pi}e^{in\theta}e^{-im\theta}\ d\theta=\frac{1}{2\pi}\dint_0^{2\pi}e^{i\theta (n-m)}\ d\theta \]
\[\Rightarrow (\rho_n,\rho_m)=\left\{\begin{matrix}
1 & n=m\\
0 & n\neq m
\end{matrix}\right.\Rightarrow (\rho_n,\rho_m)=\left\{\begin{matrix}
1 & \rho_n\iso \rho_m\\
0 & \rho_n \noniso \rho_m
\end{matrix}\right.\]
ovvero $\{\rho_n\}_{n\in \N}$ formano un sistema ortonormale.

\begin{cor} Sia $\rho: \mathbb{S}^1\rightarrow GL(V)$ una rappresentazione: allora
\[\rho\iso \sum_n a_n\rho_n\ \ \text{con}\ n\in \N\ \text{ed}\ a_n\in \Z\]
dove $a_n=(\rho,\rho_n)$.
\end{cor}





\subsection{Le rappresentazioni di $SU(2)$}



Dopo aver studiato in dettaglio $\mathbb{S}^1$, vediamo come si comporta un suo parente stretto, $SU(2)$. Ricordiamo che $SU(2)$ è definito come un sottoinsieme di $GL(\C^2)$. In particolare ogni matrice di $SU(2)$ si può scrivere come
 \[ SU(2) = \left\{  \left(\begin{array}{cc} z_1 & z_2 \\ -\overline{z_2} & \overline z_1 \end{array}\right) \quad z_1, z_2 \in \C  , \qquad |z_1|^2 + |z_2|^2 = 1\right\} \]
 È abbastanza naturale considerare $\C^2 \iso \R^4$ e accorgersi che in sostanza $SU(2) \iso \mathbb{S}^3$, in quanto tutti gli elementi rispettano l'equazione
 \[ x_1 ^2 + x_2^2 + x_3^2 + x_4^2 = 1 \]
E scrivendo in questo modo ci accorgiamo subito che quindi $SU(2)$ è un gruppo compatto e connesso.

Grazie ai teoremi visti nella sezione precedente, quindi,
ogni rappresentazione di $SU(2)$ è scomponibile in una somma di rappresentazioni irriducibili.

Nella sezione \ref{sec:quaternioni} abbiamo visto come i quaternioni possono essere visti come un sottoinsieme di $M_2(\C)$
\[
\mathbb{H}= \left\{\begin{pmatrix}
 z_1& z_2\\
 -\overline{z_2}& \overline{z_1}
\end{pmatrix}\in M_2(\C) \right\}
\]
Quindi $SU(2)$ può essere immerso in $\HH$ avendo le matrici la stessa struttura.\\
 MANCA LA PARTE SU $\rho_H$ MA NON SO QUANTO SERVA.



Come abbiamo fatto per $\mathbb{S}^1$, vorremmo andare a studiare tutte le sue rappresentazioni irriducibili. Per farlo, innanzitutto sarebbe intelligente farsi un'idea di come può essere fatta l'integrazione invariante su questo gruppo. Se lo immergiamo in $\R^4$, possiamo sfruttare l'integrazione lì definita e quindi fare un integrale triplo

 \[ I(f) = \dint_{\mathbb{S}^3} f(g) dg = \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi\]


L'invarianza dell'integrazione sotto l'azione di $G$ deriva sostanzialmente dal fatto che quando si cambia variabili bisogna moltiplicare l'integrando per il determinante dello Jacobiano. Tuttavia $det(SU(2)) = 1$ e quindi non ci sono problemi.

\paragraph{Rappresentazioni irriducibili}
Vediamo ora di classificare tutte le rappresentazioni irriducibili di $SU(2)$.
Consideriamo l'azione di $SU(2)$ sullo spazio $V_m = \C[x,y]_m$ dei polinomi omogenei su $\C$ di grado $m$ definita in questo modo

 \[
 \rho_m(A) f(x, y) = f(a_{11} x + a_{21} y , a_{12} x + a_{22} y) \qquad \forall f \in V_m, \forall A \in SU(2)
 \]

\begin{prop} $\rho_m$ è una rappresentazione irriducibile di $SU(2)$ $\forall m\in \N$.
\label{prop:irrid su2}
\end{prop}
\begin{proof} Cerchiamo di trovare un sottogruppo di $SU(2)$ di cui conosciamo già le rappresentazioni irriducibili e studiamo le $\rho_n$ ristrette a tale sottogruppo. Consideriamo il seguente insieme delle matrici speciali unitarie diagonali
\[T=
\left\{\begin{pmatrix}
z & 0\\
 0& \overline{z}
\end{pmatrix}\in M_2(\C):\ |z|^2=1 \right\}\subseteq SU(2)\Rightarrow T= \left\{\begin{pmatrix}
e^{i\theta}& 0\\
 0& e^{-i\theta}
\end{pmatrix}: 0\leq \theta \leq 2\pi \right\}\]

\'E evidente che $T$ sia isomorfo a $\mathbb{S}^1$. Dunque $\rho_m|_{\mathbb{S}^1}$ è una rappresentazine di $\mathbb{S}^1$ su $V_m$. Come agiscono le matrici in $T$? Per vederlo, consideriamo la base "canonica" di $V_m$, formata dai monomi $\{ x^ky^{m-k} \}_{k \in \{0, ..., m\}}$ e scriviamo
\[V_m=\bigoplus_{k=0}^m \C (x^k y^{m-k})\]
Dato un $\theta\in [0,2\pi]$ allora
\[\rho_m\begin{pmatrix}
e^{i\theta} & 0\\
 0& e^{i\theta}
\end{pmatrix} (x^ky^{m-k}) = (xe^{i\theta})^k(ye^{i\theta})^{m-k}=e^{i(m-2k)} x^k y^{m-k} \]

Ovvero, quello che abbiamo appena scoperto è che la base considerata è formata da autovettori per il sottogruppo $T$. Quindi, denotando con $\rho_n^{\mathbb{S}^1}$ le rappresentazioni irriducibili di $\mathbb{S}^1$, allora $\rho_m|_{\mathbb{S}^1}=\sum_{k=0}^{m}\rho_{m-2k}^{\mathbb{S}^1}$.

Quindi ora di $V_m$ abbiamo una duplice scrittura:
\[V_m=\bigoplus_{k=0}^m \C (x^k y^{m-k})=\bigoplus_{k=0}^m V_{\rho_{m-2k}^{\mathbb{S}^1}}\]
dove la seconda scrittura è una somma diretta di rappresentazioni di $T$ irriducibili e a 2 a 2 non isomorfe. Consideriamo $W\neq \emptyset \subseteq V_m$ sottospazio stabile per $SU(2)$: iniziamo con il dimostrare il seguente fatto:

\textbf{Fatto:} Sia $\pi_k:V_m\rightarrow \C [x^{m-k}y^k]$ la proiezione di un vettore sulla sua componente $(m-k)-$esima. Allora se $\pi_k|_W\neq 0$ allora $V_{m,k}:=\C[ x^{m-k}y^k]\subseteq W$.

Dato che  $\{ x^ky^{m-k} \}_{k \in \{0, ..., m\}}$ sono una base di autovettori per le matrici in $T$ allora $\forall k,\ \pi_k\in \Hom_T(V_m,V_{m,k} )\Rightarrow \pi_k|_W\in \Hom_T(W,V_{m,k})$. Se $\pi_k|_W\neq 0\Rightarrow W=\Ker(\pi_k|_W)\bigoplus W_k$ dove la $\dim(W_k)$ è forzata ad essere 1. Quindi $W_k\iso V_{m,k}$ come rappresentazioni. Tuttavia $V_{m,k}$ è l'unico sottospazio invariante per $\rho_{m-2k}^{\mathbb{S}^1}\Rightarrow V_{m,k}=W_k$.


Quindi dato che abbiamo supposto il nostro $W$ diverso dal vuoto, esiste un $k$ tale che $x^{m-k}y^k\in W$. Se mostriamo che $x^m\in W$ allora abbiamo concluso poichè data la generalità di $W$ avrei dimostrato che $x^m\in$ a tutti i sottospazi di $V_m$ non vuoti e invarianti per $SU(2)$. Quindi se $W$ fosse diverso da $V_m$ avrei $V_m=W\bigoplus U$ ma con $U\neq \emptyset \Rightarrow x^m\in U\cap W$ il che è assurdo poichè devono essere in somma diretta. Per mostrare che $x^m\in W$ riusiamo il fatto e ci riduciamo a dover dimostrare che $\pi_0|_W\neq 0$. Considero al variare di $g\in SU(2)$ il vettore $\pi_0(\rho_m(g)(x^{m-k}y^k))$ dove $x^{m-k}y^k$ è il monomio che siamo sicuri appartenga a $W$. Bene, ma allora mi basta scegliere
\[g=\frac{1}{\sqrt{2}}\begin{pmatrix}
1& -1\\
 1& 1
\end{pmatrix}\Rightarrow \rho_m(g)(x^{m-k}y^k)=x^m\]
Quindi $W=V_m$. Le $\rho_m$ sono irriducibili.
\end{proof}




 \paragraph{Le classi di coniugio di $SU(2)$}
 Dato che abbiamo dato tanto peso alla teoria del carattere per i gruppi finiti, probabilmente anche per i gruppi compatti ci saranno applicazioni interessanti. Dato che il carattere è una funzione di classe, è intelligente andare a studiare nel dettaglio le classi di coniugio di $SU(2)$.


 Abbiamo già visto che possiamo immergere $\mathbb{S}^1$ in $SU(2)$. Vediamo come questo abbia a che fare con le classi di coniugio.


 Ricordiamo che possiamo scrivere un generico punto di $\mathbb{S}^3$ come

 \[
 \begin{cases}
 x_1 = \cos\theta \\
 x_2 = \sin\theta\cos\phi \\
 x_3 = \sin\theta\sin\phi\cos\psi \\
 x_4 = \sin\theta\sin\phi\sin\psi \\
 \end{cases}
 \]

 Per cui possiamo scrivere il generico elemento di $SU(2)$ come

 \[
 g(\theta, \phi, \psi) =
 \left(
 \begin{array}{cc}
   \cos\theta + i \sin\theta \cos\phi & \sin\theta\sin\phi( \cos\psi + i \sin\psi) \\
   -\sin\theta\sin\phi(\cos\psi - i \sin\psi) & \cos\theta - i \sin\theta\cos\phi  \\

 \end{array}
 \right)
 \]


 Dato che le matrici di $SU(2)$ sono normali, possiamo usare il teorema spettrale normale per dire che sono tutte unitariamente diagonalizzabili, ovvero $\forall g \in SU(2) \exists h \in U(2) \  | \ hgh^{-1} = m$, con $m$ diagonale. Tuttavia possiamo restringerci ad avere anche $h$ in $SU(2)$ scegliendo $h' = \frac{1}{\sqrt{det(h)}} h$. Per questo motivo le classi di coniugio saranno tutte matrici di Jordan diagonali, senza 1 sulla sopradiagonale. Per questo motivo la classe di coniugio sarà univocamente determinata dagli autovalori della matrice. Il polinomio caratteristico di una matrice $2\times 2$ è

 \[ p(t) = t^2 - \tr(M) t + det(M) \]

 Ma dato che siamo in $SU(2)$ il determinante è 1. La traccia è invece con una banale somma $2\cos\theta$. I due autovalori hanno modulo 1 e quindi in sostanza sono $e^{i\theta}$ e $e^{-i\theta}$. Per questo motivo un generico elemento di $SU(2)$ è


 \[
 g(\theta, \phi, \psi) \sim \left(
 \begin{array}{cc}
   e^{i\theta} & 0 \\
   0 & e^{-i\theta} \\
 \end{array}
 \right) = g(\theta, 0 , 0)
 \]




 Per questo motivo la funzione $\frac{1}{2} \tr(g) : SU(2) \to \R$ è una funzione suriettiva in $[-1,1]$ e le sue fibre sono le classi di coniugio.

 \paragraph{Il carattere delle rappresentazioni irriducibili di $SU(2)$}


 Prendiamo una funzione $f$

 \[ f \in \C(SU(2)) ^\# = \{ \text{Funzioni di classe } f: SU(2) \to \C  \} \]

 Sappiamo che se questa funzione è di classe, allora non dipende dal rappresentante della classe di coniugio. Per questo motivo possiamo quindi dire che

 \[ f(g(\theta, \phi, \psi)) = f(g(\theta, 0 , 0)) \]
 Vorremmo andare a studiare come si comporta l'integrazione invariante su questa funzione e poi considerare il caso particolare in cui questa funzione sia per l'appunto il carattere di $\rho$, una generica rappresentazione di $SU(2)$.

 \begin{align*}
 \dint_{\mathbb{S}^3} f(g) dg &= \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &=\dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, 0, 0) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &= \dfrac{2}{\pi} \dint_0^\pi f(\theta, 0, 0) \sin^2\theta \ d\theta = \\
  &= \dfrac{1}{\pi} \dint_0^{2\pi} f(\theta, 0, 0) \sin^2\theta \ d\theta
 \end{align*}

 Quindi l'espressione si semplifica parecchio. Prendiamo ora per l'appunto una rappresentazione di $\rho$ e vediamo come si comporta il carattere. In particolare prendiamo una delle $\rho_m$ irriducibili che abbiamo trovato prima. Sappiamo che

 \[ \rho_m: SU(2) \to GL(V_m) \qquad V_m = \bigoplus_{k=0}^m V_{m,k}\]

 In particolare siamo già riusciti a diagonalizzare $\rho_m$. Avremo che

 \begin{align*}
   \chi_{\rho_m} (\theta, \phi, \psi) &= \chi_{\rho_m}(\theta) = \chi_{\rho_m}(e^{i\theta}) = \\
   &= \dsum_{k=0}^m \rho_{m-2k}^{\mathbb{S}^1}(e^{i\theta}) = \dsum_{k=0}^m e^{i(m-2k)} \\
   &= \dfrac{e^{i(m+i)\theta} - e^{-i(m+1)\theta}}{e^{i\theta} - e^{-i\theta}} = \dfrac{\sin((m+1)\theta)}{\sin\theta}
 \end{align*}



 Per cui, in analogia a quanto fatto per i gruppi finiti, diamo la definizione di

 \begin{defn}[Prodotto hermitiano di caratteri]

   \[
   \langle \chi_\rho | \chi_\sigma \rangle = \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg
   \]

 \end{defn}

 Che andiamo prontamente a calcolare in questo caso

 \[
 \langle \chi_{\rho_n} | \chi_{\rho_m} \rangle = \dfrac{1}{\pi}\dint_0^{2\pi} \dfrac{\sin((n+1)\theta)}{\sin\theta} \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta \ d\theta = \delta_{mn}
 \]


 Cosa molto simile a quanto era già successo per i gruppi finiti. A questo punto viene il sospetto che possa esserci qualcosa di più profondo sotto. Vedremo fra poco dei teoremi che generalizzano questi risultati.








 \begin{thm}
   \label{thm:ortogonalita compatto}
   Sia $G$ un gruppo compatto  e siano $\rho$ e $\sigma$ due sue rappresentazioni irriducibili. Allora vale
   \[
   \langle \chi_\rho | \chi_\sigma \rangle =
   \begin{cases}
     1 \qquad \text{se } \sigma \iso \rho \\
     0 \qquad \text{altrimenti} \\
   \end{cases}
   \]
 \end{thm}

 Per dimostrare il teorema appena enunciato ci servono un paio di lemmi:

 \begin{lemma}
   Siano $\rho: G \to GL(V)$ e $\sigma: G \to GL(W)$ due rappresentazioni\footnote{Non necessariamente irriducibili} di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$ un omomorfismo di spazi vettoriali\footnote{Per gli amici, un'applicazione lineare.}. Allora possiamo definire un $\phi^G \in \Hom_G(V, W) $ nel seguente modo

   \[ \langle \phi^G(v) | w \rangle = \dint_G \langle \sigma(g) \phi (\rho(g) v) | w\rangle dg\]

   Dove $\langle \cdot | \cdot \rangle : W \times W \to \C$ è una forma hermitiana definita positiva.

 \end{lemma}

 \begin{proof}
   Dobbiamo in sostanza dimostrare che l'omomorfismo che abbiamo definito è anche un omomorfismo di rappresentazioni, ovvero che è $G-$equivariante, in quanto la linearità è assicurata dal prodotto hermitiano. In formule, dobbiamo mostrare che
   \[ \phi^G (\rho(g) v) = \sigma(g) \phi^G(v) \qquad \forall g \in G, \forall v \in V \Leftrightarrow \phi^G(v) = \sigma(g) ^{-1} \phi^G(\rho(g) v) \qquad \forall g \in G, \forall v \in V \]

   In sostanza la tesi segue dalla definizione sfruttando il fatto che l'integrazione su $G$ sia invariante


   \[ \langle \sigma(g) ^{-1} \phi^G(\rho(g) v) | w \rangle  = \dint_G \langle \sigma(hg) ^{-1} \phi^G(\rho(hg) v) | w \rangle dh = \langle \phi(v) | w \rangle\]

   Dove ho saltato un paio di passaggi ma il succo è che essendo l'integrazione invariante per traslazioni di $G$, è evidente che l'oggetto definito in questo modo diventa invariante sotto $G$
 \end{proof}

 \begin{lemma}
   Siano $\rho: G \to V$ e $\sigma: G \to W$ rappresentazioni irriducibili di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$. Allora

   \[
   \begin{cases}
     \phi^G = 0 \qquad \text{se } \rho \noniso \sigma \\
     \phi^G = \frac{\tr \phi}{\dim V}\Id_V \qquad \text{altrimenti} \\
   \end{cases}
   \]

 \end{lemma}

 \begin{proof}
   Nel caso in cui le due rappresentazioni non siano isomorfe, la tesi segue banalmente dal lemma di Schur. Nell'altro caso, possiamo di nuovo usare Schur e dire che $V \iso W$ come spazi vettoriali, ma anche che in un certo senso sono lo stesso spazio, in quanto $\phi = \lambda \Id$, sempre per Schur. L'unica cosa che rimane da mostrare è che vale proprio $\tr \phi^G = \tr \phi$. Per mostrarlo, possiamo fissare una base ortonormale di $V$, $\{ e_n\}_{n\in I}$ e notare che in questa base

   \[ \tr(\psi) = \dsum_{n \in I}\langle \psi e_n | e_n \rangle \qquad \forall \psi \in \End(V)\]
   Per cui,

   \begin{align*} \tr \phi^G &= \dsum_{n \in I}\langle \phi^G e_n | e_n \rangle = \dsum_{n \in I} \dint_G \langle \rho(g)^{-1} \phi(\rho(g) e_n) | e_n \rangle dg = \\
     &= \dsum_{n \in I}\dint_G \langle \phi(\rho(g) e_n) | \rho(g) e_n \rangle  dg = \dsum_{n \in I} \dint_G \langle \phi e_n |  e_n \rangle dg = \\
     &= \dsum_{n\in I} \langle \phi e_n | e_n \rangle = \tr \phi
   \end{align*}


 \end{proof}


 \begin{proof}[Dimostrazione del teorema \ref{thm:ortogonalita compatto}]
 Fissiamo una base ortonormale di $V$, $\{v_n\}$, e una base ortonormale di $W$, $\{w_n\}$. In base a quanto detto poco fa, avremo che
 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle &= \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg = \dint_G \tr(\rho(g)) \tr(\sigma(g)^{-1}) dg = \\
   &= \dint_G \dsum_{i \in I} \langle \rho(g) v_i | v_i \rangle \dsum_{j \in J} \langle \sigma(g)^{-1} w_j | w_j \rangle dg = \\
   &= \dsum_{i,j} \dint_G \left\langle \rho(g) \langle \sigma(g)^{-1} w_j | w_j \rangle v_i | v_i \right\rangle dg
 \end{align*}

 Se definiamo
 \[ \phi_{ij} \in \Hom(W, V), \quad \phi(w) = \langle w | w_j \rangle v_i \]

 e con quello che abbiamo fatto nel lemma poco sopra definiamo anche $\phi^G_{ij}$, possiamo notare che in effetti l'espressione poco sopra è proprio $\phi^G_{ij}$, ovvero

 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | w_j \rangle
 \end{align*}

 Se $\rho$ e $\sigma$ non sono isomorfe, allora si ha identicamente $\phi_{ij}^G = 0$ per ogni $i$ e $j$. Di conseguenza $\langle \chi_\rho | \chi_\sigma \rangle = 0$. Altrimenti, al solito posso identificare $V \iso W$ e possiamo usare l'altra parte del lemma precedente per dire che

\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | v_j \rangle = \dsum_{i,j} \dfrac{\tr \phi_{ij}}{\dim V} \langle v_i | v_j \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{\dim V} \delta_{ij}
 \end{align*}

Tuttavia in sostanza la matriche che rappresenta $\phi_{ij}$ nelle base $v_i$ è evidentemente una matrice che ha tutti zeri tranne uno che è uno, esattamente nella posizione $(i,j)$, per cui


\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{\dim V} \delta_{ij} = \dsum_{i=j} \dfrac{1}{\dim V} = 1
 \end{align*}

 \end{proof}

 \begin{cor}
   Sia $G$ un gruppo compatto e $\rho$ una sua rappresentazione. Allora $\rho$ è irriducibile se e solo se $\langle \chi_\rho | \chi_\rho \rangle = 1$
 \end{cor}


 A questo punto possiamo concludere la dimostrazione del fatto che le rappresentazioni che abbiamo trovato prima di $SU(2)$ sono tutte e sole quelle irriducibili.


 \begin{thm}
   Sia $\rho$ una rappresentazione irriducibile di $SU(2)$. Allora $\exists m \in \N$ tale che $\rho \iso \rho_m$. Chiaramente le $\rho_m$ sono quelle definite nella proposizione \ref{prop:irrid su2}
 \end{thm}

 \begin{proof}
   Supponiamo per assurdo che sia $\langle \chi_\rho | \chi_{\rho_m} \rangle = 0 \quad \forall m \in \N$. Vogliamo mostrare che deve essere $\rho = 0$.


   \begin{align*}
     0 &= \langle \chi_\rho | \chi_{\rho_m} \rangle = \dint_G \chi_\rho(g) \overline{\chi_{\rho_m} (g)} dg = \\
     &= \dfrac{1}{\pi} \dint_0^{2\pi} \chi_\rho(\theta) \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta  \ d\theta \\
     &= \dfrac{1}{\pi} \dint_{0}^{2\pi} \left(\chi_\rho (\theta) \sin\theta \right) \sin(  (m+1) \theta) \ d\theta
   \end{align*}

   A questo punto possiamo ricordare che in realtà il carattere è una funzione di classe e in particolare si ha $\chi_\rho(\theta) = \chi_\rho(-\theta)$. La funzione $f(\theta) = \chi_\rho(\theta) \sin\theta \in C_\R[0,2\pi]$ è quindi dispari. Inoltre, è evidentemente anche $\mathbb{L}^2 [0,2\pi]$. Ma noi sappiamo dall'analisi che $\{\sin(nx)\}_{n\in \N}$ è una base delle funzioni dispari di $\mathbb{L}^2[0,2\pi]$. Di conseguenza si ha $f(\theta) = 0 \quad q.o.$, ma dato che è continua, $f(\theta) = 0$ identicamente. Per lo stesso motivo, si deve avere $\chi_\rho(\theta) = 0$

 \end{proof}


 \newpage
 \subsection{Le rappresentazioni irriducibili di $SO(3)$}

 Abbiamo studiato in dettaglio due gruppi importanti, $\mathbb{S}^1$ e $SU(2)$. Sarebbe bello a questo punto sfruttare quello che abbiamo fatto per trovare le rappresentazioni di $SO(3)$, che ha notevole rilevanza fisica. In particolare, se riuscissimo a trovare un omomorfismo continuo $\phi$ da $SO(3)$ a $SU(2)$ potremmo notare sul seguente diagramma

 \[
 \begin{diagram}
   SU(2)       & \rTo^{\rho_m}        & GL(V_m) \\
   \uTo<{\phi} & \ruTo>{\hat{\rho}_m} & \\
   SO(3)       &                      & \\
 \end{diagram}
 \]

 \noindent che otteniamo gratis un sacco di rappresentazioni continue anche per $SO(3)$. Andiamo quindi a costruire questo omomorfismo.

 \begin{defn}
   Definiamo lo spazio vettoriale su $\R$, $\mathbb{E}$ in questo modo

   \[ \mathbb{E } = \{ \text{Le matrici autoaggiunte a traccia nulla }\} = \left\{
   \left(
   \begin{array}{cc}
     x_1 & x_2 + i x_3 \\
     x_2 - i x_3 & -x_1 \\
   \end{array}
   \right)
   \  \  x_i \in \R
   \right\}\]

 \end{defn}

 Ci piacerebbe far agire in modo sensato $SU(2)$ su $\mathbb{E}$, sperando di vedere la struttura di $SO(3)$. Il primo tentativo può essere quello di definire

 \[ \phi(g) x = gx \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 Tuttavia questa azione non funziona in quanto non è per niente detto che la matrice in arrivo abbia di nuovo traccia nulla. Per mantenere la traccia invariata, la cosa più sensata da fare è agire per coniugio

 \[ \phi(g) x = gxg^{-1} \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 In questo caso è evidente che la matrice in arrivo ha traccia nulla. Controlliamo che sia anche autoaggiunta

 \[ \left( \phi(g) x \right)^\dag = \left( g x g^{-1}\right)^\dag = (g^{-1})^\dag x^\dag g ^\dag = g x g^{-1} = \phi(g) x\]

 In quanto $x$ è autoaggiunta e vale $g^{-1} = g^\dag \forall g \in SU(2)$. A questo punto se riuscissimo a trovare un prodotto scalare invariante sotto l'azione di $SU(2)$ potremmo dire di aver in qualche modo mappato $SU(2)$ in $O(3)$ e ci staremmo avvicinando all'obiettivo. In particolare notiamo che la funzione
 \[ f(x) = -det (x) = x_1 ^2 + x_2^2 + x_3^2 \qquad \forall x \in \mathbb{E}\]
 è in effetti una forma quadratica definita positiva su $\mathbb{E} \iso \R^3$ ed è anche invariante sotto $SU(2)$ in sostanza per la formula di Binet. Abbiamo quindi definito $\phi: SU(2) \to O(3)$ che è in sostanza un omomorfismo continuo di gruppi compatti. Inoltre, dato che $SU(2)$ è connesso, anche $\phi(SU(2))$ sarà connesso e conterrà l'identità. Per questo motivo possiamo quindi dire che

 \[ \phi(SU(2)) \subseteq SO(3) \]

 A questo punto, è decisamente il caso di capire meglio come $\phi$ mappi un gruppo nell'altro in modo da sfruttare quello che sappiamo per uno anche per l'altro.



 \begin{prop}
   \label{prop:omo su2 so3}
  \begin{enumerate}
    \item L'omomorfismo $\phi : SU(2) \to SO(3)$ sopra definito è surgettivo.
    \item $\ker \phi = \{\Id, -\Id\}\subseteq SU(2)$
    \item $SU(2) / \Ker(\phi) \iso SO(3)$ come gruppi.
  \end{enumerate}
  \end{prop}

Per dimostrare questa proposizione abbiamo bisogno di un lemma.


\begin{lemma}
  Sia $H \subseteq SO(3)$ un sottogruppo. Supponiamo che valgano le due seguenti affermazioni:

  \begin{enumerate}
  \item $H$ agisce transitivamente su $\mathbb{S}^2$
    \item Esiste un asse $\langle e \rangle \subseteq \R^3$ tale che $H$ contiene tutte le rotazioni intorno a quell'asse.
  \end{enumerate}

  Allora si ha $H = SO(3)$
\end{lemma}

\begin{proof}
  Consideriamo $|e| = 1$. Dato che $H$ agisce transitivamente su $\mathbb{S}^2$, $\forall g \in SO(3)\ \exists h \in H$ tale che
  \[ ge = he \Rightarrow e = h^{-1}ge \Rightarrow h^{-1}g \in \Stab(e) \]

  Ma lo stabilizzatore di $e$ è una rotazione intorno a quell'asse, quindi è in $H$, dunque $h^{-1}g\in H\ \forall g\in SO(3)$, ma allora $\forall g\in SO(3)$ abbiamo che $g\in H$
\end{proof}
















\begin{proof}[Dimostrazione della proposizione \ref{prop:omo su2 so3}] Dimostriamo ora i tre punti della proposizione applicando il lemma appena dimostrato per il primo punto:
   \begin{enumerate}
   \item{
     Dimostriamo che l'azione di $\Phi(SU(2))$ su $\mathbb{S}^2$ è transitiva. Prendiamo una matrice $x \in \mathbb{E} \cap \mathbb{S}^2$, essendo una matrice autoaggiunta, per il teorema spettrale è unitariamente diagonalizzabile, ovvero $\exists g \in U(2) | gxg^{-1}$ è una matrice diagonale. In particolare, dato che $|\det(g)| = 1$, possiamo considerare $\displaystyle\hat g = \frac{g}{\sqrt{\det(g)}}$ per dire che possiamo scegliere una matrice $\hat g \in SU(2)$. Nella sua forma diagonale la matrice sarà
     \[ \hat g x \hat g^{-1} =
     \left(
     \begin{array}{cc}
       \lambda & 0 \\
       0 & -\lambda \\
     \end{array}
     \right)
     \qquad \lambda \in \R
     \]
     e dovrà essere $-\det(m) = \lambda^2 = 1$, per cui $\lambda = \pm 1$. Dato che in $SU(2)$ c'è una matrice che scambia $\lambda$ con $-\lambda$, possiamo prendere WLOG $\lambda > 0$
     \[ \hat g x \hat g^{-1} =
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \]

     E questo vuol dire che tutte le matrici di $\mathbb{E}$ sono coniugate alla stessa. Dato che l'azione di $SU(2)$ è per coniugio, questo vuol dire che c'è un'orbita sola, ovvero l'azione è transitiva.

     Mostriamo ora che $\Phi(SU(2))$ contiene tutte le rotazioni intorno ad un asse. Questo è semplice in quanto sappiamo che $\mathbb{S}^1 \subset SU(2)$. Vediamo come agisce
     \[
     \left(
     \begin{array}{cc}
       e^{i\theta} & 0 \\
       0 & e^{-i\theta} \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       x_1 & x_2 + i x_3 \\
       x_2 - ix_3 & -x_1 \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       e^{-i\theta} & 0 \\
       0 & e^{i\theta} \\
     \end{array}
     \right)
     =
     \left(
     \begin{array}{cc}
       x_1 & e^{2i\theta}(x_2 + i x_3) \\
       e^{-2i\theta}(x_2 - ix_3) & -x_1 \\
     \end{array}
     \right)
     \]
     Che è palesemente una rotazione intorno all'asse $x_1$ di angolo $2\theta$.
   }
   \item{
     Bisogna mostrare che $\Ker \Phi = \{\Id, -\Id\}$. Se vale $\Phi(g) = \Id$, allora vale che

     \begin{align*}
     \left(
     \begin{array}{cc}
       z_1 & z_2 \\
       -\overline{z_2} & \overline{z_1} \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \left(
     \begin{array}{cc}
       \overline{z_1} & -z_2 \\
       \overline{z_2} & z_1 \\
     \end{array}
     \right)
     &=
      \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right) \\
     \Rightarrow
      \left(
     \begin{array}{cc}
       |z_1|^2-|z_2|^2 & -2z_1z_2 \\
       -2 \overline{z_1z_2} & |z_2|^2 - |z_1|^2 \\
     \end{array}
     \right)
     &=
     \left(
     \begin{array}{cc}
       1 & 0 \\
       0 & -1 \\
     \end{array}
     \right)
     \end{align*}
     e questo è vero solo per $z_2 = 0$ e $|z_1| = 1$. Se andiamo a considerare il caso studiato al punto precedente, allora è evidente che deve essere solo $z_1 = \pm 1$.


   }
     \item La tesi segue dalle proposizioni precedenti applicate insieme al primo teorema di omomorfismo.
   \end{enumerate}
	\end{proof}



   A questo punto abbiamo mostrato un isomorfismo di gruppi. Ci piacerebbe che il nostro isomorfismo fosse anche continuo, in modo da poter concludere quali siano tutte e sole le rappresentazioni irriducibili di $SO(3)$. Per farlo in qualche modo dobbiamo definire una topologia sul gruppo quoziente in modo astratto e verificare che corrisponda a quella che già conoscevamo.

   \begin{defn}
     Sia $G$ un gruppo topologico, $H$ un suo sottogruppo normale. Definiamo una topologia nel seguente modo su $G/H$: un sottoinsieme $U \subseteq G/H$ è aperto $\Leftrightarrow \pi^{-1}(U)$ è aperto in $G$. Sono banali verifiche che questa sia effettivamente una topologia su $G/H$. Inoltre questa definizione ci assicura che la mappa $\pi$ sia continua.
   \end{defn}

   \begin{rem}
     A questo punto possiamo affemare che $SU(2) / \Ker\Phi \iso SO(3)$ come gruppi topologici e non solo come gruppi.
   \end{rem}



   \begin{cor}
     Una rappresentazione irriducibile $\rho_m$ di $SU(2)$ si può proiettare su $SO(3)$ se e solo se $- \Id \in \Ker \rho_m$.
   \end{cor}


   \begin{thm}
     Le rappresentazioni irriducibili di $SO(3)$ sono tutte e sole quelle che si ottengono da $SU(2)$ che hanno $-\Id$ nel nucleo, ovvero le $\rho_m$ per $m$ pari.
   \end{thm}


   \begin{thm}[Formula di Clebsch-Gordon]
     Consideriamo il prodotto tensore di due rappresentazioni irriducibili di $SU(2)$. Vale la formula
     \[ \rho_n\otimes \rho_m = \dsum_{j=0}^m \rho_{m+n-2j} \qquad (n \geq m)\]
   \end{thm}

   \begin{proof}
     Si tratta di un banale conto con i caratteri.

     \begin{align*}
       \chi_{\rho_n\otimes\rho_m} &= \chi_{\rho_n} \chi_{\rho_m} = \dfrac{e^{i(n+1)\theta} - e^{-i(n+1)\theta}}{e^{i\theta} - e^{-i\theta}} \cdot \dsum_{j=0}^m e^{i(m-2j)\theta} =\\
                                  &= \cdots = \dsum_{j = 0}^m e^{i(n+m-2j)\theta}
     \end{align*}
   \end{proof}




\newpage
\section{Rappresentazioni di algebre}
\subsection{Prime definizioni e esempi}
	\begin{defn}
		Un insieme $A$ si dice \textit{algebra} sul campo $\K$ (o più semplicemente $\K$\textit{-algebra}) se
		\begin{itemize}
			\item $A$ è uno spazio vettoriale su $\K$
			\item il prodotto $\cdot:A\times A\to A$ è una funzione bilineare
		\end{itemize}
		Un'algebra si dice
		\begin{itemize}
			\item \textit{associativa} se il prodotto è associativo
			\item \textit{unitaria} se $\exists 1_A\in A$ l'elemento neutro per il prodotto
			\item \textit{commutativa} se il prodotto è commutativo
		\end{itemize}
	\end{defn}
	\begin{exmp} Alcuni esempi di algebre sono:
		\begin{enumerate}
			\item Algebre di divisione $/\R$ (che abbiamo già incontrato)
			\item $\K$ un campo
			\item Se $V$ è uno spazio vettoriale su $\K$ allora $\End(V)$ è una $\K$-algebra
			\item $\K[x_1,\ldots,x_n]$ è una $\K$-algebra
			\item $\K\langle x_1,\ldots,x_n\rangle$ è l'algebra libera con $n$ generatori, sono le parole nell'alfabeto $\{x_1,\ldots,x_n\}$, il prodotto è la concatenazione; è generata da $x_{i_1}\cdots x_{i_k}$ (simile come costruzione al gruppo libero su $n$ generatori)
			\item Se $A$ è una $\K$-algebra, $I$ un suo ideale bilatero, allora $A/I$ è una $\K$-algebra
		\end{enumerate}
	\end{exmp}
	\begin{exmp}
		QUI ANDREBBE L'ALGEBRA UNIVERSALE INVILUPPANTE MA NON SO QUANTO SERVA...
	\end{exmp}
	Vorremmo ritrovare la teoria delle rappresentazioni che abbiamo fatto per i gruppi anche nella teoria che faremo sulle algebre, in quest'ottica definiamo il seguente insieme
	\begin{defn}
		Sia $G$ un gruppo, definiamo l'insieme delle combinazioni lineari formali degli elementi di $G$ a coefficienti in $\K$
		\[
			\K[G] = \left\{ \sum a_g e_g, a_g\in \K \right\}
		\]
		dove abbiamo una corrispondenza biunivoca tra $\{ g\in G \}$ e $\{ e_g \}_{g\in G}$.
		Si osserva che $\K[G]$ è un $\K$-spazio vettoriale, definendo poi il prodotto sui vettori di base (ed estendendo per linearità) come $e_g\cdot e_h=e_{gh}$ si ottiene una $\K$-algebra. Essa è associativa perchè il prodotto in $G$ è associativo e unitaria poichè $e_1$ è l'unità di $\K[G]$.
	\end{defn}
	\begin{rem}
		L'azione che avevamo nella rappresentazione regolare diventa il prodotto in $\K[G]$.
	\end{rem}
	\begin{defn}
		Siano $A$ e $A'$ $\K$-algebre, sia $f:A\to A'$ una funzione, $f$ è un \textit{omomorfismo di $\K$-algebre} se
		\begin{itemize}
			\item $f:A\to A'$ è un omomorfismo di spazi vettoriali
			\item $f(ab)=f(a)f(b)$
			\item $f(1_A)=1_{A'}$
		\end{itemize}
	\end{defn}
	\begin{defn}
		Una \textit{rappresentazione di una $\K$-algebra} $A$ è un omomorfismo di $\K$-algebre
		\[ f:A\to \End(V)\]
		con $V$ un $\K$-spazio vettoriale, allora $V$ viene detto $A$\textit{-modulo}.
	\end{defn}
	\begin{defn}
		Siano $V$ e $W$ degli $A$-moduli, un omomorfismo di spazi vettoriali $f:V\to W$ si dice \textit{omomorfismo di rappresentazioni} se
		\[\forall a\in A,\ \forall v\in V \quad f(av)=af(v)\]
	\end{defn}
	\begin{exercise}
		Se $f$ è un isomorfismo di $A$-moduli, allora $f^{-1}$ è un isomorfismo di $A$-moduli.
	\end{exercise}
	\begin{exmp}
		Alcuni esempi di rappresentazioni sono
		\begin{enumerate}
			\item Se $A=\K$ allora le rappresentazioni di $A$ sono gli spazi vettoriali su $\K$ dove l'azione di $A$ è molto semplice: $\rho(a) = a\Id$.
			\item Sia $G$ un gruppo finito, $A=\K[G]$, allora le rappresentazioni di $A$ come $\K$-algebra sono tutte e sole le rappresentazioni di $G$ come gruppo (le verifiche sono abbastanza semplici, da un lato si restringe l'azione agli elementi di base, dall'altro si estende per linearità).
		\end{enumerate}
	\end{exmp}
	\begin{defn}
		Siano $\rho_1, \rho_2$ delle rappresentazioni di $A$, allora definiamo $\rho_1+\rho_2$ sulla somma diretta
		\[
			\rho_1+\rho_2:A\to \End(V_1\oplus V_2)
		\]
		allo stesso modo di come abbiamo definito la somma di rappresentazioni di gruppi (scrivendo le matrici a blocchi).
	\end{defn}

	\begin{defn}
		Sia $\rho:A\to \End(V)$ una rappresentazione, $\rho$ si dice \textit{irriducibile} se $V$ non ha sottomoduli (sottospazi invarianti per moltiplicazione per elementi di $A$) non banali, allora $V$ è detto $A$-modulo \textit{semplice}.
	\end{defn}
	\begin{defn}
		Sia $\rho:A\to V$ una rappresentazione, $\rho$ è \textit{indecomponibile} ($V$ è indecomponibile come $A$-modulo) se non è somma diretta di rappresentazioni non banali.
	\end{defn}
	\begin{exmp}
		Sia $A=\K[x]$ vista come $\K$-algebra, allora una rappresentazione è univocamente determinata dall'immagine di $x$, dunque per identificare una rappresentazione basterà sapere $V$ e $\rho(x)$. Se $\K = \overline{\K}$ si osserva che le rappresentazioni irriducibili sono quelle di grado $1$ e sono determinate dal parametro $\lambda\in \K$ tale che $\rho(x)=\lambda$. Si può vedere inoltre che le rappresentazioni indecomponibili sono solo quelle per cui $\rho(x)$ è simile a $J_{n,\lambda}$ dove $J_{n,\lambda}$ è il blocco di Jordan di dimensione $n$ e autovalore $\lambda$.\newline
		Da questo esempio si osserva che il concetto di irriducibilità è diverso da quello di indecomponibilità.
	\end{exmp}
	\begin{exercise}
		QUI CI SAREBBE UN ESERCIZIO CHE RIGUARDA LA MITICA ALGEBRA UNIVERSALE INVILUPPANTE...
	\end{exercise}
	\begin{lemma}[Schur]
		Sia $A$ una $\K$-algebra, $\phi:V\to W$ un omomorfismo di $A$-moduli con $\phi \neq 0$, allora
		\begin{enumerate}
			\item se $V$ è semplice allora $\phi$ è iniettivo
			\item se $W$ è semplice allora $\phi$ è surgettivo
		\end{enumerate}
		Dunque se $V$ e $W$ sono semplici allora $\phi$ è un isomorfismo.
	\end{lemma}
	\begin{proof}
		Identica a quella fatta per i gruppi, dove si usa che $\Ker \phi$ e $\Imm \phi$ sono sottomoduli rispettivamente di $V$ e $W$.
	\end{proof}
	\begin{cor}
		Se $\K$ è algebricamente chiuso, $\phi\in End_A(V)$, $V$ è un $A$-modulo semplice, allora $\exists \lambda \in \K^*$ tale che $\phi = \lambda \Id$.
	\end{cor}
	\begin{cor}
		Se $V$ è un $A$-modulo semplice allora $\End_A(V)$ è un'algebra di divisione.
	\end{cor}
	Quello che abbiamo definito prima più precisamente si chiama \textit{$A$-modulo sinistro}, andiamo a dare un'altra definizione
	\begin{defn}
		Chiamiamo $A^{OP}$ l'\textit{algebra opposta} di $A$, $A^{OP}=A$ come spazio vettoriale, ma il prodotto in $A^{OP}$ è $a*b = ba$.
	\end{defn}
	\begin{defn}
		Un \textit{$A$-modulo destro} è una rappresentazione di $A^{OP}$. Se $\rho:A^{OP}\to \End(V)$ è una rappresentazione allora $V$ è un $A$-modulo destro.
	\end{defn}
	\begin{exmp}
		Alcuni esempi di algebre isomorfe alle loro algebre opposte
		\begin{enumerate}
			\item $\phi:M_n(\K) \to M_n(\K)$ dove $\phi(x)= x^T$ è un isomorfismo di algebre tra $\left(M_n(\K) \right)$ e $\left( M_n(\K) \right)^{OP}$.
			\item $\phi:\K[G]\to \K[G]$ dove $\phi(e_g)=e_{g^{-1}}$ ed estendendolo per linearità, $\phi$ è un isomorfismo di algebre tra $\K[G]$ e $\K[G]^{OP}$.
		\end{enumerate}
	\end{exmp}
	\begin{defn} Sia $\rho:A\to \End(V)$ una rappresentazione di $A$, definiamo la \textit{rappresentazione duale} $\rho^*:A^{OP}\to \End(V^*)$ dove
	\[
		(\rho^*(x)f)(v) = f(x\cdot v)
	\]
	\end{defn}
	\begin{prop}
		Abbiamo un isomomorfismo di algebre $A^{OP}\iso \End_A(A)$ dove $A$ è pensato come $A$-modulo sinistro.
	\end{prop}
	\begin{proof}
		Se $x\in A$, definiamo $r_x:A\to A$ come $r_x(a)=ax$, allora $r_x\in \End_A(A)$ poichè $r_x(ba)=bax = br_x(a)$.\\
		Consideriamo allora $\Phi:A^{OP}\to\End_A(A)$ tale che $\Phi(x)=r_x$. Se $\phi\in \End_A(A)$ allora $\phi(a)=\phi(a\cdot1)=a\cdot\phi(1) = r_{\phi(1)}(a)$, cioè $\Phi$ è surgettiva.\\
		Vediamo ora che $\Phi$  è omomorfismo di algebre: $\Phi(a\ast b)(t)=\Phi(ba)(t)=r_{ba}(t)=tba=r_a(tb)=r_a\circ r_b (t)$.\\
		Mostriamo infine che è iniettiva: se $\Phi(x)=r_x=\Id$, abbiamo $ax=r_x(a)=a\;\forall a\in A$, e in particolare se $a=1$, abbiamo $x=1$
	\end{proof}
\subsection{Ideali e quozienti}
	\begin{defn}
		Un ideale \textit{sinistro} di $A\supseteq I$ è un sottospazio vettoriale tale che $aI\subseteq I\ \forall a\in A$. Un ideale \textit{destro} è un sottospazio chiuso per la moltiplicazione a destra per elementi di $A$, un ideale è \textit{bilatero} se è sia destro che sinistro.
	\end{defn}
	\begin{rem}
		$I$ è un ideale destro se e solo se $I$ è un $A^{OP}$-sottomodulo, $I$ è un ideale sinistro se e solo se è un $A$-sottomodulo.
	\end{rem}
	\begin{rem}
		Se $I\subseteq A$ è un ideale sinistro allora $A/I$ è un $A$-modulo, se è un ideale destro allora $A/I$ è un $A^{OP}$-modulo, se $I$ è bilatero allora $A/I$ è un'algebra definendo $(a+I)(b+I)=ab+I$.
	\end{rem}
	\begin{exmp}
		Sia $\phi:A\to A'$ un omomorfismo di algebre, allora $\Ker \phi$ è un ideale bilatero di $A$ e $\Imm \phi \iso A/\Ker \phi$ come algebre.
	\end{exmp}
	\begin{defn}
		$A$ è un'\textit{algebra semplice} se non ha ideali bilateri non banali.
	\end{defn}
	\begin{rem}
		Sia $V$ un $A$-modulo sinistro, fissiamo $v\in V$, sia $\phi:A\to V$ come $\phi(x)=x\cdot v$. Si osserva che $\phi$ è lineare e che è un omomorfismo di $A$-moduli vedendo $A$ come $A$-modulo, dunque $\Ker \phi$ è un ideale sinistro.
	\end{rem}
	\begin{rem}
		In aggiunta alle ipotesi di prima sia $V$ un $A$-modulo semplice, $v\neq 0$, allora $\Imm \phi = V$ e dunque $V\iso A/\Ker \phi$ come $A$-moduli.
	\end{rem}
	\begin{defn}
		Un $A$-modulo $V$ è detto \textit{semisemplice} se è somma diretta di $A$-moduli semplici. Equivalentemente $V$ è detto semisemplice se ogni sottomodulo ammette un complementare stabile.
	\end{defn}
	\begin{exercise}
		Sia $A=\K[G]$ (CON G FINITO?????), allora ogni $A$-modulo di dimensione finita è semisemplice.
	\end{exercise}
	\begin{prop}
		Sia $V$ un $A$-modulo semisemplice, allora $V$ è semplice se e solo se $\End_A(V)$ è un'algebra di divisione.
	\end{prop}
	\begin{proof}
		\begin{enumerate}
			\item[$\Rightarrow$)]Per il lemma di Schur.
			\item[$\Leftarrow$)]Supponiamo che $V=V_1\oplus V_2$ sia una decomposizione in $A$-moduli. Siano $\pi_1:V\to V$ e $\pi_2:V\to V$ le proiezioni associate alla decomposizione. Si osserva $\pi_1$ e $\pi_2$ sono omomorfismi di $A$-moduli e che entrambi sono diversi da $0$, tuttavia $\pi_1\circ \pi_2 = 0$, quindi avremmo che $\End_A(V)$ non è un'algebra di divisione, assurdo.
		\end{enumerate}
	\end{proof}
\subsection{Serie di composizione}
	\begin{defn}
		Sia $A$ una $\K$-algebra, $V$ un $A$-modulo di dimensione finita. Si chiama \textit{filtrazione} per $V$ una sequenza di sottomoduli tale che
		\[
			0=V_0\subseteq V_1\subseteq \ldots \subseteq V_n=V
		\]
		Diciamo che una filtrazione è una \textit{serie di composizione} (o serie di Jordan-Holder) se i quozienti $\displaystyle V_i/V_{i-1}$ sono tutti $A$-moduli semplici. Questi quozienti sono detti \textit{fattori di composizione} della serie.
	\end{defn}
	\begin{exercise}
		Se $W\subseteq V$ sono $A$-moduli, allora $W$ è un sottomodulo proprio massimale se e solo se $V/W$ è semplice.
	\end{exercise}
	\begin{prop}
		Ogni $A$-modulo $V$ di dimensione finita ammette una serie di composizione di lunghezza finita.
	\end{prop}
	\begin{proof}
		Per induzione su $\dim V$: se $V$ è irriducibile ho finito, altrimenti sia $V_1$ un sottomodulo irriducibile (basta prenderne uno minimale). Consideriamo ora $V/V_1$, per ipotesi induttiva esso ammette una serie di decomposizione di lunghezza finita, prendendo le controimmagini della serie per $V/V_1$ tramite la proiezione al quoziente si ottiene una serie di Jordan-Holder.
		\\FORSE DA SPIEGARE MEGLIO PERCHE'...
	\end{proof}
	\begin{rem}
		Se $V$ è somma diretta di irriducibili ($V$ semisemplice), allora i fattori di composizione di qualsiasi serie per $V$ sono proprio gli irriducibili di $V$ contati con molteplicità.
	\end{rem}
	\begin{thm}[di Jordan-Holder]\label{thm:jordan-holder}
		Sia $V$ un $A$-modulo di dimensione finita, allora tutte le serie di composizione per $V$ hanno la stessa lunghezza (che chiamiamo lunghezza del modulo), e gli stessi fattori di composizione (contati con molteplicità e a meno di permutazioni).
	\end{thm}
	\begin{proof}
		Per induzione su $\dim V$: se $V$ è irriducibile ho finito, supponiamo dunque che abbia sottomoduli non banali, siano
		\begin{gather*}
			0=V_0\subseteq\ldots \subseteq V_m = V\qquad W_i=V_i/V_{i-1}\\
			0=V'_0\subseteq\ldots \subseteq V'_n = V\qquad W'_i=V'_i/V'_{i-1}
		\end{gather*}
		due serie di Jordan-Holder per $V$. Se $V_1=V'_1$ considero $V/V_1$, per ipotesi induttiva ho l'unicità. Supponiamo dunque che $V_1\neq V'_1$, visto che $V_1$ e $V'_1$ sono irriducibili si ha che $V_1\cap V'_1=0$, dunque $V_1+V'_1=V_1\oplus V'_1$.\\
		Considero il modulo $\quot{V}{V_1\oplus V'_1}$, siano $Z_1,\ldots,Z_p$ i fattori di composizione di una serie di Jordan-Holder per $\quot{V}{V_1\oplus V'_1}$. Ma allora si ha che
		\begin{enumerate}
			\item $V/V_1$ ammette due serie di composizione con fattori
			\[
				V'_1,Z_1,\ldots,Z_p\qquad W_2,\ldots,W_m
			\]
			\item $V/V'_1$ ammette due serie di composizione con fattori
			\[
				V_1,Z_1,\ldots,Z_p\qquad W'_2,\ldots,W'_n
			\]
		\end{enumerate}
		Per ipotesi induttiva ho l'unicità delle serie per $V/V_1$ e $V/V'_1$, dunque si ha che $n=m=p+2$. Sempre per via dell'unicità si ha che
		\begin{gather*}
			\{ W'_1,Z_1,\ldots,Z_p\} = \{ W_2,\ldots,W_m\}\\
			\{ W_1,Z_1,\ldots,Z_p\} = \{ W'_2,\ldots,W'_m\}
		\end{gather*}
		da cui, aggiungendo $W_1$ agli insiemi\footnote{non sono esattamente insiemi dato che devono contare i fattori con molteplicità} nella prima riga e $W'_1$ a quelli della seconda si ottiene
		\begin{gather*}
			\{ W'_1,W_1,Z_1,\ldots,Z_p\} = \{ W_1,W_2,\ldots,W_m\}\\
			\{ W'_1,W_1,Z_1,\ldots,Z_p\} = \{ W'_1,W'_2,\ldots,W'_m\}
		\end{gather*}
		essendo uguali i membri di sinistra ho uguaglianza anche dei membri di destra e quindi segue la tesi.
	\end{proof}
	\begin{exmp}
		Sia $A=\K[G]$ con $\Char \K \nmid \ord(G)$, allora ogni rappresentazione di dimensione dinita è somma di moduli irriducibili. I fattori di composizione coincidono con gli addendi irriducibili contati con molteplicità.
	\end{exmp}
	\begin{rem}
		Sia $V$ un $A$-modulo, $W\subseteq V$ un sottomodulo, allora esiste sempre una serie di Jordan-Holder che contiene $W$ ($W=V_i$ per qualche $i$): basta considerare prima una serie per $W$ e comporla con la controimmagine tramite la proiezione al quoziente di una serie per $V/W$.
	\end{rem}
	\begin{cor}
		Sia $V$ un $A$-modulo semplice con $\dim_{\K}A<\infty$, allora $V$ compare come fattore di composizione in qualsiasi serie di Jordan-Holder per $A$ (considerando $A$ come $A$-modulo sinistro).
	\end{cor}
	\begin{proof}
		Fisso $v\in V, v\neq 0$ e considero $\phi:A\to V$ tale che $\phi(a) = av$, allora $\phi$ è surgettiva (siccome $V$ è semplice), $I=\Ker \phi \subseteq A$ è un ideale sinistro e si ha che $V\iso A/I$ come $A$-moduli. $I$ è un sottomodulo di $A$, quindi esiste una serie di composizione per $A$ che contiene $I$: esistono $I_0,\ldots,I_n$ ideali sinistri di $A$ tali che
		\[
			0=I_0\subseteq\ldots\subseteq I_n = A\qquad \text{con }I_k/I_{k-1}\text{ semplice }\forall k
		\]
		Sia $p\in \N$ tale che $I=I_p$, si osserva che $p=n-1$ poichè $A/I$ è semplice (dunque non posso allungare la serie), quindi $I_n/I_{n-1} = A/I\iso V$. Abbiamo provato dunque che in una serie di composizione troviamo un fattore uguale a $V$, avendo dimostrato l'unicità dei fattori della serie col teorema \ref{thm:jordan-holder} segue la tesi.
	\end{proof}
	\begin{exmp}
		Sia $A=\quot{\K[x]}{(x^n)}$, consideriamo la serie
		\[
			0\subset \quot{(x^{n-1})}{(x^n)}\subset \quot{(x^{n-2})}{(x^n)}\subset \ldots\subset \quot{\K[x]}{(x^n)}
		\]
		Tutti i fattori di composizione sono isomorfi alla rappresentazione banale (dove $x$ agisce come $0$ e gli scalari agiscono come scalari), dunque l'unico modulo irriducibile è quello banale (OVVERO IL CAMPO K???). In particolare $\K[x]/(x^n)$ ammette un'unica rappresentazione irriducibile a meno di isomorfismo.
	\end{exmp}
	\begin{exmp}\label{serie_comp_matrici}
		Sia $A=M_n(\K)$, un modulo irriducibile è $\K^n$, per ogni $i=1,\ldots,n$ sia
		\[
			V_i = \left\{ M\in A : M_{p,q} = 0\ \forall q>i \right\}
		\]
		ovvero $V_i$ è l'insieme delle matrici appartenenti ad $A$ con le ultime $n-i$ colonne nulle (in realtà è un sottomodulo). Consideriamo la serie
		\[
			0\subset V_1\subset\ldots\subset V_n=M_n(\K)
		\]
		si osserva che $V_i/V_{i-1}\iso \K^n$, dunque tutti i quozienti sono irriducibili e isomorfi tra loro, ma allora $\K^n$ è l'unica rappresentazione irriducibile di $M_n(\K)$ a meno di isomorfismo.
	\end{exmp}


\subsection{Teorema di Krull-Schmidt}
	\begin{defn}
		Sia $x\in A$, $x$ è \textit{nilpotente} se $\exists\ n\in \N$ tale che $x^n=0$, $x$ è \textit{idempotente} se $x^2=x$.
	\end{defn}
	\begin{prop}
		Sia $V$ un $A$-modulo, allora $V$ è indecomponibile se e solo se per ogni $\phi\in \End_A(V)$ idempotente si ha che $\phi=0$ o $\phi=\Id$.
	\end{prop}
	\begin{proof}
		\begin{enumerate}
			\item[$\Leftarrow)$] Se $V$ non è indecomponibile allora $V = W\oplus W'$ con $W,W'\neq V$, sia $p\in \End_A(V)$ la proiezione su $W$ associata, allora $p^2=p, p\neq 0, p\neq \Id$, assurdo.
			\item[$\Rightarrow$)]Sia $p\in \End_A(V)$ tale che $p^2=p$, voglio vedere che $p=0$ o $p=\Id$. Si osserva subito che $\Ker p \cap \Imm p = 0$ (per l'ipotesi $p^2=p$), dunque $V=\Ker p\oplus \Imm p$, ma allora deve succedere che $\Ker p = 0$ o $\Imm p = 0$ (poichè $V$ è indecomponibile), se $\Imm p = 0$ allora $p=0$, se $\Ker p=0$ allora $p=\Id$ poichè $p^2=p$.
		\end{enumerate}
	\end{proof}
	\begin{lemma}\label{comp_iso}
		Siano $V,W$ degli $A$-moduli indecomponibili di dimensione finita su $\K$, siano $\alpha\in \Hom_A(V,W)$ e $\beta\in \Hom_A(W,V)$ tali che $\beta\circ\alpha$ è un isomorfismo su $V$, allora $\alpha$ e $\beta$ sono isomorfismi.
	\end{lemma}
	\begin{proof}
		Vediamo che $W=\Ker\beta\oplus\Imm \alpha$:
		\begin{enumerate}
			\item $\Ker \beta\cap\Imm\alpha = 0$ poichè altrimenti $\beta\circ\alpha$ avrebbe nucleo diverso da $0$ ma sappiamo che $\beta\circ\alpha$ è un isomorfismo.
			\item Sia $x\in W$, sia $y=\left( \alpha\circ(\alpha\circ\beta)^{-1}\circ\beta \right)(x)$, allora $y\in \Imm \alpha$, consideriamo ora $\beta(y) = (\beta\circ\alpha)\circ(\beta\circ\alpha)^{-1}\circ\beta(x) = \beta(x)$, dunque $x-y\in \Ker \beta$, quindi $W=\Imm \alpha \oplus \Ker \beta$.
		\end{enumerate}
		Ma $W$ è indecomponibile, quindi $\Ker \beta = 0$ o $\Imm \alpha = 0$, la seconda situazione non può accadere (altrimenti $\beta\circ\alpha$ non sarebbe un isomorfismo), ma allora $\Ker \beta = 0$ e $\Imm \alpha = W$, visto che necessariamente $\alpha$ è iniettivo (altrimenti non lo sarebbe $\beta\circ\alpha$) si ha che $\alpha$ è un isomorfismo, e dunque anche $\beta$ lo è.
	\end{proof}
	\begin{prop}\label{end_iso_nilp}
		Sia $V$ un $A$-modulo indecomponibile e $\phi\in \End_A(V)$, allora $\phi$ è un isomorfismo oppure è nilpotente.
	\end{prop}
	\begin{proof}
		Sia $\phi\in \End_A(V)$, considero le due successioni
		\begin{gather*}
			\ldots\supseteq \Ker\phi^n\supseteq\Ker\phi^{n-1}\supseteq\ldots\supseteq\Ker\phi^0\\
			\ldots\subseteq\Imm\phi^n\subseteq\Imm\phi^{n-1}\subseteq\ldots\subseteq\Imm\phi^0
		\end{gather*}
		Queste successioni si devono stabilizzare, sia dunque $N$ tale che $\Ker\phi^N=\Ker\phi^{N+1}$ e $\Imm\phi^N=\Imm\phi^{N+1}$, allora si ha che $\Ker\phi^N\cap\Imm\phi^N=0$ (altrimenti non si sarebbero stabilizzate le successioni), ma allora $V=\Ker\phi^N\oplus\Imm\phi^N$ per questioni di dimensioni.\\
		Per ipotesi $V$ è indecomponibile dunque $\Imm\phi^N=0$ oppure $\Ker\phi^N=0$, se $\Imm\phi^N=0$ allora $\phi$ è nilpotente, se invece $\Ker\phi^N=0$ allora a maggior ragione $\Ker \phi=0$ e dunque $\phi$ è un isomorfismo.
	\end{proof}
	\begin{cor}\label{sum_nilp}
		Sia $V$ un $A$-modulo indecomponibile, siano $\theta_1,\ldots,\theta_s\in \End_A(V)$ con $\theta_i$ nilpotente per $i=1,\ldots,s$, allora $\theta_1+\ldots+\theta_s$ è nilpotente.
	\end{cor}
	\begin{proof}
		Sia $\displaystyle \theta=\sum_{i=1}^{s}\theta_i\in \End_A(V)$, per la proposizione precedente sappiamo che $\theta$ è un isomorfismo oppure è nilpotente. Procediamo per induzione su $s$:
		\begin{itemize}
			\item Se $s=1$ allora $\theta=\theta_1$ che è nilpotente.
			\item Supponiamo la tesi vera per $s=n-1$ e dimostriamola per $s=n$: supponiamo per assurdo che $\theta$ sia un isomorfismo, allora avremmo che
			\[
				\Id = \theta^{-1}\theta_1+\ldots+\theta^{-1}\theta_n
			\]
			Si osserva subito che $\theta^{-1}\theta_i$ è nilpotente (dato che ha nucleo non nullo non è un isomorfismo e dunque è nilpotente), quindi
			\[
				\Id-\theta^{-1}\theta_n = \theta^{-1}\theta_1+\ldots+\theta^{-1}\theta_{n-1}
			\]
			Sappiamo per ipotesi induttiva che la somma di $n-1$ endomorfismi nilpotenti è ancora nilpotente, dunque il termine di destra dell'equazione è nilpotente, ma questo è assurdo essendo quello di sinistra un isomorfismo: se $\phi$ è nilpotente con indice di nilpotenza $k$ allora $\Id-\phi$ è un isomorfismo, infatti
			\[
				(\Id-\phi)\cdot(1+\phi+\ldots+\phi^{k-1}) = \Id-\underbrace{\phi^k}_{=0} = \Id
			\]
		\end{itemize}
	\end{proof}
	\begin{thm}[Krull-Schmidt]\label{thm:krull-schmidt}
		Sia $A$ una $\K$-algebra di dimensione finita, $V$ un $A$-modulo con $\dim V<\infty$. Allora $V$ si decompone in modo unico come somma di rappresentazioni indecomponibili (a meno di isomorfismo e permutazioni).
	\end{thm}
	\begin{proof}
	 Poiché $V$ ha dimensione finita, per induzione lo spezzo finché non ho tutti addendi diretti indecomponibili. Occorre adesso provare l'unicità.\\
	 Sia $V=\bigoplus_{i=1}^m W_i=\bigoplus_{i=1}^n W'_i$; allora ho le inclusioni e proiezioni standard: \begin{align*} i_p &:  W_p\to V & \pi_p &: V\to W_p \\ i'_p &:  W'_p\to V & \pi'_p &: V\to W'_p\end{align*}
	 Definiamo ora $\theta_s=\pi_1\circ i'_s\circ \pi'_s\circ i_1$ seguendo la catena $W_1\xhookrightarrow{i_1} V\xrightarrow{\pi'_s}W'_s\xhookrightarrow{i'_s} V\xhookrightarrow{\pi_1} W_1$.
	 Poiché le proiezioni e le inclusioni sono omomorfismi di $A$-moduli, allora abbiamo $\theta_s\in\End_A(W_1)$.\\
	 Osserviamo ora che $\theta_1+\dots+\theta_n=\Id_{W_1}$: se infatti prendiamo una base $\{w'_{i,j}\}_j$ di $W'_i$, concatenandole otteniamo una base di $V$; scriviamo ora un elemento $v\in W_1$ in coordinate: $v=(a_{i,j})$.
	 La mappa $i_1$ non fa nulla; quando proietto su $W'_s$ invece annullo tutte le coordinate non relative a quel sottospazio, ovvero $\pi'_s\circ i_1(v)=(0,\dots,0,a_{s,1},\dots,a_{s,\dim W'_s},0,\dots,0)$; di nuovo l'inclusione non cambia le coordinate.
	 Infine $\displaystyle\sum\theta_s(v)=\sum \pi_1\circ i'_s\circ \pi'_s\circ i_1(v)=\pi_1\left( \sum i'_s\circ \pi'_s\circ i_1(v) \right)$ e per quanto detto prima, quando sommiamo su tutti i $W'_s$ è come se reincollassimo tutte le componenti; in particolare $\sum i'_s\circ \pi'_s\circ i_1(v)=(a_{i,j})$ e quindi $\pi_1(a_{i,j})=v$.\\
	 Per la proposizione \ref{end_iso_nilp}, essendo $V_1$ indecomponibile, ogni $\theta_s$ è un isomorfismo, oppure nilpotente. Se tutti i $\theta_s$ fossero nilpotenti, per il corollario \ref{sum_nilp}, anche $\sum\theta_s=\Id_{W_1}$ sarebbe nilpotente.
	 Perciò esiste un indice $\tilde s$, che possiamo supporre essere $1$ a meno di riordinare i $W'_i$, tale che $\theta_{\tilde s}$ è un isomorfismo.
	 Allora per il lemma \ref{comp_iso}, abbiamo che $\phi=\pi_1\circ i'_1$ e $\pi'_1\circ i_1$ sono isomorfismi tra $W_1$ e $W'_1$.\\
	 Siano ora $Z=\bigoplus_{i=2}^m W_i$ e $Z'=\bigoplus_{i=2}^n W'_i$; osserviamo che $V=W'_1\oplus Z'$, perciò ho la proiezione $\pi:V\to Z'$, che è omomorfismo di moduli. Osserviamo che $\dim Z=\dim Z'$ grazie all'isomorfismo di $W_1$ e $W'_1$.
	 Possiamo allora definire $\psi: Z\to Z'$ nel modo ovvio $Z\xhookrightarrow{i}V\xrightarrow{\pi}Z'$. Mostriamo che $\psi$ è un isomorfismo; essendo le dimensioni degli spazi uguali, basta l'iniettività.\\
	 Osserviamo che $\ker\psi=Z\cap\ker\pi=Z\cap i'_1(W'_1)$; se prendiamo $z\in Z\cap i'_1(W'_1)$, avremmo che $\pi_1(z)=0$, essendo $z\in Z$; inoltre $\pi_1(z)=\phi(w)$ per un qualche $w\in W'_1$. Essendo $\phi$ un isomorfismo tra $W_1$ e $W'_1$, si ha $w=\phi^{-1}(0)=0$ da cui $z=i'_1(0)=0$.
	\end{proof}

\subsection{Semisemplicità}
  \begin{defn}
   Un $A$-modulo $V$ si dice \emph{semisemplice} se ogni sottomodulo ammette un complemento stabile; ovvero per ogni $W\subset V$ tale che $AW\subset W$, esiste un $W'\subset V$ con $AW'\subset W'$ per cui $V=W\oplus W'$
  \end{defn}

  \begin{lemma}\label{quoz_smod_ss}
  	Sia $V$ un $A$-modulo semisemplice; allora quozienti e sottomoduli di $V$ sono semisemplici.
  \end{lemma}
  \begin{proof}
   Sia $W\subset V$ un sottomodulo; consideriamo allora $Z\subset W$ stabile. Ma se guardiamo $Z$ come sottomodulo di $V$, che è semisemplice, sappiamo trovare un sottomodulo $Z'\subset V$ tale che $V=Z\oplus Z'$; perciò la proiezione $p:V\to Z$ è omomorfismo di $A$-moduli. Se guardo la restrizione $p_{|_{W}}:W\to Z$ è ancora una proiezione, perciò abbiamo $W=Z\oplus\ker p_{|_{W}}$, che è quello che volevamo, in quanto il nucleo è $A$-stabile.\\
   Sia ora $W$ un quoziente di $V$, ovvero $W\iso\faktor{V}{Z}$ per un qualche sottomodulo $Z\subset V$. La proiezione al quoziente $\pi: V\to W$ è un omomorfismo di $A$-moduli, poiché $Z$ è stabile: $a\pi(v)=a(v+Z)=av+aZ=av+Z=\pi(av)$.\\
   Allora $\ker\pi$ è un sottomodulo di $V$, che allora ammette complemento stabile $V=\ker\pi\oplus W'$; sappiamo che $W\iso\faktor{V}{\ker\pi}$, ma allora $W\iso W'$ tramite $\pi_{|_{W'}}$
  \end{proof}


  \begin{prop}
   Sia $V$ un $A$-modulo di dimensione finita. Allora sono equivalenti
   \begin{enumerate}
    \item $V$ è semisemplice
    \item $V$ è somma diretta di $A$-moduli semplici
    \item $V$ è somma di $A$-moduli semplici
   \end{enumerate}
  \end{prop}
  \begin{proof}$ $
    \begin{itemize}
     \item[$1\Rightarrow2$] Per induzione su $\dim V$; se $V$ è semplice, ho finito. Se invece esiste un sottomodulo $W\subset V$, trovo un complemento stabile $V=W\oplus W'$ e ho $\dim W,\dim W'<\dim V$; inoltre per il lemma \ref{quoz_smod_ss} sia $W$ che $W'$ sono semisemplici, quindi posso applicare l'induzione e scriverli come somma diretta di moduli semplici.
     \item[$2\Rightarrow3$] Ovvio
     \item[$3\Rightarrow1$] Scriviamo $V=\sum V_i$ con i $V_i$ semplici; prendiamo un sottomodulo $W$ e cerchiamone un complemento.\\
     In particolare, sia $W'$ un sottomodulo massimale tale che $W\cap W'=0$; supponiamo per assurdo che $W\oplus W'\subsetneq V$.
     Allora esiste un $V_i\not\subset W\oplus W'$, e in particolare $V_i\cap W\oplus W'=0$ in quanto $V_i$ è semplice.\\
     Ma allora $V_i\cap W'=0$, perciò posso considerare $V_i\oplus W'$; inoltre se $z\in V_i\oplus W'\cap W$, vuol dire che $w=v+w'$, cioè $V_i\ni v=w-w'\in W\oplus W'$, ovvero $v=0$, e poi $w=w'=0$. Dunque $V_i\oplus W'$ sarebbe un sottomodulo che non interseca $W$ e più grande del massimale, assurdo.\\
     Quindi si ha $W\oplus W'=V$.
    \end{itemize}
   \end{proof}
   
   \begin{thm}[Maschke]\label{thm:maschke}
    Sia $G$ un gruppo finito, $\K$ un campo tale che $\Char\K\nmid\ord G$. Allora ogni rappresentazione di $\K[G]$ è semisemplice.
   \end{thm}
   \begin{proof}
    Osserviamo che se $\Char\K=0$, l'abbiamo fatto nella prima parte (teorema \ref{thm:gruppo finito completamente riducibile}).\\
    Chiamiamo $A=\K[G]$ l'algebra e $V$ la rappresentazione. Sia $W\subset V$ un sottomodulo.\\
    Consideriamo intanto $W'\subset V$ complemento di $W$, non stabile, ma tale che $V=W\oplus W'$; sia $p:V\to W$ la proiezione.\\
    Definiamo allora l'applicazione lineare \[\pi(v)=\frac1{\ord G}\sum_{g\in G} g\cdot p(g^{-1}\cdot v)\]
    Osserviamo che possiamo dividere per $\ord G$ per l'ipotesi sul campo (cioè $\ord G\neq 0$).\\
    Vale banalmente la proprietà $\pi(w)=w$ se $w\in W$: infatti $g^{-1}w\in W$, perciò $gp(g^{-1}w)=w$.\\
    Inoltre $\pi\in\Hom_A(V,W)$, poichè $\displaystyle\pi(hv)=\frac1{\ord G}\sum_{g\in G} g\cdot p(g^{-1}hv)=\frac1{\ord G}\sum_{g\in G} hh^{-1}g\cdot p((h^{-1}g)^{-1}v)=\frac1{\ord G}\sum_{t\in G} ht\cdot p(t^{-1}v)=h\pi(v)$  dove abbiamo effettuato la sostituzione $t=h^{-1}g$ che permuta gli elementi del gruppo.\\
    Infine $\pi$ è idempotente: $\displaystyle\pi^2(v)=\frac1{\ord G}\sum_{g\in G}\pi\left( g\cdot p(g^{-1}\cdot v)\right)=\frac1{\ord G}\sum_{g\in G}g\cdot\pi\left( p(g^{-1}\cdot v)\right)$; dato che $p$ mappa tutto su $W$, abbiamo $\pi(p(z))=p(z)$, perciò abbiamo esattamente quello che ci serviva.\\
    Allora $\pi$ è una proiezione, e vale $V=W\oplus\ker\pi$, dove $\ker\pi$ è un $A$-modulo, essendo $\pi$ un omomorfismo di rappresentazioni.
   \end{proof}
   
   \begin{defn}
    Una $\K$-algebra $A$ si dice \emph{semisemplice} se $A$ è un $A$-modulo semisemplice
   \end{defn}
   \begin{thm}
    Sia $G$ un gruppo finito, $\K$ un campo. Allora $\K[G]$ è semisemplice se e solo se $\Char\K\nmid\ord G$.
   \end{thm}
   \begin{proof}
    Una freccia l'abbiamo appena fatta: basta considerare $\K[G]$ come rappresentzione su se stesso, e applicare \ref{thm:maschke}.\\
    Dimostriamo ora che se $\Char\K\mid\ord G$, allora $A=\K[G]$ non è semisemplice.
    Sia $s=\sum_{g\in G}e_g$ e $V_0=\K s$ che è banalmente un sottomodulo di $A$.\\
    Supponiamo per assurdo che $A$ sia semisemplice; allora $V_0$ avrebbe un complementare stabile $A=V_0\oplus V_1$, e avremmo la proiezione $p:A\to V_0$ che è omomorfismo di $A$-moduli.\\
    Allora varrebbe $p(e_g)=p(ge_1)=gp(e_1)=p(e_1)$ poiché su $V_0$ $G$ agisce banalmente. Perciò $p(s)=\sum p(e_g)=\ord G\cdot p(e_1)=0$.\\
    Ma questo è assurdo, poiché dovrebbe essere $p(s)=s$ essendo $p$ la proiezione su $V_0$.
    
   \end{proof}




\subsection{Struttura di $\K[G]$ come algebra}
In questa sezione vogliamo dare un teorema di struttura per $\K[G]$ (con $\Char \K  \nmid |G|$).\\
Cerchiamo ora di descrivere $\K[G]$ come algebra nel caso di $\K=\overline{\K}$.
\begin{thm}\label{thm:end_matrici}
	Sia $G$ gruppo finito, $\K=\overline{\K}$ tale che $\Char \K \nmid |G|$, e sia $V$ una rappresentazione irriducibile di $\K[G]$. Allora $\dim V <\infty$, e si ha $\End_G(V^{\oplus n}) \iso M_n(\K)$ come $\K$-algebre.
\end{thm}

\begin{proof}
	Sia $A=\K[G]$, $0\neq v\in V$; allora ho un omomorfismo di rappresentazioni $f(a)=av$, la cui immagine è $V$ essendo semplice. Perciò si ha $V\iso\faktor{A}{\ker f}$, e poiché $A$ ha dimensione finita, anche $V$ ha dimensione finita.\\
	Considero ora le immersioni/proiezioni canoniche
	\begin{align*}
		q_i:V\to V^{\oplus n}\qquad \text{con }i=1,\ldots,n\\
		p_i:V^{\oplus n}\to V\qquad \text{con }i=1,\ldots,n
	\end{align*}
	Sia $\varphi\in \End_G(V^{\oplus n})$, consideriamo $\varphi_{ij}:V\to V$ definita come $\varphi_{ij} = p_i\circ\varphi\circ q_j\in\End_G(V)$:
	\[
		V \xrightarrow{q_i} V^{\oplus n} \xrightarrow{\varphi} V^{\oplus n} \xrightarrow{p_j} V
	\]
	$V$ è irriducibile quindi $\varphi_{ij} = c_{ij}\Id_V$ con $c_{ij}\in \K$ (per Schur), dunque trovo un'applicazione
	\begin{align*}
		\psi:\End_G(V^{\oplus n})&\to M_n(\K)\\
		\varphi&\to (c_{ij})
	\end{align*}
	Mostriamo ora che $\psi$ è un isomorfismo di algebre: $\psi$ è surgettiva, infatti, data $(c_{ij})\in M_n(\K)$, se consideriamo $\varphi = \sum c_{ij} q_i\circ p_j$ abbiamo che $\psi(\varphi) = (c_{ij})$; dimostriamo che è un omomorfismo, infatti siano $\varphi_1 = \sum c_{ij} q_i\circ p_j$ e $\varphi_2 = \sum d_{ij} q_i\circ p_j$, allora $\varphi_1\circ\varphi_2 = \sum c_{ij} d_{hk}\ q_i\circ p_j\circ q_h\circ p_k = \sum c_{ij}d_{jk}\ q_i\circ p_k$ dove l'ultima uguaglianza si ottiene dal fatto che se $j=h$ allora $p_j\circ q_h=\Id_V$, altrimenti è l'endomorfismo nullo, infine $\psi(\varphi_1\circ \varphi_2) = \sum c_{ij}d_{jk}\  q_i\circ p_k$ come per il prodotto tra matrici; la verifica dell'iniettività è immediata.
\end{proof}

\begin{thm}
	Sia $\K=\overline{\K}$ con $\Char \K \nmid |G|$ dove $G$ è un gruppo finito. Siano $V_1,\ldots,V_n$ le rappresentazioni irriducibili di $G$ su $\K$ (a meno di isomorfismi) e siano $m_1,\ldots,m_n$ le loro dimensioni. Allora come $\K$-algebre abbiamo un isomorfismo $\K[G] \iso M_{m_1}(\K)\times\ldots\times M_{m_n}(\K)$. In particolare $\K[G]$ è isomorfo ad un prodotto di algebre di matrici e $M_{m_h}(\K) = m_hV_h$.
\end{thm}

\begin{proof}
	$\K[G]$ è semisemplice e di dimensione finita, sarà somma diretta di moduli semplici: $\K[G]\iso W_1\oplus \ldots\oplus W_n$ con $W_i$ un $G$-modulo semplice. Si osserva che tutti gli irriducibili di $G$ compaiono tra i $W_i$: avevamo visto che tutti gli irriducibili devono comparire come fattori di composizione delle serie di Jordan-Holder di $A$. Quindi posso riscrivere $\K[G] = \oplus V_i^{p_i}$ dove $p_i$ è la molteplicità di $V_i$, allora $\dim \Hom_G(\K[G],V_i) = \dim\Hom_G(V_i^{\oplus p_i},V_i) = p_i$. Considero ora l'omomorfismo $\psi:\Hom_G(\K[G],V_i)\to V_i$ definito come $\psi(\phi) = \phi(e_1)$, esso è un isomorfismo (la verifica è facile), e quindi $p_i = m_i$. Ma $\K[G]\iso \K[G]^{OP}$ grazie all'applicazione inversa $g\to g^{-1}$, $\K[G]^{OP}$ lo posso pensare come $\End_{\K[G]}(\K[G]) \iso \End_{\K[G]}(\oplus V_i^{m_i}) \iso \oplus\End_{\K[G]}(V_i^{m_i})$ dove l'ultimo isomorfismo segue da Schur. Per il teorema \ref{thm:end_matrici} si ha che $\End_{\K[G]}(V_i^{m_i}) \iso M_{m_i}(\K)$ ma allora $\K[G] \iso \oplus M_{m_i}(\K)$.
\end{proof}

Da adesso considereremo $A$ un'algebra di dimensione finita.
\begin{prop}
	Se $A$ è un'algebra semisemplice allora ogni rappresentazione di $A$ di dimensione finita è semisemplice.
\end{prop}


\begin{proof}
	Sia $V$ un $A$-modulo, $\dim V = r$, consideriamo l'$A$-modulo di dimensione finita $A^{\oplus r}$ dove $A$ agisce per componenti. Sappiamo che $A$ è semisemplice, quindi $A=\oplus_{j=1}^{k} I_j$ con $I_j$ irriducibile (ovvero ideali sinistri minimali), allora $A^r=\oplus_{h=1}^{r}\oplus_{j=1}^{k} I_{hj}$ dove $I_{hj}$ è $I_j$ immerso nella $j$-esima copia di $A$. Gli $I_{hj}$ sono tutti moduli irriducibili quindi $A^r$ è semisemplice. Fissiamo un insieme di generatori $V=\langle v_1,\ldots,v_r\rangle$ e consideriamo l'omomorfismo
	\begin{align*}
		\psi:A^r&\to V\\
		(a_1,\ldots,a_r)&\to a_1v_1+\ldots+a_rv_r
	\end{align*}
	$\psi$ è evidentemente surgettiva ed è un omomorfismo di $A$-moduli, quindi $V$ è semisemplice poichè $V\iso A^r/\Ker \psi$ e abbiamo dimostrato che quozienti di semisemplici sono semisemplici (lemma \ref{quoz_smod_ss}).
\end{proof}

\begin{defn}
	Un'algebra di divisione $D$ sul campo $\K$ è un'algebra associativa, unitaria su $\K$ tale che ogni suo elemento ammette inverso.
\end{defn}

\begin{rem}
	Se $\K=\overline{\K}$ e $D$ è una $\K$-algebra di divisione con $\dim D<\infty$, allora $D=\K$. Infatti $\K\subset D$ (poichè $\K=Span(1)$), supponiamo per assurdo che esista $x\in D\setminus \K$, ma visto che $\dim D < \infty$ allora le potenze di $x$ soddifano un'equazione finita in $\K$ (quindi esiste un polinomio minimo per $x$ a coefficienti in $\K$), ma dato che $\K=\overline{\K}$ si ha che $x\in \K$, assurdo.
\end{rem}

\begin{rem}
	Sia $D$ una $\K$-algebra di divisione, considero $M_n(D)$: allora essa è una $\K$-algebra di divisione; se $\dim_{\K}D<\infty$ allora $\dim_{\K} M_n(\K) < \infty$.
\end{rem}

\begin{thm}
	Sia $D$ una $\K$-algebra di divisione con $\dim_{\K} D=k$, sia $A=M_n(D)$, allora $A$ è un'algebra semisemplice con un'unica rappresentazione irriducibile: $D^n$.
\end{thm}

\begin{proof}
	Come nell'esempio \ref{serie_comp_matrici}, possiamo considerare la serie di composizione di $A$ composta dai sottospazi con le ultime colonne nulle. I quozienti di due sottospazi consecutivi sono isomorfi a $D^n$; verifichiamo ora che $D^n$ è una rappresentazione irriducibile di $A$.\\
	Prendiamo un $0\neq v\in D^n$, quindi avrà una componente non nulla $v_i$; se prendo la matrice $M$ che ha tutti zeri, tranne $\dfrac1{v_i}$ in posizione $i,i$, allora $Mv=e_i$.
	Considerando ora le matrici $M_{jk}$ che hanno solo un $1$ sulla $k$-esima riga e $j$-esima colonna; allora $M_{jk}e_j=e_k$, quindi posso generare tutta la base di $D^n$.\\
	Sappiamo cioè $Av=D^n$ per tutti i $v$ , quindi non esistono sottospazi invarianti non banali, coè $D^n$ è irriducibile.\\
	Dato che i fattori di composizione della serie descritta prima sono solo $D^n$, e che ogni rappresentazione di $A$ compare come fattore di una serie di $A$, le rappresentazioni di $M_n(D)$ sono solo $V=D^n$.
\end{proof}


\begin{defn}
	Una $\K$-algebra è \textit{semplice} se non ha ideali bilateri non banali.
\end{defn}

\begin{prop}
	Sia $A$ un'algebra semplice di dimensione finita, allora $A$ è semisemplice.
\end{prop}
\begin{proof}
	Definiamo $J\subset A$ la somma di tutti gli ideali sinistri minimali di $A$: $J=\sum_j I_j$. $J$ è evidentemente un ideale sinistro, vediamo che è anche destro: considero $a\in A$, $I\subset A$ un ideale minimale, allora $Ia = 0$ oppure $Ia$ è un ideale minimale sinistro isomorfo a $I$, quindi $Ia\subset J$ e dunque $J$ è un ideale bilatero. $A$ era un'algebra semplice quindi $J=0$ o $J=A$. Se $J=A$ abbiamo finito in quanto abbiamo scritto $A$ come somma di sottomoduli semplici. Ma nell'ipotesi di $\dim A <\infty$ non avviene che $J=0$: si può fare per induzione sulla dimensione: se $A$ è irriducibile abbiamo finito, se invece ha un $A$-sottomodulo ci si può restringere a quello (che ha dimensione strettamente inferiore) e in un numero finito di passi si conclude.
\end{proof}

\begin{exmp}[Algebra di Weyl]
	Il teorema precedente è falso se $\dim A = \infty$: consideriamo $A=\C\left[ x,\frac{\diff}{\dx} \right]$ l'algebra dei polinomi in $x$ e nelle derivate rispetto a $x$. Più formalmente
	\[
		A=\quot{\C\langle x,y\rangle}{(yx=1+xy)}
	\]
	infatti $\frac{\diff}{\dx}(xp(x)) = p(x)+x\frac{\diff}{\dx}p(x)$ per cui $\frac{\diff}{\dx}x=1+x\frac{\diff}{\dx}$. $A$ è una $\C$ algebra, $\dim_{\C} A = \infty$, $A$ non ha ideali bilateri propri (è semplice) ma non è semisemplice (in particolare non ha ideali minimali sinistri).
\end{exmp}

\begin{prop}
 Se $V$ è un $A$-modulo semisemplice, allora possiede sottomoduli minimali (ovvero rappresentazioni irriducibili)
\end{prop}
\begin{proof}
 Sia $I\subset A$ un ideale massimale di $A$, e $0\neq v\in V$. Osserviamo che $Iv\subset Av$ sono sottomoduli di $V$, quindi sono semisemplici.\\
 In particolare esiste un $W\subset V$ stabile tale che $Av=Iv\oplus W$; se per assurdo $W$ non fosse irriducibile, avremmo $W=W_1\oplus W_2$, ovvero $Iv\oplus W_1$ sarebbe un sottomodulo di $Av$, cioè del tipo $Jv$ con $I\subset J\subset A$ ideale; ma le due inclusioni sono entrambe proprie grazie a $W_1$ e $W_2$, contraddicendo la massimalità di $I$.
\end{proof}

\begin{prop}
 Sia $D$ un'algebra di divisione su $\K$. Allora la $\K$-algebra $M_n(D)$ è semplice.
\end{prop}
\begin{proof}
 Sia $J\subset M_n(D)$ un ideale bilatero non nullo.\\
 Consideriamo $J_{ij}=\{x\in D:\exists X\in J, x=[X]_{i,j}\}$ ovvero gli elementi di $D$ che compaiono in posizione $ij$ di una qualche matrice di $J$.
 Vediamo che tutti i $J_{ij}$ sono ideali bilateri di $D$: infatti è ovvio che sia chiuso rispetto alla somma; per vedere che è chiuso a destra e a sinistra per moltiplicazione per $d\in D$, basta moltiplicare a destra o a sinistra per $d\Id$.\\
 Essendo $D$ un'algebra di divisione, gli ideali possono essere solo $0$ o $D$: se $d\in I\subset D$, allora anche $1=d\cdot\frac1 d\in I$, e allora $a\in I\;\forall a\in D$.\\
 Quindi ogni $J_{ij}$ è o $0$ o $D$; poiché $J\neq 0$, esiste una coppia di indici per cui $J_{ij}=D$, e in particolare $1\in J_{ij}$.
 Sia $X\in J$ tale che $[X]_{i,j}=1$; allora, essendo ideale bilatero, anche $E_{ii}XE_{jj}=E_{ij}\in J$, dove $E_{\alpha,\beta}$ sono le matrici elementari (nulle ovunque tranne un $1$ nella posizione $\alpha,\beta$).\\
 Poiché $E_{ab}E_{bc}=E_{ac}$, è immediato osservare che $E_{ab}=E_{ai}E_{ij}E_{jb}\in J$, perciò $J$ contiene una base di $M_n(D)$, perciò è tutto.
\end{proof}

\begin{prop}
 Siano $A_1,\dots,A_n$ algebre semisemplici di dimensione finita. Allora $A=A_1\times\dots\times A_n$ è semisemplice, e gli ideali bilateri di $A$ sono della forma $I=I_1\times\dots\times I_n$, dove $I_i$ è un ideale bilatero di $A_i$.
\end{prop}
\begin{proof}
 Poiché $A_i$ sono semisemplici, possiamo scrivere $A_i=\bigoplus_j I_{ij}$, dove $I_{ij}$ è un ideale minimale, ovvero un modulo semplice.
 Allora possiamo anche scrivere $A=\bigoplus_i\bigoplus_j I_{ij}$, dove gli ideali sono ancora minimali.\\
 Sia poi $J\subset A$ un ideale bilatero; se consideriamo $J_j=J\cap A_j$, questo è un ideale bilatero di $A_j$; inoltre $\bigoplus J_j\subset J$.
 Per vedere che sono uguali, prendiamo $a\in J$ e scriviamolo come $a=a_1+\dots+a_n$ dove $a_i\in A_i$; considero allora l'unità $e_i$ di $A_i$, e vedo che $e_ia=a_i$ appartiene ancora a $J$, cioè $a_i\in J_i$.\\
 FORSE VA RISCRITTA MEGLIO FORMALIZZANDO CON LE N-UPLE AL POSTO DELLE SOMME? COSÌ È PIÙ EVIDENTE CHE LE COMPONENTI NON SI PARLANO\\

\end{proof}











\subsection{Un teorema di divisibilità}
  L'obiettivo di questa sezione è dimostrare il seguente teorema:
  \begin{thm}\label{dim_mid_ordine}
    Sia $G$ un gruppo finito, $V$ una rappresentazione irriducibile di $G$ su $\C$ (ovvero un $\C[G]$ modulo semplice). Allora $$\dim V\mid  \ord(G)$$
  \end{thm}
  \begin{rem}$ $
   \begin{itemize}
    \item Il teorema è falso su campi non algebricamente chiusi: basta considerare $\Z/3\Z$ su $\R$ che ha una rappresentazione irriducibile di grado 2.
    \item Il teorema vale più in generale su un campo $\K$ algebricamente chiuso per cui $\Char \K \nmid \ord(G)$.
    \item Se invece $\Char \K\mid \ord(G)$, il teorema è in generale falso: consideriamo $G=SL_2(\mathbb F_{13})$ e $\K=\overline{\mathbb F_{13}}$; prendiamo poi la rappresentazione $V=\K[x,y]_5$, che è irriducibile (IO NON SO FARLO) di grado $6$. Tuttavia $\ord(G)=\binom{14}{3}=2^2\cdot7\cdot13$, quindi non c'è divisibilità.
   \end{itemize}
  \end{rem}
  Introduciamo ora alcuni concetti di teoria degli anelli.\\
  Sia $R$ un anello commutativo unitario.\\
  \begin{defn}
    Un elemento $x\in R$ si dice \textit{integrale} su $\Z$ se è radice di un polinomio monico a coefficienti interi.\\
    Se $x\in\C$ lo chiamo \textit{intero algebrico}.
  \end{defn}
  \begin{rem}\label{int_alg_raz}
   Gli interi algebrici di $\Q$ sono esattamente gli interi, per il criterio delle radici razionali.
  \end{rem}
  Possiamo inoltre definire una struttura di modulo usando come scalari gli elementi di $R$:
  \begin{defn}
   Sia $M$ un gruppo abeliano; allora $M$ è un $R$-modulo se esiste un'applicazione $\cdot:R\times M\to M$ che soddisfa
   \begin{itemize}
    \item $r\cdot(m+n)=r\cdot m+r\cdot n\;\forall r\in R\;\forall m,n\in M$
    \item $(r+s)\cdot m=r\cdot m+s\cdot m\;\forall r,s\in R\;\forall m\in M$
    \item $r\cdot(s\cdot m)=(ab)\cdot m\;\forall r,s\in R\;\forall m\in M$
   \end{itemize}
   Diciamo inoltre che $M$ è finitamente generato se lo è come gruppo.
  \end{defn}
  \begin{rem}
   Gli $\Z$-moduli sono esattamente i gruppi abeliani.
  \end{rem}
  Definiamo ora un'importante proprietà degli anelli
  \begin{defn}
   Un anello $R$ è detto \textit{Noetheriano} se tutti i suoi ideali sono finitamente generati; o equivalentemente, se ogni catena ascendente di ideali si stabilizza.
  \end{defn}
  \begin{thm}[base di Hilbert]
   Se $R$ è un anello Noetheriano, allora anche $R[x]$ è Noetheriano.
  \end{thm}
  \begin{proof}
   Sia $I\subset R[x]$ un ideale per assurdo non finitamente generato. Allora posso costruire una successione di polinomi $p_n$ tali che, se $I_n=(p_1,\dots,p_{n-1})$ allora $p_n\in I\setminus I_n$, e sceglo $p_n$ di grado minimale.\\
   Definiamo ora $a_n$ come il termine di testa di $p_n$, e consideriamo $R\supset J=(a_1,a_2,\dots)$ l'ideale generato da tutti gli $a_i$.\\
   Poiché $R$ è Noetheriano, vale che $J=(a_1,\dots,a_{N-1})$ per un certo $N$; ma allora posso scrivere $a_N=\sum_{i<N}b_ia_i$ con $b_i\in R$.\\
   Prendo ora il polinomio $\displaystyle f=\sum_{i<N}b_i x^{\deg p_N -\deg p_i}p_i$ (è una buona definizione, poiché i gradi dei polinomi sono crescenti).
   Osserviamo che $f\in I_N$ essendo combinazione lineare dei $p_i$; inoltre ha lo stesso grado e lo stesso coefficiente di testa di $p_N$.\\
   Ma allora $p_N-f\in I\setminus I_N$, perché se fosse $p_N-f\in I_N$ avremmo $p_N=(p_N-f)+f\in I_N$; tuttavia $\deg (p_N-f)<\deg p_N$, che è assurdo in quanto $p_N$ ha grado minimo in $I\setminus I_N$
  \end{proof}
  \begin{cor}
   Si vede facilmente per induzione che $R[x_1,x_2,\dots,x_n]$ è Noetheriano se $R$ lo è.
  \end{cor}
  \begin{prop}\label{sottomod_fin_gen}
   Sia $R$ un anello Noetheriano, $M$ un $R$-modulo finitamente generato. Allora ogni sottomodulo di $M$ è finitamente generato.
  \end{prop}
  LUI FA UNA ZINGARATA ASSURDA. CREDO CHE INVECE DEL TEO DI HILBERT SERVA CHE $R^n$ SIA NOETH.
  \begin{proof}
   Scriviamo $M=Rm_1+Rm_2+\dots+Rm_n$, dove $m_1,\dots,m_n$ sono i generatori di $M$.\\
   Consideriamo la mappa $\varphi:R^n\to M$ tale che $\varphi(a_1,\dots,a_n)=a_1m_1+\dots+a_nm_n$, che è surgettiva per ipotesi.\\
   Detto allora $I=\ker\varphi$, si ha $M\iso\faktor{R^n}{I}$ e in particolare si ha la corrispondenza tra ideali di $M$, ovvero sottomoduli, e ideali di $R^n$ che contengono $I$.
   Ovvero, se $N\subset M$ è un sottomodulo allora esiste un ideale $J\subset R^n$ tale che $N\iso\faktor{J}{I}$.\\
   Se $R$ è Noetheriano, anche $R^n$ è Noetheriano, quindi $J$ è finitamente generato, e quindi lo è anche $N$ (i generatori sono le proiezioni di quelli di $J$).
  \end{proof}
  Possiamo ora dare un criterio di integralità per elementi di $R$.
  \begin{prop}\label{crit_integ}
   Dato $R$ anello commutativo unitario e un elemento $x\in R$, sono equivalenti:
   \begin{itemize}
    \item $x$ è integrale su $\Z$
    \item Detto $\Z[x]\subset R$ il sottoanello generato da $x$, $\Z[x]$ è uno $\Z$-modulo finitamente generato
    \item Esiste uno $\Z$-sottomodulo di $R$ finitamente generato che contiene $\Z[x]$
   \end{itemize}
  \end{prop}
  \begin{proof}
   \begin{itemize}
    \item[$2)\iff 3)$] Usiamo la proposizione \ref{sottomod_fin_gen} ed il fatto che $\Z$ è Noetheriano.
    \item[$1)\Rightarrow2)$] Sia $t^n+a_{n-1}t^{n-1}+\dots+a_0$ un polinomio a coefficienti interi che annulla $x$; allora $\Z[x]$ è generato da $\{1,x,\dots,x^{n-1}\}$.
    \item[$2)\Rightarrow1)$] Osserviamo che $\Z[x]=\bigcup R_n$, con $R_n$ il sottomodulo generato da $\{1,x,\dots,x^{n-1}\}$. Se $\Z[x]$ è finitamente generato, allora deve essere $\Z[x]=R_N$ per un certo $N$.
    Ma allora si ha semplicemente che $x^N=\sum_{i<N}a_ix^i$ con $a_i\in\Z$
   \end{itemize}
  \end{proof}
  Per dimostrare un'importante proprietà degli elementi integrali, dobbiamo costruire il prodotto tensore di moduli.\\
  \begin{defn}
   Dati due $R$-moduli $M,N$ si dice prodotto tensore $M\otimes N$ un $R$-modulo $V$ per cui esiste una funzione bilineare $\psi:M\times N\to V$ tale che per ogni $R$-modulo $V'$ con una funzione bilineare $\phi:M\times N\to V'$ esiste un'unica funzione lineare $\phi':V\to V'$ che commuta.
   \[\tridiag{M\times N}{ \psi }{M \otimes N}{\phi'}{V'}{\phi}\]
   Si indica $m\otimes n=\psi(m,n)$
  \end{defn}
  Si dimostra che il prodotto tensore esiste ed è unico; un'idea di costruzione è la seguente.\\
  Consideriamo il prodotto $M\times N$ con tutti i suoi elementi $(m,n)$ che identifichiamo con i simboli $m\ast n$; prendiamo ora $P$ il gruppo libero generato da tutti i simboli $m\ast n$.\\
  Quozientiamo poi $P$ per le relazioni che ci interessano, ovvero:
  \begin{itemize}
   \item $m\ast n+m\ast n'-m\ast(n+n')$
   \item $m\ast n+m'\ast n-(m+m')\ast n$
   \item $(rm)\ast n-m\ast(rn)$
  \end{itemize}
  con $m\in M,n\in N,r\in R$.\\
  L'ultima relazione ci dice come dotare questo gruppo di un prodotto per scalare, ovvero $r\cdot(m\otimes n)=(rm)\otimes n=m\otimes(rn)$.\\
  Possiamo ora andare avanti con la seguente
  \begin{prop}
   L'insieme degli elementi di $R$ integrali su $\Z$ è un sottoanello di $R$
  \end{prop}
  \begin{proof}
   Per il criterio \ref{crit_integ} se $x,y$ sono integrali su $\Z$, allora $\Z[x]$ e $\Z[y]$ sono finitamente generati.\\
   Considero allora l'applicazione lineare $\phi:\Z[x]\otimes_\Z\Z[y]\to\Z[x,y]$ data da $\phi(p\otimes q)=p\cdot q$; questa è surgettiva poiché posso scomporre monomio per monomio.\\
   Inoltre il prodotto tensore di due moduli finitamente generati è finitamente generato; ma allora $\Z[x,y]$ è immagine di un modulo finitamente generato, quindi anche lui è finitamente generato.\\
   Applicando di nuovo il criterio \ref{crit_integ}, abbiamo che $x+y$ e $xy$ sono integrali su $\Z$, in quanto $\Z[x+y]$ e $\Z[xy]$ sono sottomoduli di $\Z[x,y]$
  \end{proof}

  Ritorniamo ora al nostro $\C[G]$; ricordiamo che per UN CRITERIO A CASO FATTO PRIMA (MAGARI SI PUÒ SPOSTARE QUA?) $Z(\C[G])$ ha per base gli elementi $c_i=\sum_{x\in\mathcal C_i}x$ dove $\mathcal C_i$ sono le classi di coniugio di $G$.\\
  \begin{prop}
   Sia $Z(\C[G])\ni u=\sum u_ic_i$; se $u_i$ sono interi algebrici, allora $u$ è integrale su $\Z$.
  \end{prop}
  \begin{proof}
   Poiché gli elementi integrali sono un sottoanello, ci basta mostrare che tutti i $c_i$ sono integrali.\\
   Osserviamo che per ogni $j,k$ l'elemento $c_jc_k$ appartiene ancora al centro, e dunque è combinazione lineare dei $c_i$; questo vuol dire che $\Z[c_1,\dots,c_k]$ è finitamente generato (in particolare dai $c_i$).\\
   Ma allora di nuovo per il criterio \ref{crit_integ}, dato che $\Z[c_i]\subset\Z[c_1,\dots,c_k]$, abbiamo che $c_i$ è integrale.
  \end{proof}
  \begin{lemma}\label{u_int_alg}
   Sia $V$ una rappresentazione irriducibile di $G$, $\chi$ il suo carattere, $Z(\C[G])\ni u=\sum gu_g$ con $u_g$ interi algebrici. Allora $$\frac1{\dim V}\sum u_g\chi(g)$$ è un intero algebrico.
  \end{lemma}
  \begin{proof}
   Per Schur, $u$ agisce su $V$ come lo scalare $\displaystyle\frac{\tr(u)}{\dim V}=\frac{\sum u_g\chi(g)}{\dim V}$.\\
   Abbiamo allora un'omomorfismo $\varphi:Z(\C[G])\to\C$ dato proprio da $\displaystyle\varphi(u)=\frac{\sum u_g\chi(g)}{\dim V}$, in quanto $(u_1u_2)v=u_1(u_2v)$ con $v\in V$, cioè $\varphi(u_1u_2)v=\varphi(u_1)\varphi(u_2)v$.\\
   Per il lemma precedente sappiamo che $u$ è integrale, quindi soddisfa un polinomio monico; sfruttando il fatto che $\varphi$ è omomorfismo, sappiamo che anche $\varphi(u)$ soddisfa lo stesso polinomio monico, ovvero $\varphi(u)$ è un intero algebrico.
  \end{proof}
  Armati di questo lemma possiamo finalmente dimostrare il teorema \ref{dim_mid_ordine}
  \begin{proof}
   Sia $\chi$ il carattere della rappresentazione; poiché il gruppo è finito vale $g^{\ord(G)}=1$, da cui gli autovalori di $g$ sono tutti radici dell'unità; allora $\chi(g)$ è intero algebrico, essendo somma di radici dell'unità che sono interi algebrici.\\
   Poniamo $\displaystyle u=\sum_{g\in G}\chi(g^{-1})g$ e osserviamo che appartiene al centro: $\displaystyle hu=\sum_{g\in G} \chi((hg)^{-1}h)hg=\sum_{\sigma\in G} \chi(\sigma h)\sigma$ e similmente $\displaystyle uh=\sum_{\tau\in G}\chi(h\tau)\tau$; poiché $\tr(\sigma h)=\tr(h\sigma)$, abbiamo $uh=hu\;\forall h\in G$.\\
   Ma allora per il lemma \ref{u_int_alg} $z=\displaystyle\frac1{\dim V}\sum_{g\in G}\chi(g^{-1})\chi(g)$ è un intero algebrico.\\
   Osserviamo che $\displaystyle z=\frac{\ord(G)}{\dim V}\cdot \frac{\sum \chi(g^{-1})\chi(g)}{\ord(G)}=\frac{\ord(G)}{\dim V}\cdot\langle \chi,\chi\rangle$; essendo $V$ irriducibile, il prodotto dei caratteri vale $1$.\\
   Quindi si ha che $\dfrac{\ord(G)}{\dim V}\in\Q$ è un intero algebrico, e quindi per l'osservazione \ref{int_alg_raz} sta in $\Z$.
  \end{proof}



\section{Rappresentazioni di $S_n$}
Cerchiamo ora quali sono le rappresentazioni del gruppo simmetrico; in particolare caratterizziamo i moduli semplici di $A=\Q[S_n]$.\\
Come prima osservazione, sappiamo che le rappresentazioni irriducibili di un gruppo $G$ sono al più tante quante le classi di coniugio di $G$; per $G=S_n$, le classi di coniugio sono in corrispondenza biunivoca con le partizioni di $n$: infatti tutti gli elementi di una classe condividono la stessa struttura ciclica.\\
A loro volta, le partizioni possono essere pensate come particolari tabelle, nel modo seguente.
\begin{defn}
	Una $n$-tabella è un insieme di $n$ quadratini disposti come in una scacchiera, in modo che il numero di quadratini presenti in ogni riga sia minore o uguale al numero di quadratini presenti nella riga sopra.
\end{defn}
Per costruire i moduli che ci interessano, è necessario riempire le tabelle con dei numeri.
\begin{defn}
	Un $n$-diagramma $D$ è una $n$-tabella dove in ogni quadratino vi è un numero da $1$ a $n$, in modo che ogni numero compaia una ed una sola volta nell'intera tabella.
\end{defn}
\begin{exmp}
	Sia $n=7$, $\mathcal{C}$ la classe di coniugio di $(123)(45)(67)$; allora la partizione corrispondente a $\mathcal{C}$ è $3,2,2$, che ha la seguente tabella; uno dei possibili diagrammi è \\ $\ydiagram{3,2,2}$ $\ytableaushort{356,14,27}$
\end{exmp}
Osserviamo che $S_n$ agisce transitivamente sull'insieme dei diagrammi con la stessa forma nel modo ovvio.\\
Fissato un diagramma $D$, siamo interessati ai seguenti due sottogruppi di $S_n$:
\begin{defn}$ $
	\begin{itemize}
		\item $R(D)=\{\sigma\in S_n : \sigma(i) \text{ sta nella stessa riga di } i\;\forall i=1,\dots,n\}$
		\item $C(D)=\{\sigma\in S_n : \sigma(i) \text{ sta nella stessa colonna di } i\;\forall i=1,\dots,n\}$
	\end{itemize}
\end{defn}
Osserviamo che $R(D)\cap C(D)=\{id\}$.\\
Possiamo ora definire l'elemento generatore degli ideali che che ci interessano.
\begin{defn}
	Dato un diagramma $D$, il suo simmetrizzatore è $$e_D=\left(\sum_{p\in R(D)}p \right)\left(\sum_{q\in C(D)}q\varepsilon_q\right)$$
	Inoltre il modulo corrispondente è $V_D=Ae_D$
\end{defn}
\begin{prop}\label{prop:assorbimento eD}
	Se $p\in R(D)$ e $q\in C(D)$ allora $pe_D=e_D$ e $e_Dq=e_D\varepsilon_q$
\end{prop}
\begin{prop}
	Sia $g\in S_n$; allora $e_{gD}=ge_Dg^{-1}$
\end{prop}
\begin{proof}
	Vediamo che $R(gD)=gR(D)g^{-1}$. Infatti $\sigma\in R(gD)\iff\sigma(i)$ sta nella stessa riga di $i$ all'interno del diagramma $gD$, ovvero $\sigma g (j)=g(g^{-1}\sigma g)(j)$ sta nella stessa riga di $g(j)$ (abbiamo scritto $i=g(j)$ con $j$ pensato in $D$); questo si verifica se e solo se $g^{-1}\sigma g(j)$ sta nella stessa riga di $j$ in $D$, ovvero $g^{-1}\sigma g(j)\in R(D)$, che è quello che volevamo.\\
	Similmente $C(gD)=gC(D)g^{-1}$.\\
	Allora $\displaystyle e_{gD}=\left(\sum_{p\in R(gD)}p \right)\cdot\left(\sum_{q\in C(gD)}q\varepsilon_q\right)=\left(\sum_{p'\in R(D)}gp'g^{-1} \right)\cdot\left(\sum_{q'\in C(D)}gq'g^{-1}\text{sgn}(gq'g^{-1})\right)$; tirando fuori dalle parentesi le $g$ e usando $\text{sgn}(gq'g^{-1})=\text{sgn}(q')$ si ha la tesi.
\end{proof}
\begin{cor}
	Se $D$ e $D'$ sono diagrammi con la stessa forma, allora $V_D\iso V_{D'}$
\end{cor}
\begin{proof}
	Poiché $D$ e $D'$ hanno la stessa forma, esiste un $g\in S_n$ tale che $D'=gD$, e perciò $e_{D'}=ge_Dg^{-1}$.\\
	Allora $Ae_{D'}=Age_Dg^{-1}$. Osserviamo che $1=g^{-1}g\in Ag$ perciò $Ag=A$; inoltre $Ae_Dg^{-1}\iso Ae_D$ grazie all'omomorfismo $xe_Dg^{-1}\mapsto xe_D$ che è iniettivo
\end{proof}
\begin{lemma}\label{Sn:g=pq}
	Sia $D$ un diagramma e $g\in S_n$. Allora possiamo scrivere $g=pq$ con $p\in R(D)$ e $q\in C(D)$ se e solo se nessuna coppia di numeri che si trova nella stessa diga di $D$ sta anche sulla stessa colonna di $gD$.
\end{lemma}
\begin{proof}$ $
	\begin{itemize}
		\item[$\Rightarrow$] Scrivo $g=pq$; siano $a,b$ due numeri sulla stessa riga di $D$. Allora $a,b$ stanno sulla stessa riga anche in $pD$. Inoltre $gD=(pqp^{-1})pD$ e $pqp^{-1}\in C(pD)$; perciò $gD$ ha le stesse colonne di $pD$, e in particolare poiché $a,b$ stavano in colonne diverse di $pD$, stanno anche su colonne diverse di $gD$.
		\item[$\Leftarrow$] Per transitività mostro che $gD=pqD$ con certi $p,q$ della forma cercata. Guardo la prima colonna di $gD$: tutti i suoi elementi stanno in righe diverse di $D$ per ipotesi, quindi posso scegliere un $p_1\in R(D)$ tale che $p_1D$ e $gD$ abbiano gli stessi elementi nella prima colonna. Continuando in questo modo trovo un $p\in R(D)$ tale che $pD$ e $gD$ hanno le stesse colonne; allora esiste un $q'\in C(pD)$ per cui $q'pD=gD$; ma $q'=pqp^{-1}$ con $q\in C(D)$, perciò si ha proprio $pqD=gD$.
	\end{itemize}
\end{proof}
Osserviamo che le partizioni di $n$ possono essere ordinate in modo lessicografico.\\
Dimostriamo ora questo importante lemma di ortogonalità dei simmetrizzatori.
\begin{lemma}\label{Sn:ortogonalita}
	Sia $D$ un diagramma di forma $\lambda$ e $D'$ un diagramma di forma $\lambda'$; allora se $\lambda>\lambda'$ vale $e_{D'}\cdot e_{D}=0$
\end{lemma}
\begin{proof}
	Per prima cosa vogliamo trovare due simboli $a,b$ nella stessa riga di $D$ e nella stessa colonna di $D'$; supponiamo per assurdo che non esistano.\\
	Diciamo che $\lambda: a_1\ge\dots a_k$ e $\lambda':a_1'\ge\dots a_h'$; notiamo che gli $a_1$ simboli della prima riga di $D$ devono stare tutti in colonne diverse di $D'$, per l'ipotesi di assurdo. Ciò vuol dire che $D'$ ha almeno $a_1$ colonne, ma valeva anche $a_1\ge a_1'$: perciò si deve avere $a_1=a_1'$. Sia ora $m$ il numero di righe lunghe $a_1$ elementi di $\lambda$ e $m'$ il suo analogo; poiché $\lambda>\lambda'$ si ha $m\ge m'$. Inoltre tutti gli elementi delle prime $m$ righe di $D$ devono andare nelle prime $m'$ righe di $D'$, ma allora in $D$ ho $ma_1$ elementi da mettere in $m'a_1$ posti in $D'$, ovvero $m'\ge m$, perciò $m=m'$.\\
	Non può essere che queste $m$ righe fossero tutta la tabella, poiché $\lambda$ e $\lambda'$ sono diversi; allora le tabelle continuano, e poiché gli elementi delle prime $m$ righe restano nelle prime $m$ righe, possiamo dimenticarci di loro e ripetere il ragionamento togliendo le prime $m$ righe sia da $\lambda$ che da $\lambda'$.\\
	Ma quindi abbiamo creato un processo infinito, in cui ogni volta la lunghezza della prima riga decresce strettamente, il che è assurdo.\\
	Allora abbiamo davvero $a,b$ nella stessa riga di $D$ e nella stessa colonna di $D'$; sia $t=(a,b)\in S_n$, allora $t\in R(D)\cup C(D')$.\\
	Si ha dunque $e_{D'}\cdot e_{D}=e_{D'}\cdot t\cdot t\cdot e_{D}=(-e_{D'})\cdot e_D$ per l'osservazione \eqref{prop:assorbimento eD}, ovvero $2\cdot e_{D'}\cdot e_{D}=0$
\end{proof}
Troviamo ora una caratterizzazione di $V_D$
\begin{lemma}\label{Sn:caratterizzazione}
	Fissiamo un diagramma $D$; sia $x\in A$ tale che $\forall p\in R(D),q\in C(D)$ si abbia $pxq=\varepsilon_qx$. Allora $\exists\gamma\in\Q$ tale che $x=\gamma e_D$.
\end{lemma}
\begin{proof}
	Scriviamo $x=\sum a_gg$; l'ipotesi è che $x=\varepsilon_q p^{-1}x^{-1}=\varepsilon_q=\varepsilon_q\sum a_g p^{-1}gq^{-1}\sum a_{pgq}g$ shiftando gli indici $g\mapsto pgq$. Questo ci dice che $\varepsilon_q a_{pgq}=a_g$ per tutti i $p,q$; in particolare con $g=\mathrm{id}$ otteniamo $a_pq=\varepsilon_q a_1$, che sono esattamente i coefficienti di $e_D$.\\
	Dimostriamo che se $g\neq pq$ con $p\in R(D),q\in C(D)$ allora $a_g=0$; per il lemma \eqref{Sn:g=pq} esistono $a,b$ sulla stessa riga di $D$ e sulla stessa colonna di $gD$. Allora $t=(a,b)\in R(D)\cap C(gD)$, e inoltre $q=g^{-1}tg\in C(D)$ è una trasposizione.\\
	Allora usiamo la formula trovata con $p=t,q=g^{-1}tg$, da cui $a_g=-a_{tgg^{-1}tg}=-a_g$, cioè $a_g=0$
\end{proof}
\begin{prop}
	Esiste un $\gamma\in\Q^\ast$ tale che $e_D^2=\gamma e_D$, ovvero $u_D=\gamma^{-1}u_D$ è idempotente
\end{prop}
\begin{proof}
	Per il lemma precedente, mi basta far vedere che $pe_D^2q=\varepsilon_q e_D^2$; ma questa è ovvia poiché $pe_D^2q=(pe_D)(e_Dq)=e_D\cdot\varepsilon_q e_D$. Allora $e_D^2=\gamma e_D$.\\
	La cosa che vogliamo è $\gamma\neq0$. Sia $T:A\mapsto Ae_D$ l'omomorfismo $T(x)=xe_D$, e calcoliamo $\tr T$ in due modi diversi.\\
	Una base di $A$ è $S_n=\{g_1,g_s\dots,g_{n!}\}$ con $g_1=\mathrm{id}$. Se scriviamo $e_D=\sum a_i g_i$, si vede che il coefficiente di $g_j$ nella scrittura di $T(g_j)$ è esattamente $a_1$, poiché la moltiplicazione per un elemento di base permuta i coefficienti. Allora vale $\tr T=a_1\cdot n!=n!$ in quanto $a_1=1$ dalla definizione.\\
	Consideriamo ora $\Imm T=Ae_D$ che è un sottospazio di $A$; se $x\in Ae_D$, allora $x=ye_D$ per cui $T(x)=ye_D^2=y\gamma e_D=\gamma x$, ovvero in $Ae_D$ tutti gli elementi sono autovettori. Allora è chiaro che se prendo una base di $Ae_D$ e la completo a base di $A$, $T$ si scriverà nella forma $\left(\begin{array}{ccccc}
	\gamma & & & & \\
	& \ddots & & & \\
	& & \gamma & & \\
	& & & \ddots & \\
	& & & & 0
	\end{array}\right)$ perciò $tr T=\gamma\cdot\dim Ae_D=\gamma m$.\\
	Si ha dunque $n!=\tr T = \gamma m$, da cui $\gamma=\frac{n!}{m}\neq0$; dato che $e_D$ ha come coefficienti solo $\pm1$, allora $e_D^2\in\Z[S_n]$, perciò si deve avere $\gamma\in\N$, ovvero $\dim V_D\mid |S_n|$
\end{proof}

Armati di tutti questi lemmi possiamo avviarci alla conclusione, ovvero dimostrare che i $V_D$ sono tutte le rappresentazioni irriducibili, al variare della forma di $D$.

\begin{prop}
	Dato un diagramma $D$, $V_D=Ae_D$ è un ideale minimale, ovvero un sottomodulo semplice
\end{prop}
\begin{proof}
	Grazie alla proposizione precedente, sappiamo che $u_D$ è idempotente ed è un multiplo scalare di $e_D$; perciò $Ae_D=Au_D$.\\
	Usando il criterio METTERE IL REF QUANDO SARÀ SCRITTO, $Au_D$ è minimale se e solo se $u_DAu_D$ è un corpo, cosa che avviene se e solo se $e_DAe_D$ lo è.\\
	Prendiamo $x\in e_DAe_D$, cioè $x=e_Dye_D$; osserviamo che $pxq=(pe_D)y(e_Dq)=e_Dye_D\varepsilon_q=\varepsilon_q x$, perciò per il lemma \eqref{Sn:caratterizzazione} abbiamo $x=ke_D$ con $k\in\Q$.\\
	Allora $e_DAe_D=\Q e_D=\Q u_D$ che è un corpo isomorfo a $\Q$ con elemento neutro del prodotto proprio $u_D$.
\end{proof}
\begin{prop}
	Siano $D,D'$ diagrammi di forma diversa $\lambda\neq\lambda'$, allora $V_D\noniso V_{D'}$
\end{prop}
\begin{proof}
	Supponiamo che $\lambda>\lambda'$. Per assurdo $V_D\iso V_{D'}$.\\
	Essendo minimali, esiste un $a\in A$ tale che $Au_Da=Au_{D'}$, cioè $u_{D'}=bu_{D}a$. Allora $u_{D'}^2=u_{D'}\cdot u_{D'}=u_{D'}bu_{D}a$.\\
	Tuttavia, $\forall g\in S_n$ vale che $u_{D'}gu_{D}g^{-1}=u_{D'}\cdot u_{gD}=0$ per il lemma \eqref{Sn:ortogonalita}. Ma allora $u_{D'}bu_{D}=0\;\forall b\in A$, che porterebbe a $u_{D'}=0$, assurdo.
\end{proof}
Possiamo ora unificare tutto il nostro lavoro nel seguente
\begin{thm}$ $
	\begin{enumerate}
		\item C'è una corrispondenza biunivoca $\{\text{classi di coniugio di }S_n\} \leftrightarrow \{\text{partizioni di }n\} \leftrightarrow \{n-\text{tabelle}\}$
		\item Abbiamo una mappa surgettiva $\{\text{diagrammi}\}\to\text{Irr}(S_n)$ data da $D\mapsto V_D=Ae_D$
		\item $V_D\iso V_{D'}$ se e solo se $D$ e $D'$ hanno la stessa forma
	\end{enumerate}
\end{thm}

Facciamo ora un po' di esempi e casi particolari.\\
\begin{itemize}
	\item
	\begin{ytableau}
		$ $ & &\none[\cdots] & \\
	\end{ytableau}\\
	La tabella di una sola riga lunga $n$, a cui corrisponde $e_D=\sum_{\sigma\in S_n}\sigma$ cioè la rappresentazione banale.

	\item
	\begin{ytableau}
		$ $ \\
		\\
		\none[\vdots]\\
		\\
	\end{ytableau}\\
	La tabella di una sola colonna lunga $n$, a cui corrisponde $e_D=\sum_{\sigma\in S_n}\sigma\varepsilon_\sigma$ che è la rappresentazione segno.

	\item
	\begin{ytableau}
		$ $ & &\none[\cdots] & \\
		$ $\\
	\end{ytableau}\\
	La tabella con $n-1$ quadratini nella prima riga e $1$ nella seconda è la rappresentazione standard di $S_n$
\end{itemize}

Concludiamo con una formula che ci permette di calcolare la dimensione di una rappresentazione a partire dalla sua tabella.
\begin{thm}[formula degli uncini]
	Sia $\lambda$ una $n$-tabella, allora $$\dim V_\lambda=\frac{n!}{\prod_{i=1}^n\ell_i}$$ dove $\ell_i$ è la lunghezza dell'uncino che parte dalla casella $i$, ovvero il massimo percorso a L rovesciata che passa dalla casella $i$
\end{thm}



\end{document}
