\documentclass[11pt]{article}


\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[a4paper]{geometry}
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{array}
\usepackage{xy}
\usepackage{multicol}
%\usepackage{slashbox}
\usepackage{fancyhdr}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[T1,OT1]{fontenc} 
\usepackage[nohug,small]{diagrams}
\usepackage{bm}


\usepackage{grffile}
\usepackage{tikz}
\usepackage{pgf,tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc}

\usetikzlibrary{arrows}
\topmargin 0cm
\oddsidemargin 0cm
\evensidemargin 0cm
\textwidth 16.5cm
\textheight	23.5cm
\marginparwidth 2cm
\marginparpush 2cm



\title{Dispense del corso di Teoria della Rappresentazione}
\author{Fabio Zoratti}
\date{\today}



\makeindex

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{post}[thm]{Postulato}
\newtheorem*{cor}{Corollario}

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{prob}{Problema}[section]
\newtheorem{hint}{Suggerimento}[section]
\newtheorem{sol}{Soluzione}[section]
\newtheorem*{rem}{Osservazione}

\theoremstyle{remark}
\newtheorem*{note}{Nota}





\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}

\newcommand{\tr}{\textnormal{tr}}



\newcommand{\tridiag}[6]{
	\[
	  \begin{diagram}
	  #1 & \rTo^{#2}  & #3        \\
	     & \rdTo_{#6} & \dTo>{#4}   \\
	     &          & #5
	  \end{diagram}
	\]
}  
\newcommand{\quaddiag}[8]{
      \[
	\begin{diagram}
	#1     & \rTo^{#2} & #3 \\
	\dTo<{#6} &         & \dTo>{#4} \\
	#7     & \rTo^{#8} & #5
	\end{diagram}
      \]
}
















\begin{document}
\maketitle
\tableofcontents



\newpage
\section{Teoria dei gruppi}

\begin{defn}[Gruppo] Un gruppo è un insieme dotato di un'operazione binaria $\cdot : G\times G \to G$ che gode delle seguenti proprietà:
\begin{enumerate}
	\item Associatività: presi comunque $a,b,c\in G$ vale che $(a\cdot b)\cdot c = a\cdot(b\cdot c)$
	\item Esiste $e\in G$, chiamato \emph{unità}, o \emph{identità}, o \emph{elemento neutro}, tale che $\forall a\in G$ vale $e\cdot a = a = a\cdot e$
	\item Per ogni $a\in G$ esiste un $a'$ tale che $a'\cdot a$ e $a\cdot a'$ sono unità, ovvero si comportano come l'elemento $e$ al punto precedente.
		  Un tale $a'$ si dice \emph{inverso} di $a$.
\end{enumerate}
Per comodità di solito si omette il puntino. Se $G$ è finito, $card(G) = n$, si dice che $G$ ha \emph{ordine} $n$.
\end{defn}

\paragraph{Esempi}
\begin{enumerate}
	\item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}$ con l'operazione di somma.
	\item $\mathbb{Q}^*, \mathbb{R}^*, \mathbb{C}^*$ con l'operazione di moltiplicazione (senza lo 0).
	\item $GL_n(\mathbb{R})$ oppure $GL(V)$
	\item $f:I\to I $ biunivoca, con $I$ insieme e con l'operazione di composizione. Nel caso in cui $I$ sia un insieme finito, tanto vale scegliere $I = \{1,2,3,\ldots, n\}$. In tal caso questo gruppo si chiama $S_n$.
\end{enumerate}

\paragraph{Alcuni teoremi elementari}
\begin{enumerate}
	\item L'unità $e$ è unica.
	
	Dimostrazione: supponiamo che $e$ ed $e'$ siano entrambe unità. Allora vale
	
	\[e = ee' = e' \]
	
	\item Dato $a\in G$, l'inverso di $a$ è unico (e usualmente si denota con $a^{-1}$).

	Dimostrazione: supponiamo che $a', a''$ siano entrambi inversi di $a$. Allora
		
	\[(a' a)a'' = a'(aa'') \implies e a'' = a' e \implies a'' = a' \]
	
	\item Dati $a_1, a_2, \ldots, a_n$, il prodotto $a_1\cdot a_2 \cdots\cdot a_n$ è ben definito senza bisogno di parentesi.
	\item Se $ab = e$, allora anche $ba = e$, dunque $a$ e $b$ sono uno l'inverso dell'altro.

	Dimostrazione: $ba = bae = babb^{-1} = beb^{-1} = bb^{-1} = e$.
	
	\item Dato un intero positivo $k$ e un elemento $a\in G$, definiamo $a^k=\underbrace{a\cdot a \cdots\cdot a}_{k\text{ volte}}$.
	Inoltre poniamo $a^0 = e$ e infine $a^{-k} = (a^{-1})^k$, così abbiamo definito le potenze con esponente in $\Z$.
	Non è difficile dimostrare che, se $k,h$ sono interi (non necessariamente positivi), valgono le usuali proprietà: 
	\[a^{k+h} = a^k \cdot a^h \quad\quad\quad (a^k)^h = a^{kh} \]
	Però non è vero in generale che $(ab)^k = a^kb^k$ (sarebbe vero se l'operazione fosse commutativa). Osserviamo infine che 
	\[ (ab)^{-1} = b^{-1}a^{-1}\]
	infatti $(ab)(b^{-1} a^{-1}) = a(bb^{-1})a^{-1} = aea^{-1} = aa^{-1} = e$.
\end{enumerate}



\begin{defn}[Sottogruppo]
Sia $G$ un gruppo, $H\subseteq G$ si dice sottogruppo di $G$ se:
\begin{itemize}
	\item $e\in H$
	\item $x,y\in H \implies xy\in H$
	\item $x\in H \implies x^{-1}\in H$
\end{itemize}
e si indica $H \leq G$. In altre parole $H$ è sottogruppo se,
ereditando l'operazione di $G$, è esso stesso un gruppo.
\end{defn}

\begin{exmp}[Sottogruppo generato da un elemento]
Sia $G$ un gruppo e $a$ un suo elemento. L'insieme delle potenze di $a$, ovvero $\{a^k | k\in\Z\}$, è un sottogruppo di $G$,
che di solito viene denotato con $\langle a\rangle$.
\end{exmp}

\begin{rem}
Se $G$ è gruppo, $a\in G$ ed esiste un intero $n>0$ tale che $a^n=e$, allora tutti gli elementi di $\langle a\rangle$ sono della forma
$a^k$ per qualche $0\le k < n$. Infatti se si considera un qualsiasi $a^s$ con $s\in\Z$, si può scrivere $s=nq+r$ con $0\le r<n$.
Allora \[a^s=a^{nq+r} = (a^n)^qa^r = e^qa^r = a^r\]
Se $n$ è il minimo intero positivo tale che $a^n=e$, allora si dice che $a$ ha \emph{ordine} $n$. In tal caso è facile verificare che
l'insieme $\langle a\rangle$ contiene esattamente $n$ elementi distinti, ovvero $a^0, a^1, \dots a^{n-1}$.
Infatti, se fosse $a^i=a^j$ con $0\le i<j<n$, allora $a^{j-i}=e$, che sarebbe assurdo siccome $0<j-i<n$.

Se $\langle a\rangle$ è finito (il che è certo se ad esempio $G$ è finito) allora di sicuro esiste $n>0$ tale che $a^n=e$. Infatti basta prendere $0\le i < j$
tali che $a^i = a^j$ e osservare che $a^{j-i} = e$. Questi $i$ e $j$ esistono per forza perché se tutte le potenze fossero distinte allora $\langle a\rangle$ sarebbe infinito.
\end{rem}


\begin{defn}[Sottogruppo normale]
Sia $G$ un gruppo, $H \leq G$ si dice \emph{normale} in $G$ se
\[
	\forall h\in H, \forall g\in G\qquad ghg^{-1}\in H
\]
e si indica $H \trianglelefteq G$.
\end{defn}

\begin{defn}[Laterale]
	Sia $G$ un gruppo e $H<G$ un suo sottogruppo, definiamo \textit{laterale destro} o \textit{classe laterale destra} di $H$
	un sottoinsieme di $G$ del tipo
	\[
		gH=\{ gh\ |\ h\in H\}
	\]
\end{defn}

\begin{defn}[Quoziente]\label{defn:quoziente}
	Sia $G$ un gruppo, $H<G$, chiamiamo \textit{quoziente} di $G$ per $H$ l'insieme delle classi laterali di $H$, che indicheremo con $G/H$, ovvero
	\[
		G/H=\{gH\ |\ g\in G\}
	\]
	dove vengono identificati gli insiemi uguali (infatti non è detto che se $g,g'\in G$, con $g\neq g'$, allora $gH\neq g'H$).
\end{defn}

\begin{rem}
	Dato $G$ un gruppo, $H<G$, non è difficile mostrare che tutte le classi laterali di $H$ in $G$ hanno la stessa cardinalità, in particolare hanno tutte la cardinalità della classe $eH$, ma $eH=H$ come insieme, quindi tutte le classi laterali di $H$ hanno la stessa cardinalità di $H$.
\end{rem}


\begin{thm}
	Il quoziente di un gruppo $G$ per un suo sottogruppo $H$ fornisce una partizione di $G$: per ogni $g\in G$ esiste un unico $H$-laterale destro $g'H$ tale che $g\in g'H$. 
\end{thm}
\begin{proof}
	Si osserva che $g\in gH$ visto che $gH=\{gh\ |\ h\in H\}$ e che $e\in H$, se si avesse che $g\in \alpha H$ allora $g=\alpha h_1$ per qualche $h_1$. Si osserva allora che i due laterali coinciderebbero:
	\[
		\alpha H=\{ \alpha h\ |\ h\in H\} = \{ \alpha h_1 h\ |\ h\in H \} = \{ gh\ |\ h\in H\} = gH
	\]
\end{proof}

\begin{thm}[Teorema di Lagrange]
	Sia $G$ un gruppo finito, $H<G$, allora $|H|$ divide $|G|$ e, in particolare, $\displaystyle |G/H|=\frac{|G|}{|H|}$; il numero $|G/H|$ viene chiamato \textit{indice} di $H$ in $G$.
\end{thm}


\begin{defn}[Gruppo quoziente]
	Sia $G$ gruppo, $H\trianglelefteq G$ (osservare che si richiede che il sottogruppo sia \textit{normale}), allora chiameremo \textit{gruppo quoziente} di $G$ su $H$ l'insieme quoziente come l'abbiamo definito \eqref{defn:quoziente} munito della seguente operazione:
	\[
		(g_1H)\cdot(g_2H)=g_1g_2H
	\]
	Non riportiamo la dimostrazione del fatto che l'operazione così definita rispetti effettivamente gli assiomi dei gruppi.
\end{defn}

\begin{defn}[Classi di coniugio]
Sia $G$ un gruppo, $x \in G$, la classe di coniugio di $x$ è l'insieme $\{ gxg^{-1} | g\in G \}$. Si dimostra facilmente che le classi di coniugio di tutti gli elementi di $G$ formano una partizione del gruppo stesso. Si osserva inoltre che un sottogruppo è normale se e solo se è unione di classi di coniugio (\textsc{Attenzione:} è raro che unendo a caso classi di coniugio si ottenga un sottogruppo).
\end{defn}


\begin{exmp}[Le classi di coniugio di $GL_n(\C)$]
Nel caso del gruppo $GL_n(\C)$ due matrici stanno nella stessa classe di coniugio se e solo se sono simili, quindi per ogni classe di coniugio esiste un rappresentante canonico che è la forma di Jordan di una qualsiasi matrice nella classe (con opportune convenzioni sull'ordine dei blocchi e degli autovalori).
\end{exmp}

\begin{defn}[Centro di un gruppo]
	Sia $G$ un gruppo, il \textit{centro} di $G$ si indica con $Z(G)$ ed è il sottoinsieme degli elementi che commutano con tutto $G$:
	\[
		Z(G)=\{ h\in G\ |\ hg=gh\ \forall g\in G \}
	\]
	\`E immediato verificare che $Z(G)$ è un sottogruppo normale di $G$.

\end{defn}

\begin{defn}[Prodotto diretto di gruppi]
Siano $G$ e $H$ gruppi. Si definisce prodotto diretto di $G$ e $H$ il gruppo formato dall'insieme $G \times H = \{ (g, h) | g \in G, h \in H\}$ con l'operazione componente per componente, ovvero separatemente per i due gruppi di partenza.
\end{defn}


\begin{defn}[Omomorfismo (isomorfismo) di gruppi]
Siano $G$ ed $H$ gruppi, un'applicazione $\varphi:G\to H$ si dice \textit{omomorfismo di gruppi} se
\[
	\forall g_1,g_2\in G\qquad \varphi(g_1 g_2)=\varphi(g_1)\varphi(g_2)
\]
dove la prima moltiplicazione è fatta in $G$ mentre la seconda in $H$.
Se $\varphi$ è bigettiva, allora si dice \textit{isomorfismo}, e i due gruppi si dicono \emph{isomorfi}.
Indichiamo con $Hom(G,H)$ l'insieme degli omomorfismi da $G$ ad $H$.
\end{defn}

\begin{defn}
	Siano $G$ e $H$ gruppi, $f:G\to H$ un omomorfismo di gruppi, allora definiamo
	\begin{gather*}
		Ker f = \{g\in G\ |\ f(g)=e_H\}\\
		Imm f =\{ h\in H\ |\ \exists g\in G\text{ t.c. }f(g)=h\}
	\end{gather*}
	Non è difficile verificare che sia $Ker f$ che $Imm f$ sono sempre sottogruppi rispettivamente di $G$ e di $H$, inoltre si può osserevare che $Ker f$ è un sottogruppo normale di $G$.
\end{defn}

\begin{rem}
	Non è difficile dimostrare che, dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, esso è \textit{iniettivo} se e solo se $Ker f = \{e\}$.
\end{rem}


\begin{thm}[Primo teorema di omomorfismo]\label{alg:primo_teo_omo}
	Dati due gruppi $G$ e $H$ e un omomorfismo $f:G\to H$, vale che
	\[
		G/Ker f \cong Imm f
	\]
\end{thm}
\begin{rem}
	Se la $f$ del teorema precedente è iniettiva, allora $G/Ker f\cong G$ e quindi $G\cong Imm f$. Invece se $f$ è surgettiva, allora $G/Ker f\cong H$.
\end{rem}



\begin{defn}[Azione di un gruppo su un insieme] Sia $G$ un gruppo e $I$ un insieme. Chiamiamo azione $a$ di $G$ su $I$ una funzione $a:G\times I \to I$ che rispetti la regola di composizione, ovvero che se $h,g\in G$ e $i \in I$, valga

\[ a(h,a(g,i)) = a(hg, i) \]

Normalmente si usa una notazione abbreviata in cui invece di scrivere $a(g,i)$ si scrive direttamente $g\cdot i$ o addirittura $gi$


\label{defn:azione}
\end{defn}


\begin{defn}[Azione transitiva]
Un'azione di un gruppo $G$ su un insieme $I\neq \emptyset$ si dice \textit{transitiva} se $\forall\ i,j\in I\ \exists s\in G$ t.c. $j=s\cdot i$.
\label{defn:azione transitiva}
\end{defn}

SAREBBE UTILE SCRIVERE UN COMANDO PER SCRIVERE ORB(X) SOLO CHE NON SO COME SI FA...
\begin{defn}[Orbita di un elemento]
Sia $G$ un gruppo che agisce sull'insieme $I$, dato $x\in I$ si chiama \textit{orbita} di $x$ in $G$ l'insieme $Orb_{G}(x)=\{ g\cdot x\ |\ g\in G \}$, se il gruppo utilizzato è chiaro si può scrivere semplicemente $Orb(x)$. Si osserva subito che un'azione è transitiva se e solo se induce una unica orbita.
\label{defn:orbita}
\end{defn}

\begin{rem} Le orbite di un gruppo $G$ sull'insieme $I$ formano una partizione dell'insieme. La verifica non è difficile. Vale inoltre la formula $|Orb_G(x)| \cdot |Stab_G(x)| = |G|$



\end{rem}



\begin{rem}
	Un gruppo $G$ può agire su se stesso per coniugio, ovvero dati $g\in G$ (qui $G$ è pensato come gruppo che agisce) e $x\in G$ ($G$ pensato come insieme),
	si pone $g\cdot x = gxg^{-1}$. Non è difficile verificare che si tratta davvero di una azione.
	Osserviamo che le classi di coniugio sono le orbite degli elementi generate mediante l'azione per conugio.
\end{rem}




\begin{defn}[Azione semplicemente transitiva]
Un'azione di $G$ su un insieme $I\neq \emptyset$ si dice \textit{semplicemente transitiva}
se presi comunque $i,j\in I$ esiste un unico $s\in G$ tale che $j=s\cdot i$.
\end{defn}


\begin{defn}[Funzione $G$ equivariante]

Dato un gruppo $G$ che agisce su due insiemi $I$ e $J$, una funzione $\phi: I \to J$ si dice $G$ equivariante se 

\[ \phi(s \cdot_I i) = s \cdot_J \phi(i) \qquad \forall s \in G, \ \ \forall i \in I \]


\end{defn}


















\newpage
\subsection{Proprietà dei gruppi ciclici}

\begin{defn}[Gruppo ciclico] Un gruppo $G$ si dice ciclico se esiste un elemento $a\in G$ tale che ogni
elemento di $G$ è una potenza di $a$, ovvero $G=\langle a\rangle$. Si dice che $a$ è un generatore di $G$.
\end{defn}

\begin{rem}
Sia $G$ un gruppo ciclico di cardinalità $n$ e generatore $a$. Allora $n$ è il più piccolo intero positivo tale che $a^n = e$,
e ogni elemento di $G$ si scrive in modo unico come $a^k$ con $0\le k < n$.
\end{rem}

\begin{exmp}[Radici dell'unità]
Dato $n>0$ intero, l'insieme $\mu_n\subset \C^*$ delle radici $n$-esime dell'unità è un gruppo ciclico con $n$ elementi.
\end{exmp}

\begin{rem} Se $n$ è un intero positivo esiste (a meno di isomorfismo) un unico gruppo ciclico di cardinalità $n$.
Abbiamo già visto che esiste (basta considerare $\mu_n$),
inoltre dati due gruppi ciclici di cardinalità $n$ e generatori rispettivamente $a$ e $b$ è immediato costruire un
isomorfismo $f:\langle a\rangle\to\langle b\rangle$ ponendo $f(a^k) = b^k$ per $0\le k < n$.
\end{rem}


\begin{prop} Sia $C_n$ un gruppo ciclico di cardinalità $n$. Allora
\[ n = card(Hom(C_n,\C^*))\]
\end{prop}
\textsc{Dimostrazione:} Sia $a$ un generatore di $C_n$. Fissato $\omega\in\mu_n$ posso definire
una funzione $f:C_n\to\C^*$ ponendo $f(a^k) = \omega^k$ per $0\le k < n$.
Verifichiamo che $f\in Hom(C_n, \C^*)$. A tal fine prendiamo due elementi di $C_n$, che sono della forma $a^k, a^h$ per certi interi $0\le k,h < n$.
\[f(a^k \cdot a^h) = f(a^{k+h}) = \omega^{k+h} = \omega^k \omega^h = f(a^k)f(a^h)\]
Dunque $f$ è omomorfismo. Variando la scelta di $\omega\in\mu_n$ si producono effettivamente $n$ omomorfismi differenti (infatti se $\omega$ cambia allora cambia anche $f(a)$).
Mostriamo che non ci sono altri omomorfismi oltre a questi.
Sia $f\in Hom(C_n,\C^*)$. Visto che $a^n=e$, deve valere $f(a)^n = f(a^n) = 1$. Allora $f(a)$ deve essere una radice $n$-esima
dell'unità, che chiamiamo $\omega$. A questo punto il fatto che $f$ è omomorfismo implica che $f(a^k) = \omega^k$ per ogni intero $k$. \qed



\subsection{Proprietà dei gruppi abeliani}
\begin{defn}[Gruppo abeliano] Un gruppo $G$ si dice abeliano se l'operazione di gruppo è commutativa, cioè $\quad\forall a,b\in G\quad ab=ba$.
\end{defn}

\begin{rem} Un gruppo ciclico è sempre abeliano.
\end{rem}

Potrebbe essere utile conoscere il seguente risultato, la cui dimostrazione richiederebbe una conoscenza più approfondita della teoria dei gruppi.
\begin{thm}Ogni gruppo abeliano finito è isomorfo al prodotto diretto di gruppi ciclici.
\end{thm}

\begin{rem} Sia $G$ un gruppo abeliano. Allora 
\[ |G| = card(Hom(G,\C^*))\]

La dimostrazione si ottiene ricordando che $G$ è prodotto diretto di gruppi ciclici e facendo un ragionamento simile a quello
della proposizione analoga per gruppi ciclici.
Se invece $G$ non è abeliano allora nella formula precedente all'uguale va sostituito un $>$.
\end{rem}




\subsection{Proprietà del gruppi simmetrici}
Il gruppo simmetrico $S_n$ è stato introdotto come l'insieme delle funzioni bigettive da $\{1,2,\dots,n\}$ in sé, dotato dell'operazione di composizione.
Dunque $S_n$ agisce in modo naturale su $\{1,2,\dots,n\}$, permutandone gli elementi. Per descrivere un elemento $\sigma \in S_n$ 
è spesso conveniente usare la notazione di prodotto di cicli disgiunti, che ora descriviamo informalmente.

Si comincia a costruire la lista $(1, \sigma(1), \sigma^2(1), \dots)$. Visto che abbiamo a disposizione un numero finito di elementi,
ad un certo punto sarà $\sigma^k(1) = 1$. Allora se scriviamo $(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ tutti i numeri tra le 
parentesi saranno diversi tra loro (questo segue dal fatto che $\sigma$ è bigettiva). Inoltre ognuno dei numeri scritti viene mandato da $\sigma$
nel numero immediatamente successivo nella lista, e l'ultimo numero viene mandato nel primo. Quello che abbiamo appena scritto è un \emph{ciclo}.
\`E anche possibile che la lista sia semplicemente $(1)$, il che vorrebbe dire che $1$ viene lasciato fisso da $\sigma$.
Se avessimo cominciato il procedimento con $\sigma(1)$ al posto di $1$ avremmo ottenuto $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$, che
descrive ugualmente bene il modo in cui $\sigma$ sposta gli elementi scritti. Anche se la scrittura è diversa, per noi
$(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ e $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$ sono esattamente lo stesso ciclo,
e un ragionamento analogo vale per gli altri numeri facenti parte della lista: non importa da quale si parte.
Può darsi che non tutti i numeri da $1$ a $n$ compaiano nel ciclo appena scritto: in tal caso si prende un numero ancora non scritto e si ricomincia da capo
da lui, creando una nuovo ciclo, e si continua così finché non sono stati scritti tutti i numeri.
Alla fine ci ritroviamo un elenco di cicli che sono necessariamente disgiunti per via della bigettività di $\sigma$.
\`E facile convincersi che in questo modo si descrive completamente $\sigma$. Inoltre a meno di variare
l'ordine con cui sono scritti i cicli e di cambiare i ``punti di partenza'' dei singoli cicli questa scrittura come cicli disgiunti è unica.
Riassumiamo quanto detto nel seguente teorema:

\begin{thm}
Ogni elemento $\sigma \in S_n$ si scrive in modo unico come prodotto di cicli disgiunti a meno dell'ordine dei fattori e a meno di 
cambiare il modo in cui i singoli cicli sono presentati.
\end{thm}

Spesso nella scrittura in cicli disgiunti si tralasciano i cicli di lunghezza uno. Ad esempio $(3,5,7)(4,1)\in S_{12}$ ha perfettamente senso:
i numeri $1,3,4,5,7$ vengono ``spostati'' da $\sigma$ nel modo descritto e tutti gli altri vengono lasciati fissi.

I cicli, più che essere delle liste, vanno pensati come elementi di $S_n$, ovvero come funzioni bigettive da $\{1,\dots,n\}$ in sé.
In quanto tali possono essere moltiplicati, nel senso di composizione delle funzioni. Ad esempio
\[(1,2,3)\cdot(3,5)\]
chiaramente non è la scrittura come prodotto di cicli disgiunti di un elemento di $S_n$, in quanto appunto i due cicli scritti
non sono disgiunti. Ma il loro prodotto ha perfettamente senso, e usando la stessa convenzione
che si usa di solito per la composizione di funzioni vanno fatti ``agire'' da destra a sinistra.
Ad esempio il prodotto scritto manda il numero $5$ nel numero $1$ (infatti il ciclo a destra manda $5$ in $3$, il quale viene mandato in $1$ dal ciclo
a sinistra). Se lo volessimo scrivere come prodotto di cicli disgiunti otterremmo:
\[(1,2,3,5)\]

\begin{thm}
Ogni elemento $\sigma \in S_n$ si può scrivere come prodotto di \emph{trasposizioni}, ovvero cicli di lunghezza $2$, non necessariamente disgiunti.
\end{thm}
\textsc{Dimostrazione: }
Considerato il teorema sulla decomposizione in cicli, basta mostrare la tesi nel caso in cui $\sigma$ è un ciclo.
Supponiamo $\sigma = (a_1, a_2, \dots, a_k)$. Allora è facile verificare che
\[\sigma = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Qualcuno potrebbe essere turbato dal caso in cui il ciclo ha lunghezza $1$, ossia $\sigma$ è l'identità.
In tal caso possiamo dire che $\sigma$ è il prodotto di un insieme vuoto di trasposizioni.
Chi fosse ancora turbato potrebbe scrivere, almeno nel caso $n\ge 2$, $\sigma = (1,2)(1,2)$.
\qed

Osserviamo che il teorema precedente assicura solo l'esistenza di una scrittura come prodotto di trasposizioni
ma non l'unicità. In effetti questa non sussiste, infatti se in fondo ad un prodotto di trasposizioni aggiungo $(1,2)(1,2)$
allora il risultato non cambia. Per avere un esempio leggermente più sofisticato:
\[(2,1)(2,3) = (1,3)(1,2)\]
Tuttavia quello che non cambia è la parità del numero di trasposizioni, come precisato dal seguente teorema.
\begin{thm}
Siano $\tau_1, \tau_2, \dots, \tau_t, \sigma_1, \sigma_2, \dots, \sigma_s$ trasposizioni in $S_n$. Supponiamo che
\[\tau_1\tau_2\dots\tau_t = \sigma_1\sigma_2\dots\sigma_s\]
Allora $s\equiv t \mod 2$.
\end{thm}
\textsc{Cenno di dimostrazione: }
Definiamo la seguente funzione $f:S_n\to\N$:
\[f(\rho) = card\left(\left\{\quad(a,b)\in\{1,\dots,n\}^2 \quad | \quad a < b, \quad\rho(a) > \rho(b) \quad\right\}\right)\]
Non è difficile verificare che se $\tau\in S_n$ è una trasposizione allora $f(\rho)$ e $f(\tau\rho)$ hanno parità diversa.
Il risultato segue immediatamente visto che $f(\tau_1\tau_2\dots\tau_t) = f(\sigma_1\sigma_2\dots\sigma_s)$.
\qed

\begin{defn}[Segno di una permutazione]
Il teorema appena visto permette di definire il \emph{segno} di ogni elemento $\sigma\in S_n$, che si pone uguale a $1$
se $\sigma$ si scrive come prodotto di un numero pari di trasposizioni, si pone uguale a $-1$ altrimenti.
\end{defn}

\begin{prop}
Il segno di un ciclo di lunghezza $k$ è esattamente $(-1)^{k-1}$
\end{prop}
\textsc{Dimostrazione: }
Abbiamo già visto un modo in cui un ciclo di lunghezza $k$ si può scrivere come prodotto di trasposizioni:
\[(a_1, a_2, \dots, a_k) = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Dunque la tesi segue immediatamente.
\qed

\begin{defn}
L'insieme degli elementi di $S_n$ aventi segno $+1$ è un sottogruppo di $S_n$,
chiamato \emph{gruppo alterno} e indicato con $A_n$.
\end{defn}


\subsection{Proprietà dei gruppi diedrali}

\begin{defn}[Gruppo diedrale]
Consideriamo in $\R^2$ un poligono regolare di $n$ lati con centro nell'origine.
L'insieme $D_n$ delle isometrie di $\R^2$ che mandano il poligono in sé è un gruppo con l'operazione di composizione.
Si verifica che questo gruppo ha $2n$ elementi, di cui $n$ rotazioni (ovvero elementi di $O_2(\R)$ con determinante $1$)
e $n$ riflessioni (ovvero elementi di $O_2(\R)$ con determinante $-1$).
Inoltre, detta $\rho$ una rotazione di $2\pi/n$ (che ha ordine $n$, e per inverso ha $\rho^{n-1}$) e $\sigma$ una qualunque riflessione (che ha ordine $2$), esse generano
il gruppo $D_n$, che si può presentare nel seguente modo: $$D_n=\langle\rho,\sigma|\rho^n=\sigma^2=id,\ \sigma\rho\sigma=\rho^{-1}\rangle$$
\end{defn}

\begin{rem}
 Le $n$ potenze distinte di $\rho$ sono tutte e sole le rotazioni di $D_n$, mentre gli elementi della forma $\sigma\rho^{i},\ i=0,1,..,n-1$ 
 sono tutte e sole le riflessioni. 
\end{rem}

\begin{rem}
 Si dimostra facilmente che la relazione $\sigma\rho\sigma=\rho^{-1}$ è verificata da qualsiasi rotazione $\rho$
 e qualsiasi riflessione $\sigma$.
\end{rem}

\begin{defn}[Gruppo diedrale]
L'insieme $D_n$ delle rotazioni e simmetrie di un poligono regolare di $n$ lati è un gruppo con l'operazione di composizione.
Detta $\rho$ una rotazione di $2\pi/n$ (che ha ordine $n$, e per inverso ha $\rho^{n}$) e $\sigma$ una qualunque riflessione (che ha ordine $2$), esse generano
il gruppo $D_n$, che si può quindi presentare nel seguente modo: $$D_n=\langle\rho,\sigma|\rho^n=\sigma^2=id,\ \sigma\rho\sigma=\rho^{-1}\rangle$$
\end{defn}









\newpage
\section{Algebra lineare}
In questa sezione diamo alcune definizioni e teoremi di algebra lineare che sono stati utilizzati nel corso o che sono utili per avere una visione d'insieme di certi argomenti. Non saranno presenti le dimostrazioni che possono essere trovate su molti libri di algebra lineare.
\begin{thm}[Diagonalizzazione simultanea]
\label{thm:diag_sim}
	Date due matrici $M, N\in \mathcal{M}(n,n,\K)$, diremo che sono \textit{simultaneamente diagonalizzabili} se esiste una base comune di autovettori per entrambe.\\
	Date $M, N\in \mathcal{M}(n,n,\K)$, se esse commutano e sono entrambe diagonalizzabili allora sono simultaneamente diagonalizzabili.
\end{thm}
\begin{cor}
	Date $M_1,\ldots,M_k \in \mathcal{M}(n,n,\K)$, se $M_iM_j=M_jM_i\ \forall\ i, j$ e ogni $M_i$ è diagonalizzabile, allora esiste una base comune di autovettori per tutte quante.
\end{cor}


\begin{defn}[Ideale di un endomorfismo]
	Se $p(x)=a_n x^n+\ldots+a_0$, allora scriviamo $p(f)$ per intendere $a_nf^n+\ldots+a_0f^0$ dove $f^0=Id$ e $f^k=\underbrace{f\circ\ldots\circ f}_{k \text{ volte}}$.\\
	Sia $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$. Definiamo \textit{ideale di $f$} l'insieme
	\[
		I(f)=\left\{ p(x)\in \K[x]\ |\ p(f)=0 \right\}
	\]
	

\end{defn}


\begin{thm}[Teorema di decomposizione primaria]
\label{thm:dec_primaria}
	Siano $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$ e $q(x)\in I(f)$. Sia $q=q_1\cdot\ldots\cdot q_k$ tale che $MCD(q_i,q_j)=1\ \forall\ i\neq j$, allora $V=Ker(q_1(f))\oplus\dots\oplus Ker(q_k(f))$ e gli addendi sono $f$-invarianti.\\
	In particolare se $f$ è triangolabile e $\lambda_1,\ldots,\lambda_k$ sono gli autovalori di $f$ con molteplicità algebrica rispettivamente $\alpha_1,\ldots,\alpha_k$, allora $V=Ker\left((f-\lambda_1 Id)^{\alpha_1}\right)\oplus\dots\oplus Ker\left((f-\lambda_k Id)^{\alpha_k}\right)$.
\end{thm}

\begin{thm}[Forma canonica di Jordan]
	Sia $M\in \mathcal{M}(n,n,\K)$ una matrice triangolabile, siano $\lambda_1,\ldots,\lambda_k$ i suoi autovalori, allora $M$ è simile alla sua \textit{forma canonica di Jordan} che è nella forma
	\begin{align*}
		&\begin{pmatrix}
			J_1 & & \\
			& \ddots & \\
			& & J_t
		\end{pmatrix}		
		&\text{ dove }J_i=\begin{pmatrix}
		                  	\lambda & 1 & & & \\
							& \lambda & 1 & & \\
		                  	& & \ddots & \ddots & \\
		                  	& & & \ddots & 1 & \\
		                  	& & & & \lambda
		                  \end{pmatrix} \text{ per qualche }\lambda \in \left\{ \lambda_1,\ldots,\lambda_k \right\}
	\end{align*}
	La dimensione e il numero di blocchi di ciascun tipo sono univocamente determinati dalla matrice $M$, ne segue che la forma canonica di Jordan è unica a meno di permutazione dei blocchi e dunque, scelta una convenzione sull'ordine dei blocchi, essa è un sistema completo di invarianti per similitudine: due matrici sono simili se e solo se hanno la stessa forma di Jordan.

\end{thm}
\begin{defn}[Forma hermitiana]
Siano $V,W$ due $\C$-spazi vettoriali, una funzione $h:V\times V\to W$ si dice \textit{forma hermitiana} se $\forall v,w,z\in V,\ \forall \alpha \in \C$ vale che
\begin{gather*}
	h(v,w) = \overline{h(w,v)}\\
	h(\alpha v, w) = \alpha h(v,w)\\
	h(v+z,w)=h(v,w)+h(z,w)
\end{gather*}
\end{defn}

\begin{defn}
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{definita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V, v\neq 0$ si ha che $\phi(v,v)>0$ (rispettivamente $\phi(v,v)<0$), ossarvare che $\phi(v,v)\in \R\ \forall\ v\in V$, dunque ha senso chiedere che sia maggiore o minore di $0$.\newline
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{semidefinita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V$ si ha che $\phi(v,v)\geq 0$ (rispettivamente $\phi(v,v)\leq 0$)
\end{defn}

\begin{thm}
	Ogni forma hermitiana definita positiva su uno spazio vettoriale $V$ di dimensione finita ammette una \textit{base ortonormale}, ovvero esiste una base $\{v_1,\ldots,v_n\}$ di $V$ tale che $\phi(v_i,v_j)=\delta_{ij}$. 
\end{thm}




\newpage
\section{Algebra multilineare}
\subsection{Alcune generalizzazioni di algebra lineare}

\begin{defn}[Base di uno spazio vettoriale]
Sia $V$ un $\K-$spazio vettoriale e $I$ un insieme; una base di $V$ è una funzione $e: I \to V$ tale che 
per ogni $v \in V$ esiste un'unica funzione $a: I \to \K$ a supporto finito per cui vale $v=\sum_{i\in I}a_i e_i$.
Questa definizione è compatibile con la definizione di base come insieme di vettori generatori linearmente indipendenti.
\end{defn}


\begin{lemma}
Siano $V,W$ dei $\K-$spazi vettoriali, sia $e:I\to V$ una base di $V$ e $f: I \to W$ una funzione. Allora $\exists!\  \phi: V \to W$ lineare tale che
\[\phi(e_i) = f_i \]
Inoltre $\phi$ è un isomorfismo $\Leftrightarrow$ $f$ è una base.
\end{lemma}

\begin{lemma}
Dato $I$ insieme, esiste uno spazio vettoriale $V$ con base una certa $e:I\to V$.
\end{lemma}
\begin{proof}
Definisco il seguente insieme, che è in modo naturale un $\C-$spazio vettoriale:
\[ \C^I = \{ v:I\to\C \quad|\quad v \text{ ha supporto finito}\}\]
Ora è facile osservare che $e:I\to\C^I$ definita da $e_i(j) = \delta_{i,j}$ è una base.
\end{proof}


\subsection{Prodotto tensoriale}

\begin{defn}[Prodotto tensoriale]
   Siano $V, W$ due $\mathbb{C}$-spazi vettoriali. Si dice prodotto tensore di $V$ e $W$, 
   e si indica come $V\otimes W$, uno spazio vettoriale con una funzione bilineare 
   $\otimes: V \times W \to V\otimes W$ tale che per ogni data funzione bilineare $h: V\times W \to  Z$,
   esiste unica $\phi: V\otimes W \to Z$ lineare per cui $\phi(v \otimes w)=h(v,w)$. Ovvero questa $\phi$ fa commutare il diagramma:
   \tridiag{V\times W}{ \otimes }{V \otimes W}{\phi}{Z}{h}
   Questa proprietà viene detta proprietà universale del prodotto tensoriale e la funzione $\otimes: V \times W \to V\otimes W$
   viene detta funzione universale.
\label{defn:prodotto tensoriale}
\end{defn}


\begin{prop}
Se ho due prodotti tensoriali $V \otimes W$ e $V \overline{\otimes} W$, allora esiste un unico isomorfismo 
$\phi: V \otimes W \to V \overline{\otimes} W$ tale che

\[ \phi (v\otimes w) = v \overline{\otimes} w\]
\end{prop}


\begin{note}
\`E importante notare che non tutti gli elementi $z \in V \otimes W$ si scrivono come $z = v \otimes w$. In particolare, per fare un esempio concreto che mostra che questa cosa non funziona, prendiamo $W = V^*$. Vedremo fra poco che $V\otimes V^*$ è canonicamente isomorfo allo spazio delle applicazioni bilineari da $V$ in $\C$, che sappiamo scriverlo come matrici $n\times n$. Tuttavia se un elemento si scrive in termini di matrici come $z = v\otimes w$, allora la matrice associata a $z$ in una base avrà rango al massimo 1, ben lontano da coprire tutto lo spazio.
\end{note}


\begin{prop}
L'insieme degli elementi di $V\otimes W$ della forma $v\otimes w$ con $v\in V, w\in W$ generano tutto lo spazio $V\otimes W$.
\end{prop}


\begin{defn}[Prodotto tensoriale di mappe lineari]
Date $f:V \to V'$ e $g:W \to W'$ funzioni lineari, si definisce prodotto tensoriale tra $f$ e $g$ la funzione lineare $f \otimes g : V \otimes W \to V' \otimes W'$ tale che $(f \otimes g)(v \otimes w)=f(v) \otimes g(w)$ $\forall v\in V, w\in W$

\end{defn}

\begin{rem}

\[ id_V \otimes id_W = id_{V\otimes W}\]
\end{rem}




\begin{prop}

Se $e_i$ è una base di $V$ e $f_i$ è una base di $W$ allora $e_i \otimes f_j$ è una base di $V \otimes W$
\end{prop}


\begin{cor}
\[dim(V \otimes W) = dim V \cdot dim W \]

\end{cor}

















\begin{defn}


% CHE ACCIDENTI VUOLE DIRE QUELLO CHE C'È SCRITTO QUI SOTTO??

% La \emph{traccia} di un elemento del prodotto tensoriale, ovvero 
% \[ tr(f\otimes g)\], 
% è l'unico funzionale lineare $tr: V \otimes W \to \K$ che soddisfa le proprietà:
% \begin{enumerate}
% \item $tr[ (v_1 \otimes w_1) (v_2 \otimes w_2) ] = tr[ (v_2 \otimes w_2) (v_1 \otimes w_1) ] $ 
% \item $tr[ (id_V \otimes id_W)  ]  = \mbox{dim} V \cdot \mbox{dim} W $
% \end{enumerate}


La \emph{traccia} viene definita come l'unica funzione $tr: End(V\otimes W) \to \K$, dove $V$ e $W$ sono spazi vettoriali su $\K$, tale che 


\begin{enumerate}
\item{ $tr\left( f \otimes g \circ h \otimes k   \right) = tr\left( h \otimes k \circ f \otimes g \right) \qquad \forall f, h \in End(V), \ \forall g, k \in End(W)$}
\item{ $tr\left( id_V \otimes id_W \right) = dimV \cdot dimW$}
\end{enumerate}







\end{defn}


\begin{thm}
Se $f:V\to V$ e $g:W\to W$ sono endomorfismi di spazi vettoriali, allora vale la formula

\[tr(f\otimes g) = tr(f) tr(g)  \]

\end{thm}

\textsc{Dimostrazione:} L'applicazione $\phi: V \otimes W \to \K$ che manda $(f\otimes g)$ in $tr(f) tr(g)$ è bilineare, e quindi corrisponde a un funzionale $\Phi: V \otimes W \to \K$ lineare. Bisogna verificare che soddisfi le due proprietà della definizione.  È evidente che $\Phi[ (id_V \otimes id_W)  ]  $ 


\subsection{Prodotto esterno e prodotto simmetrico}

\begin{defn}[Applicazione $r$-lineare simmetrica/alternante]
 Una applicazione $\phi: V^n \to Z$ si dice $r$-lineare se è lineare in ogni componente dopo aver fissato le altre $n-1$.

 Inoltre $\phi$ si dice simmetrica se $\phi(v_{s(1)},\ldots,v_{s(n)})=\phi(v_1,\ldots,v_n),\ \forall s \in S_n$, mentre si dice 
 alternante se $\phi(v_{s(1)},\ldots,v_{s(n)})=\mathrm{sgn}(s)\phi(v_1,\ldots,v_n),\ \forall s \in S_n$.
 
\end{defn}

\begin{prop}
 Un'applicazione $h: V^n \to W$ è alternante se e solo se $h(v_1,\ldots,v_n)=0$ se $v_i=v_j$ per qualche $i\neq j$.
 \end{prop}

 \begin{prop}
  Un'applicazione $h: V^n \to W$ è nulla se i vettori $v_1,\ldots,v_n$ sono linearmente dipendenti.
 \end{prop}

\begin{defn}[Prodotto esterno]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto esterno è uno spazio vettoriale indicato con $\bigwedge^n V$
dotato di una funzione $n$-lineare alternante $\wedge: V^n \to \bigwedge^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1\wedge v_2\wedge\ldots\wedge v_n \in \bigwedge^n V$, tale che $\forall h: V^n \to Z$ $n$-lineare alternante, 
esiste unica $\phi: \bigwedge^n V \to Z $ lineare per cui vale $\phi(v_1\wedge v_2\wedge \ldots \wedge v_n)=h(v_1,\ldots,v_n)$.

\label{defn:prodotto esterno}
\end{defn}





\begin{thm}[Dimensione del prodotto esterno]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i| 1 \leq i \leq n\}$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme $E=\{e_{i_1} \wedge e_{i_2}\wedge \ldots \wedge e_{i_k}| 1 \leq i_1 < i_2 <\ldots< i_k \leq n\}$ è una base di $\bigwedge^k V$ 
e si ha $|E|= \binom {n}{k}$.

\label{thm:prodotto esterno}
\end{thm}


MANCANO UN SACCO DI PROPRIETA' E LE DIMOSTRAZIONI





\begin{defn}[Prodotto simmetrico]

Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto simmetrico è uno spazio vettoriale indicato con $S^n V$
dotato di una funzione $n$-lineare simmetrica $V^n \to \bigwedge^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1 v_2\ldots v_n \in S^n V$, tale che $\forall h: V^n \to Z$ $n$-lineare simmetrica, 
esiste unica $\phi: S^n V \to Z $ lineare per cui vale $\phi(v_1 v_2 \ldots v_n)=h(v_1,\ldots,v_n)$.

\label{defn:prodotto simmetrico}
\end{defn}





\begin{thm}[Dimensione del prodotto simmetrico]

Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i| 1 \leq i \leq n\}$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme $E=\{e_{i_1} \wedge e_{i_2}\wedge\ldots \wedge e_{i_k}| 1 \leq i_1 \leq i_2 \leq\ldots\leq i_k \leq n\}$ è una base di $S^k V$ 
e si ha $|E|= \binom {n+k-1}{k}$.

\label{thm:prodotto simmetrico}
\end{thm}


\begin{defn}[Potenza simmetrica e potenza esterna di un'applicazione lineare]

Sia $f: V\to V$ un endomorfismo di uno spazio vettoriale. Definiamo 

\[\bigwedge^k f : \bigwedge^k V \to \bigwedge^k V | f(v_1 \wedge \ldots \wedge v_n) = f(v_1) \wedge \ldots \wedge f(v_n) \]

In modo analogo si definisce la potenza simmetrica.


\end{defn}




\begin{prop}
Sia $f: V \to V$ un endomorfismo di uno spazio vettoriale. Allora vale

\[ 
\begin{cases}
tr(\bigwedge^2 f ) = \dfrac{(tr(f))^2 - tr(f^2)}{2} \\
tr(S^2 f ) = \dfrac{(tr(f))^2 + tr(f^2)}{2} \\
\end{cases}
\]



\end{prop}
















\newpage
\section{Prime proprietà delle rappresentazioni}

\begin{defn}[Rappresentazione] Sia $G$ un gruppo. Una rappresentazione $\rho$ di $G$ è una coppia composta da uno spazio vettoriale di dimensione qualsiasi $V_\rho$ e una funzione $\rho: G \to GL(V_\rho)$ che manda ciascun elemento del gruppo in un'applicazione lineare di $V_\rho$, ovvero un suo endomorfismo. Affinché $\rho$ sia una rappresentazione deve essere un omomorfismo di gruppi, ovvero in parole semplici deve rispettare la regola di composizione. In formule, se $s, t \in G$ deve valere

\[ \rho(st) v = \rho(s)\rho(t) v \qquad \forall v \in V_\rho, \quad \forall s,t \in G\]

La dimensione di $V_\rho$ viene detta grado della rappresentazione.

\end{defn}

\begin{prop} $\rho(G)$ è evidentemente un sottogruppo di $GL(V_\rho)$, quindi esistono sempre inversi, potenze e tutte le cose che valgono per i gruppi.

\end{prop}


\textbf{Esempi.}
\begin{enumerate}
	\item La rappresentazione banale, di grado qualsiasi, indicata con $\rho_1$ che manda qualsiasi elemento di $g$ nell'identità di $V_\rho$, ovvero
	
	\[ \rho(s ) = id_{V_\rho} \qquad \forall s \in G\]
	\item Dato $S_n$, il segno di un elemento $s\in S_n$ è una rappresentazione di grado 1. Infatti si ha $sgn(st) = sgn(s) sgn(t)$.
	\item L'azione naturale di $S_n$ sui vettori della base. Prendiamo quindi $G = S_n$ e uno spazio vettoriale di dimensione $n$, che sarà sicuramente isomorfo a $\C^n$. Prendiamo la base canonica di $\C^n$ e la chiamiamo $e_i$. Descriviamo la rappresentazione $\rho: S_n \to GL(\C^n)$ dicendo cosa fa agli elementi della base. Per linearità si estenderà a tutto lo spazio.
	
	\[ \rho(s) e_i = e_{s(i)}\]
	
	Notare che in questo caso $deg(\rho) = n$. Notiamo inoltre che se rappresentiamo nella base canonica le matrici associate a $\rho(s)$ queste matrici sono unitarie. Inoltre, ogni colonna (e anche ogni riga) contiene esattamente un 1 e tutti gli altri sono 0.
	
	Prendiamo come esempio $S_3$ e vediamo cosa succede. Notiamo innanzitutto che $ |S_3| = 3! = 6$
	FINISCI DI SCRIVERE
\end{enumerate}








\begin{prop}
Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\forall g \in G$ la matrice $\rho(g)$ ammette una base di autovettori in $V_\rho$, ovvero è diagonalizzabile. Inoltre, tutti gli autovalori di $\rho(g)$ sono radici $n-$esime dell'unità.

\textsc{Nota bene:} Per ogni matrice in generale la base è diversa, quindi le varie matrici in generale \textbf{non} sono simultaneamente diagonalizzabili.
Però se $G$ è abeliano tutte le matrici $\rho(s)$ sono simultaneamente diagonalizzabili. 

\label{prop:diagonalizzabilita rappresentazioni}
\end{prop}

\textsc{Dimostrazione:} Se $G$ è un gruppo finito, allora esiste un intero positivo $k$ tale che $g^k = e$. Dato che $\rho:G\to GL(V_\rho)$ mantiene queste proprietà in quanto omomorfismo, dovrà essere

\[ \rho(g)^k = id\]

Visto che il polinomio minimo di $\rho(g)$ non ha radici multiple, con il teorema di decomposizione primaria \eqref{thm:dec_primaria} si mostra facilmente che $\rho(g)$ è diagonalizzabile. Inoltre da questa formula è anche evidente che tutti gli autovalori di $\rho(g)$ hanno modulo $1$ e in particolare saranno radici $k-$esime dell'unità.

Ricordiamo un teorema di algebra lineare per mostrare che se $G$ è abeliano allora
tutte le matrici $\rho(g)$ sono simultaneamente diagonalizzabili:
due endomorfismi di uno spazio vettoriale diagonalizzabili sono simultaneamente diagonalizzabili se e solo se commutano tra loro.
\qed




\begin{defn}[Omomorfismo di rappresentazioni]
Siano $\rho$ e $\sigma$ due rappresentazioni di $G$ su $V_{\rho}$ e $V_{\sigma}$ rispettivamente. Un omomorfismo di spazi vettorali $\varphi:V_{\rho}\to V_{\sigma}$ si dice \textit{omomorfismo di rappresentazioni} se
\[
	\forall\ a\in G, \forall\ v\in V_{\rho}\quad \varphi(\rho(a)(v)) = \sigma(a)(\varphi(v))
\]
oppure equivalentemente
\[
	\forall\ a\in G\quad \varphi\circ \rho(a) = \sigma(a)\circ \varphi
\]


%Siano $G, H$ due gruppi e $\rho: G \to V_\rho$ e $\sigma: H \to V_\sigma$ due loro rappresentazioni. Una funzione lineare da $V_\rho \to V_\sigma$ \footnote{Ovvero un omomorfismo da $V_\rho$ a $V_\sigma$} è un omomorfismo di rappresentazioni se rispetta la regola di composizione


%\[ \qquad \forall v,w \in V_\rho, V_\sigma\]


\end{defn}



\begin{defn}[Rappresentazioni isomorfe]
Due rappresentazioni si dicono \textit{isomorfe} se esiste un omomorfismo di rappresentazioni tra di loro che è anche bigettivo.
\end{defn}




\paragraph{Rappresentazioni di grado 1}
Dato un gruppo $G$, le sue rappresentazioni di grado $1$ sono per definizione
omomorfismi che vanno da $G$ all'insieme degli isomorfismi di $\C-$spazi vettoriali di dimensione $1$.
Senza perdere di generalità possiamo supporre che lo spazio vettoriale sia proprio $\C$. Dunque le
rappresentazioni di grado $1$ non sono altro che omomorfismi $G\to\C^*$.
\begin{thm}
Gli omomorfismi $G\to\C^*$ sono rappresentazioni di $G$ tra loro non isomorfe.
\end{thm}
\begin{proof}
Siano $\rho:G\to\C^*$ e $\sigma:G\to\C^*$ rappresentazioni isomorfe. Allora
esiste $\varphi:\C\to\C$ isomorfismo di $\C$ (ovvero $\varphi\in\C^*$) tale che per ogni $g\in G$ vale $\varphi \rho(g) = \sigma(g) \varphi$.
Visto che $\C^*$ è commutativo abbiamo allora che $\rho(g) = \sigma(g)$ per ogni $g\in G$, il che vuol dire che 
in realtà $\rho$ e $\sigma$ sono proprio la stessa rappresentazione.
\end{proof}

Negli esercizi sarà necessario trovare le possibili rappresentazioni di un gruppo $G$ (che qui supporremo finito), un buon punto di partenza è cercare per prima cosa le rappresentazioni di grado 1. Per fare questo c'è un metodo generale (indicheremo con $\rho$ la rappresentazione cercata e con $\mu_m$ il sottoinsieme di $\C$ che contiene le radici $m$-esime dell'unità):\footnote{stiamo cercando un omomorfismo di gruppi da $G$ a $GL(\C)=\C^*$}:
\begin{enumerate}
	\item cercare i generatori del gruppo $G$, che indicheremo con $g_1, \ldots, g_k$
	\item per ogni generatore $g_i$ trovare il suo ordine $n_i$ (ovvero il minimo intero $n_i$ tale che $g_i^{n_i}=e$)
	\item imporre che $\rho(g_i)\in \mu_{n_i}$ per ogni $i=1,\ldots,k$
	\item imporre infine che $\rho:G\to GL(\C)$ sia veramente un omomorfismo di gruppi (per ora abbiamo solo posto delle condizioni necessarie), per fare ciò bisogna controllare che le \textit{relazioni} con cui può essere presentato il gruppo siano rispettare nell'immagine\footnote{questo in parole povere significa che tutte le regole con cui vengono moltiplicati gli elementi devono essere rispettate, per dare una spiegazione formale servirebbero i prodotti liberi che però non sono necessari per questo corso}.
\end{enumerate}
Questo definisce un omomorfismo da $G$ a $\C^*$: dato $h\in G$ t.c. $h=g_{i_1}^{a_1}\cdots g_{i_t}^{a_t}$, allora $\rho(h) = \rho(g_{i_1})^{a_1}\cdots \rho(g_{i_t})^{a_t}$.
\begin{rem}
	Non è detto che tutte le rappresentazioni che si ottengono siano non isomorfe, questo metodo solamente le produce tutte.
\end{rem}


\begin{exmp}[Rappresentazioni di grado 1 di $C_n$]
Dato $G=C_n$, prendiamo un suo generatore $g$ (un qualsiasi elemento di ordine $n$), imponiamo che $\rho(g)\in \mu_n$ (ovvero $\rho(g)=e^{2k\pi i/n}$ per qualche $k\in \{0,\ldots,n-1\}$). Visto che $\forall h\in G\ \exists k\in \N$ t.c. $h=g^k$, ho già definito $\rho$ su ogni elemento di $G$.\newline
Ora abbiamo definito tutte le rappresentazioni di $C_n$ di grado 1, ma sono tutte distinte? In effetti mostriamo che in questo case il processo che abbiamo operato ha prodotto tutte rappresentazioni non isomorfe. Supponiamo di avere due rappresentazioni $\rho_1, \rho_2$ trovate con il metodo descritto sopra, supponiamo inoltre di avere $\varphi:\rho_1\to\rho_2$ un isomorfismo di rappresentazioni, vediamo che questo è assurdo: se fosse un isomorfismo di rappresentazioni dovrebbe valere che $\forall x\in \C, \forall g\in G$ $\varphi( \rho_1(g)x ) = \rho_2(g)( \varphi(x) )$, ma $\rho_1(g)$ è la moltiplicazione per uno scalare e $\varphi$ è lineare, quindi vorrebbe dire che $\rho_1(g)\varphi( x ) = \rho_2(g)( \varphi(x) )$, preso $x\neq 0$ si ottiene che $\rho_1(g)=\rho_2(g)\forall g\in G$, e questo è assurdo perchè differiscono almeno su un generatore di $G$.
\end{exmp}

L'esempio di prima era particolarmente semplice quindi non abbiamo dovuto faticare troppo, però il procedimento descritto è abbastanza laborioso con gruppi più complicati, vediamo un altro metodo che, conoscendo un quoziente abeliano di $G$, fornisce alcune rappresentazioni di grado 1 del gruppo.
\subparagraph{}
Sia $G$ un gruppo, $H\trianglelefteq G$ t.c. $G/H$ sia abeliano, allora trovando le rappresentazioni di $G/H$ di grado 1 siamo capaci di ricostruire delle rappresentazioni di grado 1 di $G$: consideriamo $\pi:G\to G/H$ la proiezione al quoziente, ovvero $\pi(g)=gH$, e un omomorfismo di gruppi $\rho:G/H\to \C^*$ (la rappresentazione di $G/H$), allora come omomorfismo $\sigma:G\to GL(\C)$ (ovvero una rappresentazione di $G$) prendiamo l'omomorfismo che fa commutare il seguente diagramma:
\tridiag G \pi {G/H} \rho {GL(\C)} \sigma
ovvero $\sigma(g)=\rho(\pi(g))$.

Questo metodo è particolarmente potente perché, per ogni rappresentazione $\sigma:G\to GL(\C)$, l'immagine è un gruppo abeliano, quindi, in virtù del primo teorema di omomorfismo\eqref{alg:primo_teo_omo}, l'immagine di una qualsiasi rappresentazione $\sigma$ di grado 1 è un quoziente abeliano di $G$.
Questo implica che in realtà il metodo esposto è in grado di trovare \textbf{tutte} le rappresentazioni di $G$ di grado $1$.


\begin{exmp}[Rappresentazioni di grado 1 di $S_3$]

\end{exmp}

\begin{exmp}[Rappresentazioni di grado 1 di $C_n \times C_n$]


(generalizzazione a prodotto di $C_{n_i}$)
\end{exmp}














\newpage
\subsection{Operazioni con le rappresentazioni}

\begin{defn}[Somma di rappresentazioni]
  Date due diverse rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \ \sigma: G \to GL(V_\sigma)$ si può definire la rappresentazione somma $\rho + \sigma$ definita sullo spazio vettoriale $V_\rho \oplus V_\sigma$ definita in modo ovvio

  \[ (\rho + \sigma)(g) v = \rho(g) \pi_{V_\rho}(v) + \sigma(g) \pi_{V_\sigma}(v) \qquad \forall v \in V_\rho \oplus V_\sigma \]

  Questa definizione ha senso, infatti proiettando $v$ sui due spazi di partenza le due rappresentazioni sono definite e rimangono nello spazio di partenza. Si può poi fare la somma se riportiamo il tutto nello spazio più grande.
\label{defn:somma di rappresentazioni}
\end{defn}

\textsc{Osservazioni:}

\begin{enumerate}
\item $\rho + \sigma \cong \sigma + \rho$
\item $\rho + (\sigma + \tau) \cong (\rho + \sigma ) + \tau$
\item Esiste l'elemento neutro che è la rappresentazione di grado 0 ma non esiste l'inverso.

\end{enumerate}





\begin{defn}[Prodotto di rappresentazioni]
  Date due rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \sigma: G \to GL(V_\sigma)$ possiamo definire il prodotto di rappresentazioni che si indica con $\rho \otimes \sigma$  ma anche con $\rho\sigma$\footnote{Quest'ultima notazione può portare a confusione in quanto può essere scambiata con la composizione se le due rappresentazioni sono definite sullo stesso spazio, quindi cercheremo di evitarla.} definita sullo spazio $V_\rho \otimes V_\sigma$ tale che

  \[ \rho \otimes \sigma(g) (v \otimes w) = \rho(g) v \otimes \sigma(g) w \qquad \forall v \in V_\rho, w \in V_\sigma\]

  Non è restrittivo dare la definizione solo per gli elementi di $V_\rho \otimes V_\sigma$ decomponibili, in quanto sappiamo che sono una base dello spazio. Gli altri si otterranno per linearità.
  
\label{defn:prodotto di rappresentazioni}
\end{defn}


\textsc{Osservazioni:}


\begin{enumerate}
\item $1\otimes \rho \cong \rho$
\item $\rho \otimes \sigma \cong \sigma \otimes \rho$
\item $0 \otimes \rho \cong 0$
\item $\rho \otimes (\sigma \otimes \tau) \cong (\rho \otimes \sigma)\otimes \tau$
\item $\rho \otimes (\sigma_1 + \sigma_2) \cong \rho \otimes \sigma_1 + \rho \otimes \sigma_2$

\end{enumerate}





\begin{defn}[Rappresentazione duale]
Sia $\rho$ una rappresentazione di $G$ su $V_\rho$. Allora la rappresentazione duale $\rho^*$ è la rappresentazione di $G$ su $V_\rho ^*$ tale che $\rho^*(s)=\rho(s^{-1})^t$
\label{defn:rappresentazione duale}
\end{defn}

\begin{note}
$\rho^*(s)=\rho(s^{-1})^t=\left(\rho(s)^{-1}\right)^t=\left(\rho(s)^t\right)^{-1}$. Inoltre, notare che la presenza di inverso e trasposto fa in modo che $\rho^*(s)$ sia una rappresentazione.
\end{note}


\textsc{Osservazione:} vale
\[ (\rho + \sigma)^* \cong \rho^* + \sigma^* \]
\begin{proof}
Consideriamo la funzione $\Theta : (V_\rho \oplus V_\sigma)^*\to V_\rho ^* \oplus V_\sigma ^*$ definita da
\[ \Theta(f) = (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) \]
per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$, dove $\imath_{V_\rho}$ e $\imath_{V_\rho}$ sono le immersioni di $V_\rho$ e $V_\sigma$ dentro la loro somma diretta.
\`E facile osservare che si tratta di un'applicazione lineare. Consideriamo anche
la funzione $\Xi: V_\rho ^* \oplus V_\sigma ^* \to (V_\rho \oplus V_\sigma)^*$ definita da
\[ \Xi(h,k) = h\circ\pi_{V_\rho} + k\circ\pi_{V_\sigma} \]
per ogni coppia di funzionali $h\in V_\rho ^*$, $k\in V_\sigma ^*$,
dove $\pi_{V_\rho}$ e $\pi_{V_\sigma}$ indicano come al solito le proiezioni sui sottospazi. 
Anche $\Xi$ è lineare, inoltre è facile osservare che $\Theta$ e $\Xi$ sono una l'inversa dell'altra. Dunque sono degli 
isomorfismi. Resta da mostrare che $\Theta$ è omomorfismo di rappresentazioni. Prendiamo $g\in G$: dobbiamo mostrare che 
\[ (\rho^*+\sigma^*)(g) \circ \Theta = \Theta \circ (\rho + \sigma)^*(g) \]
ovvero che per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$ vale
\[ [(\rho^*+\sigma^*)(g)] (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che si riscrive:
\[ ([\rho^*(g)](f\circ \imath_{V_\rho}), [\sigma^*(g)](f\circ \imath_{V_\sigma})) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che è equivalente a:
\[ (f\circ \imath_{V_\rho} \circ [\rho(g^{-1})], f\circ \imath_{V_\sigma}\circ[\sigma(g^{-1})]) = (f\circ [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho} , f\circ[(\rho+\sigma)(g^{-1})]\circ \imath_{V_\sigma}) \]
Per ottenere la tesi basta osservare che valgono
\[ \imath_{V_\rho} \circ [\rho(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho}\]
\[\imath_{V_\sigma} \circ [\sigma(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\sigma}\]
Notiamo infine che l'isomorfismo trovato è canonico, ovvero non dipende da alcuna scelta delle basi.
\end{proof}





\begin{defn}[Rappresentazione regolare]
  Consideriamo un gruppo $G$, per semplicità finito, e consideriamo uno spazio vettoriale $V_\rho$ di dimensione $|G|$ su $\C$. Una base di questo spazio ha sicuramente dimensione $|G|$. Possiamo indicare gli elementi della base con $e_g, \ \ \forall g \in G$. Un generico vettore di questo spazio si scrive quindi come

  \[ v = \dsum_{g \in G} a_g e_g \]

  Dove $a_g$ sono dei numeri complessi. Possiamo definire una rappresentazione di $G$ su questo spazio in questo modo:

  \[ \rho(h)v = \rho(h) \dsum_{g \in G} a_g e_g = \dsum_{g\in G} a_g \rho(h) e_g := \dsum_{g \in G} a_g e_{gh} \]

  Notare che questa definizione ha senso in quanto essendo $G$ un gruppo, $gh \in G$ e quindi sicuramente $e_{gh}$ è un elemento della base. Questa particolare rappresentazione di $G$ si chiama \emph{rappresentazione regolare} di $G$


  
  \label{defn:rappresentazione regolare}
\end{defn}

\begin{exmp}[La rappresentazione regolare di $S_3$]


\end{exmp}


\begin{thm}

\[\mathcal{R}_G \cong \dsum_i deg(\rho_i) \rho_i \]

\end{thm}



\subsection{Sottospazi invarianti e scomposizione delle rappresentazioni}



\begin{defn}[Sottorappresentazione]
Sia $\rho$ una rappresentazione di $G$ su $V_{\rho}$, una sottorappresentazione di $\rho$ è un sottospazio vettoriale $W\subseteq V_{\rho}$ tale che $\rho(s)(W)\subseteq W\ \forall\ s\in G$. Posso definire una rappresentazione $\sigma$ con $V_{\sigma}=W$ e $\sigma(s)=\rho(s)|_W$ (la indicherò con $\sigma\subseteq \rho$).
\end{defn}



\begin{defn}[Rappresentazione irriducibile]
Una rappresentazione $\rho$ di $G$ è \textit{irriducibile} se
\begin{enumerate}
	\item $\rho \neq 0$ ($\deg(\rho) \geq 1$)
	\item $\rho$ non ha sottorappresentazioni non banali (diverse da 0 e $V_{\rho}$).
\end{enumerate}

\end{defn}


\begin{defn}[Rappresentazione completamente riducibile]
Una rappresentazione si dice completamente riducibile se si può scrivere come somma di rappresentazioni irriducibili.
\end{defn}


\begin{rem}
Attenzione al gioco di parole in italiano: una rappresentazione irriducibile è completamente riducibile. Il nome della definizione può in effetti portare a confusione.
\end{rem}


\begin{rem} Normalmente la cosa che si fa più spesso in teoria della rappresentazione è cercare di scomporre la rappresentazione di un gruppo come somma di rappresentazioni irriducibili. Vedremo quindi adesso diversi teoremi che ci aiuteranno in questi problemi.

\end{rem}



\begin{exmp}[Rappresentazione regolare di $S_3$]


\end{exmp}



\begin{thm}[Le rappresentazioni di un gruppo finito sono completamente riducibili]
  Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\rho$ è completamente riducibile.
  \label{thm:gruppo finito completamente riducibile}
\end{thm}
Per dimostrare questo teorema ci servono diversi lemmi che enunciamo e andiamo a dimostrare. Finiti i lemmi seguirà la dimostrazione.


\begin{prop}[Prodotto hermitiano invariante] Sia $G$ un gruppo finito e $\rho: G \to V_\rho$ una sua rappresentazione. Allora lo spazio vettoriale $V_\rho$ ammette una forma hermitiana invariante sotto l'azione di $G$, ovvero un prodotto tale che $h(v,w) = h(\rho(g), v, \rho(g) w) \ \forall v,w\in V_\rho, \forall g \in G$
\end{prop}

\textsc{Dimostrazione:}

Lo spazio $V_\rho$ ammette sicuramente una forma hermitiana, che chiamiamo $h$. Ora andiamo a fare una sorta di media per trasformare questa forma in una invariante. Consideriamo quindi

\[h_G(v, w) := \dfrac{1}{|G|} \dsum_{g \in G} h(\rho(g)v,\rho(g) w) \]

\'E abbastanza facile mostrare adesso che effettivamente $h_G$ è invariante sotto l'azione di $G$. Infatti,

\[ h_G(\rho(h)v, \rho(h) w) = \dsum_{g \in G} h(\rho(gh)v , \rho(gh) w)\]

Ma dato che $G$ è un gruppo, questo vuol dire solo \emph{far partire la somma da un indice diverso.} Di conseguenza $h_G$ è $G$-invariante \qed


\begin{lemma}
Sia $h: V_\rho \times V_\rho \to \C$ una forma hermitiana definita positiva e invariante per $\rho: G \to GL(V_\rho)$ e sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora se $W^\perp$ è l'ortogonale di $W$, $\rho|_{W^\perp}: G \to GL(W^\perp)$ è una sottorappresentazione.

\end{lemma}

\textsc{Dimostrazione:}

Per noti teoremi di algebra lineare sappiamo che

\[ V_\rho = W \oplus W^\perp \]

Di conseguenza un generico vettore di $V_\rho$ si potrà scrivere come somma di $w_1 + w_2$, con $w_1 \in W$ e $w_2 \in W^\perp$. Inoltre sappiamo che $\rho_{|_W}$ è una sottorappresentazione. Mostriamo che anche $\rho_{|_W^\perp}$ è una sottorappresentazione: quello che dobbiamo mostrare è che $\rho(g) w_2 \in W^\perp \ \forall g \in G$. Dato che abbiamo un prodotto hermitiano la cosa più facile da verificare è che $w_2$ sia ortogonale a $W$. Consideriamo quindi l'espressione

\[ 0 = h(w_1, w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp\]

Ma noi sappiamo che $h$ è $G$-invariante, quindi

\[ 0 = h(w_1, w_2) = h(\rho(g) w_1, \rho(g) w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp \]

E dato che $\rho(g) W = W$ per ipotesi, abbiamo mostrato che $\rho(w_2) \in W^\perp$. \qed




\begin{lemma}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo finito $G$. Sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora esiste una sottorappresentazione $\sigma: G \to GL(W')$ tale che

\[\rho = \rho|_W + \sigma \]
\end{lemma}


\textsc{Dimostrazione:} La tesi segue dai due lemmi precedenti. L'ipotesi di gruppo finito si usa per l'esistenza della forma hermitiana invariante. \qed



\begin{rem} Notare che il teorema precedente è falso per gruppi infiniti. Un esempio si può costruire prendendo $G=\Z$, $V=\C^2$, e come rappresentazione
\begin{align*}
	\rho:&\Z\to GL(\C^2)\\
	&k\to M^k
\end{align*}
dove $M=\begin{pmatrix}
        	1 & 1\\
        	0 & 1
        \end{pmatrix}$ è scritta nella base canonica.\newline
Si vede subito che una sottorappresentazine è $Span(e_1)$ essendo $e_1$ autovettore per ogni $\rho(k)$, ma non esiste un suo complementare $G$-invariante: se esistesse avrebbe dimesione 1, quindi avremmo diagonalizzato $\rho(k)\ \forall k\in \Z$, ma sappiamo che tali endomorfismi non sono diagonalizzabili. 
\end{rem}


\textsc{Dimostrazione del teorema \ref{thm:gruppo finito completamente riducibile}:}

Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di $G$. Se $\rho$ è irriducibile, allora è completamente riducibile e quindi segue la tesi. Se invece esiste un sottospazio invariante $W$, allora per i lemmi precedenti esiste $Z \subset V_\rho$ tale che $V_\rho = W \oplus Z$ e tale che $Z$ sia una sottorappresentazione. Per induzione si procede fino ad ottenere la tesi. \qed





\begin{thm} Se $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ sono rappresentazioni di $G$ e $f: V_\rho \to V_\sigma$ è un omomorfismo di rappresentazioni, allora $Im(f)$ è una sottorappresentazione di $\sigma$ e $Ker(f)$ è una sottorappresentazione di $V_\rho$
\end{thm}
\begin{proof}
Se $v\in Ker(f)$ allora per la definizione di omomorfismo di rappresentazioni ho che $\forall s\in G$ $f(\rho(s)v)=\sigma(s)f(v)=0$ e quindi $\rho(s)v\in Ker(f)$. Allo stesso modo, se $w\in Im(f)$ allora $w=f(v)$ per qualche $v\in V_\rho$ e quindi sempre per la definizione di omomorfismo di rappresentazione $\sigma(s)w=\sigma(s)f(v)=f(\rho(s)v)\in Im(f)$
\end{proof}




\begin{thm}Sia $G$ un gruppo abeliano finito. Allora ogni rappresentazione di $G$ è isomorfa alla somma di rappresentazioni di grado 1.
\end{thm}
\begin{proof}
	\'E una conseguenza immediata della proposizione \eqref{prop:diagonalizzabilita rappresentazioni}: sia $\rho:G\to GL(V_{\rho})$, con $dim V_{\rho} = n$, dato che $G$ è finito $\forall g\in G$ $\rho(g)$ è diagonalizzabile inoltre, visto che è abeliano, si sfrutta l'osservazione alla fine della proposizione per dedurre che le $\rho(g)$ sono simultaneamente diagonalizzabili. A questo punto il teorema è dimostrato: sia $\{v_1,\ldots,v_n\}$ una base comune di autovettori, per ogni $1\leq i\leq n$ $Span(v_i)$ è una sottorappresentazione di grado 1 di $G$ (perchè invariante per $G$), visto che $V_{\rho}=Span(v_1)\oplus\ldots\oplus Span(v_n)$, allora $\rho\cong\rho_1+\ldots+\rho_n$, dove $\rho_i:G\to GL(Span(v_i))$.
\end{proof}


\begin{prop} La rappresentazione regolare $\mathcal{R}$ di $C_n$ è isomorfa alla somma delle $n$ rappresentazioni irriducibili di grado 1 di $C_n$.

\end{prop}


\begin{lemma}
Date $\rho_1, \rho_2, \sigma$ rappresentazioni di $G$, allora

\[Hom(\rho_1 + \rho_2, \sigma) \cong Hom(\rho_1, \sigma) \oplus Hom(\rho_2, \sigma)\]

\end{lemma}


\begin{thm}[Lemma di Schur]
Siano $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ due rappresentazioni irriducibili di $G$ gruppo finito e $\phi:V_\rho \to V_\sigma$ un omomorfismo di rappresentazioni, allora $\phi$ è un isomorfismo oppure è identicamente nullo. Se poi $f:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni e lo spazio vettoriale $V_\rho$ è su $\C$ o su un campo algebricamente chiuso $\K$, allora $f$ è una moltiplicazione per scalare.
\end{thm}
\begin{proof}
Supponiamo che $\phi\neq 0$, allora sappiamo che $Ker(\phi)\subseteq V_\rho$ è una sottorappresentazione di $\rho$, ma $\rho$ è irriducibile e quindi $Ker(\phi)=0\Rightarrow \phi$ iniettiva. Ma anche $Im(\phi)\subseteq V_{\sigma}$ è una sottorappresentazione di $\sigma$ e, non essendo nulla ed essendo $\sigma$ irriducibile, coincide con tutto $V_\sigma \Rightarrow \phi$ suriettiva, da cui $\phi$ è un isomorfismo.
Consideriamo ora $f$: sia $\lambda$ un autovalore di $f$, che esiste perché $G$ è finito e stiamo lavorando su $\C$, allora $f-\lambda Id:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni. Ma non è iniettivo, perché c'è almeno un autovettore relativo a $\lambda$, e quindi per la prima parte del lemma di Schur ho che $f-\lambda Id$ è identicamente nullo, da cui ricaviamo che $f$ è la moltiplicazione per uno scalare ($\lambda$).
\end{proof}


\begin{thm}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione e 

\[\rho = \dsum_{i=1}^N n_i \rho_i \]

una sua scomposizione come somma di rappresentazioni irriducibili a due a due non isomorfe. Allora la scomposizione è unica.
\end{thm}



\begin{lemma}
Sia $\rho$ una rappresentazione di $G$ e $\mathcal{R}$ la sua rappresentazione regolare. Allora 
\[deg(\rho) = dim(Hom(\mathcal{R}, \rho))\]
\end{lemma}




\begin{thm}
Sia $\mathcal{R}$ la rappresentazione regolare di $G$, un gruppo finito, e sia 

\[ \mathcal{R} = \dsum_{i=1}^Nn_i \rho_i\]

Con $\rho_i$ irriducibili e a due a due non isomorfe. Allora ogni rappresentazione irriducibile di $G$ è isomorfa ad una $\rho_i$. Inoltre $n_i = deg(\rho_i)$ 
\label{thm: teorema importantissimo}
\end{thm}



\begin{cor}
Se $G$ è abeliano allora ha $|G|$ rappresentazioni irriducibili di grado 1 e $\mathcal{R}$ è la somma di queste.
\end{cor}



\begin{cor}
Sia $G$ un gruppo finito. $G$ ha un numero finito di rappresentazioni irriducibili, a meno di isomorfismi. Inoltre
\[|G| = \dsum n_i^2\]
\end{cor}



















\newpage
\section{Teoria dei caratteri}


\begin{defn}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo $G$. Definiamo carattere di $\rho$ la funzione che associa ad ogni elemento del gruppo $G$ la traccia della matrice associata all'elemento, ovvero

\[\chi_\rho(s) := tr (\rho(s)) \qquad \forall s \in G \]
Notare che $\chi_{\rho}$ è una funzione che va dal gruppo in $\C$, ovvero $\chi_{\rho}: G \to \C$

\end{defn}

Vediamo delle proprietà elementari del carattere

\textsc{Osservazioni:}
\begin{enumerate}
	\item Se $deg(\rho) = 1$ allora il carattere di $s$ è uguale a $\rho(s)$
	\item $\chi_{\rho_1} = deg(\rho)$. \footnote{Al solito $\rho_1$ è la rappresentazione che manda ogni elemento nell'identità di $V_\rho$}\\
	Questo è vero poichè $[\rho_1]=I_n\Rightarrow tr(\rho_1)=n$ ed $n=deg(\rho)$.
	\item $\chi_{\rho + \sigma}(s) = \chi_\rho(s) + \chi_\sigma(s)$.\\ 
	Questo è dovuto al fatto che la somma di rappresentazioni si può scrivere come matrice a blocchi. Una volta scritto così è evidente il risultato.
	\item $\chi_{\rho\sigma}(s) = \chi_\rho(s)\chi_\sigma(s)$.\\ 
	Questo deriva dal seguente fatto generale:
	
\begin{lemma} 
Se $f: V \to V$ e $g: W \to W$ sono endomorfismi di spazi vettoriali, allora $tr(f \otimes g) = tr(f)tr(g)$.
\end{lemma}
\textbf{Dimostrazione:} Iniziamo a considerare il caso in cui sia $f$ che $g$ siano diagonalizzabili: prendendo due basi $a:I\rightarrow V$ , $b:J\rightarrow W$ di autovettori rispettivamente per $f$ e per $g$, si verifica facilmente la verità della proposizione nella base indotta su $V\otimes W$ (ovvero in quella formata dagli $a_i\otimes b_j$).\\
Ora, essendo la traccia una funzione continua e le matrice diagonalizzabili dense nello spazio delle matrici, la proprietà affermata dal lemma si estende al caso generale per continuità.
	\item $\chi_{\rho}(s^{-1})=\overline{\chi_{\rho}(s)}$\\
Essendo $G$ un gruppo finito, $\forall s\in G\ \rho(s)^n = id$ dove $n=|G|$: dunque tutti gli autovalori di $\rho(s)$ sono radici ennesime dell'unità e $\rho(s)$ è diagonalizzabile\footnote{Si veda la proposizione \ref{prop:diagonalizzabilita rappresentazioni}}. In tale base è evidente che:
$$\chi_{\rho}(s^{-1})=tr(\rho (s^{-1}))=tr(\rho (s)^{-1})=\sum_i\lambda_i^{-1}=\sum_i\overline{\lambda_i}=\overline{tr(\rho(s))}=\overline{\chi_{\rho}(s)}$$
in quanto, avendo gli autovalori modulo 1, l'inverso coincide con il coniugio.  	
	\item $\chi_{\rho^*}(s)\footnote{Ricordiamo che $\rho^*(s) = (\rho(s)^{-1})^*$} = \overline{\chi_\rho(s)}$.\\
		Per l'osservazione precedente vale che
		$$\chi_{\rho^*}(s)=tr(^t\rho(s^{-1}))=tr(\rho(s^{-1}))=\overline{tr(\rho(s))}=\overline{\chi_\rho(s)}$$
	\item $\chi_{\rho}(hsh^{-1})=\chi_{\rho}(s)$ ovvero $\chi_\rho$ è costante sulle classi di coniugio di $G$. La motivazione è semplice: se due elementi sono coniugati tra loro questo significa che le matrici corrispondenti saranno simili e la traccia è un invariante di similitudine.
	
Di conseguenza, non sarà necessario calcolare il carattere per ogni elemento del gruppo ma basterà farlo per le classi di coniugio di $G$.

Le funzioni che costanti sulle classi di coniugio di un gruppo vengono dette $funzioni\ di\ classe$. L'insieme delle funzioni di classe di un gruppo viene normalmente indicato con $Cl(G)$ e si verifica che esso è un sottospazio di $\mathbb{C}^G$.
	\item Supponiamo di avere una rappresentazione per permutazioni. Sia $I$ un insieme finito e $G$ un gruppo allora 
$$\chi_{\rho_{I}}(s)=\#punti\ fissi\ di\ \rho_I(s)=|I^s|$$
dove $I^s:=\{i\in I| s\circ i=i\}$. La veridicità di questo fatto si vede scrivendo esplicitamente la matrice che rappresenta $\rho_I(s)$.
	\item Consideriamo la rappresentazione per permutazioni regolare $R$. Calcoliamone il carattere:
	\[ \chi_{\mathcal{R}}(s) = \begin{cases}
|G| \qquad \text{se } s=id \\
0 \qquad \text{se } s\neq id\\
\end{cases} \]
semplicemente perchè $s\circ g=g\Leftrightarrow s=id$.
\end{enumerate}
\textbf{Esempio:} $G=S_3$, $I=\{1,2,3\}$. Allora

\[ \chi_{\rho_I}(s) = \begin{cases}
3 \qquad \text{se } s=id \\
1 \qquad \text{se } s\ \text{è una trasposizione}\\
0 \qquad \text{se } s\ \text{è un treciclo}\\
\end{cases} \]
Ricordandoci che $\chi_{\rho_I}=\chi_{1+\rho}$ si ha che 
\[ \chi_{\rho}(s) = \begin{cases}
2 \qquad \ \ \text{se } s=id \\
0 \qquad \ \ \text{se } s\ \text{è una trasposizione}\\
-1\qquad \text{se } s\ \text{è un treciclo}\\
\end{cases} \]

\begin{defn}[Prodotto hermitiano dei caratteri]

\[ \langle f | g \rangle = \dfrac{1}{|G|} \dsum_{s \in G} f(s)\overline{ g(s)} \]

\end{defn}


\begin{thm}[Relazioni di ortogonalità]
Se $\rho$ e $\sigma$ sono rappresentazioni irriducibili di $G$, allora vale

\[\langle \chi_{\rho}|\chi_{\sigma} \rangle = \begin{cases}
1 \qquad \text{se } \rho \cong \sigma \\
0 \qquad \text{altrimenti }\\
\end{cases} \]
\label{relazione di ortogonalita}
\end{thm}

Per dimostrare questo teorema abbiamo bisogno di un lemma che ora enunciamo e dimostriamo.




\begin{lemma}
Se $(\rho, V_\rho)$ e $(\sigma, V_\sigma)$ sono rappresentazioni \footnote{Non necessariamente irriducibili} di $G$, allora vale

\[ \langle \chi_\rho | \chi_\sigma \rangle  = dim(Hom (\sigma, \rho))\] 
\label{lemma relazioni ortogonalita}
\end{lemma}
\textsc{Dimostrazione:}

L'idea principale per dimostrare questo lemma è di ridurci al caso più facile in cui una delle due rappresentazioni è quella banale. Per farlo notiamo un paio di cose

\[ \langle \chi_\rho | \chi_\sigma \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \overline{\chi_\rho(s)} \chi_\sigma(s) = \dfrac{1}{|G|} \dsum_{s\in G} {\chi_{\rho^*}(s)} \chi_\sigma(s) = \dfrac{1}{|G|} \dsum_{s\in G} \chi_{\rho^*\sigma}(s)  = \langle \chi_{\rho^*\sigma} \rangle | 1\]

Siamo passati da due rappresentazioni ad una sola. In particolare lo spazio vettoriale su cui agisce questa rappresentazione è 

\[ V_{\rho^* \sigma} = V_{\rho}^* \otimes V_\sigma \cong Hom(V_\rho, V_\sigma)\]

E questo isomorfismo segue semplicemente dalle proprietà del prodotto tensore di spazi vettoriali. Notiamo che sullo spazio degli omomorfismi\footnote{Dato che sono spazi vettoriali in questo caso si tratta semplicemente di applicazioni lineari} $Z = Hom(V_\rho, V_\sigma)$ è possibile definire una rappresentazione completamente analoga a $\rho\sigma^*$ in questo modo: se $f \in Z$, allora possiamo definire la rappresentazione $\tau, V_\tau = Z$ di $G$ in questo modo

\[ \tau(s)f = \rho(s) \circ f \circ \sigma^{-1}(s)\]

\'E possibile mostrare che se chiamo $\Psi$ la mappa tale che

\[
\begin{cases}
V_\rho^* \otimes V_\sigma \xrightarrow{\Psi} Hom(V_\rho, V_\sigma)\\
\rho\sigma^* \xrightarrow{\Psi} \tau \\
\end{cases}
\]

Allora $\Psi$ è un isomorfismo di rappresentazioni. Dimostriamolo rapidamente. Innanzitutto definiamo in modo esplicito $\Psi$. Basterà definirlo per i tensori decomponibili, per il resto dello spazio basterà estenderlo per linearità.

\[ \Psi(\phi \otimes v ) (w) = \phi(w) v \qquad \forall \phi \in V_\rho^*, \forall v \in V_\sigma, \forall w \in V_\rho\]

Per mostrare che è un isomorfismo di rappresentazioni ci basta mostrare che ha la giusta proprietà di commutazione in quanto sappiamo già che $\Psi$ è un isomorfismo di spazi vettoriali. Vediamo quindi di mostrare che 

\[\Psi(\tau(s)\cdot (\phi \otimes v))(w) = \tau(s) \cdot \Psi(\phi \otimes v)(w) \qquad \forall s \in G \quad \text{eccetera} \]


Partiamo dal membro di sinistra e facciamo i calcoli


\[\Psi(\tau(s)\cdot (\phi \otimes v))(w) = \Psi\left( \rho(s) (\phi \otimes v ) \sigma(s)^{-1}  \right)(w)  = \]

\[= \Psi((\phi \circ \sigma(s)^{-1})\otimes (\rho(s) v) )(w) =  \]

\[ = (\phi \circ \sigma(s)^{-1}) (w) \rho(s) v = \phi(\sigma(s)^{-1} w) \rho(s) v\]

\[ = \rho(s)\phi(\sigma(s)^{-1} w) v = \rho(s) \Psi(\phi \otimes v) \sigma(s)^{-1} (w) = \tau(s) \Psi (\phi \otimes v)(w)\]


A questo punto possiamo andare a cercare i sottospazi invarianti per $\tau$, ovvero stiamo andando a cercare le sottorappresentazioni irriducibili di $\tau$ sperando di usare teoremi che già conosciamo. In particolare stiamo quindi cercando dei sottospazi $W \subset Z = Hom(V_\rho, V_\sigma)$ tali che $\tau(s) W \subset W \quad \forall s \in G$ 

In particolare, cerchiamo le funzioni $f \in Hom(V_\sigma, V_\rho)$ tali che $\tau(s) f = f$. Dalla definizione di $\tau$ si vede che

\[f = \tau(s) f =  \rho(s) \circ f \circ \sigma^{-1}(s) \Rightarrow  f \sigma(s)= \rho(s) f\] 


Ovvero le applicazioni $f$ invarianti per $\tau $ sono gli omomorfismi di rappresentazioni da $(\rho, V_\rho) $ a $(\sigma, V_\sigma)$

A questo punto

\[ (V_{\sigma^*\rho})^G \cong Hom(V_\sigma, V_\rho)^G \cong Hom(\sigma, \rho)\]

Per cui dato che noi stiamo cercando $dimHom(\sigma, \rho)$, basterà trovare $dim(V_{\sigma^*\rho})^G$

Visto che ci siamo ricondotti al caso in cui una rappresentazione è banale, ora facciamo i conti cercando di trovare la dimensione dello spazio invariante per $G$. Scriviamo la definizione di quello che vogliamo calcolare


\[ \langle \chi_\rho | 1 \rangle = \dfrac{1}{|G|} \dsum_{s \in G} tr \rho(s)\]

Se definiamo l'operatore $T$ come operatore lineare

\[ T = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s)\]


Allora si nota che 

\[ \rho(t) Tv = \dfrac{1}{|G|} \dsum_{s\in G} \rho(t)\rho(s) v = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s) = Tv\]

Per cui sappiamo che $V_\rho^G \subseteq ImT$. L'obiettivo è mostrare che quella non è una disuguaglianza ma un'uguaglianza. In realtà questa è la disuguaglianza stupida in quanto se $v \in V_\rho^G$ allora è chiaro che $Tv = v$, basta applicare la definizione. Per cui $ImT = V_\rho^G$. A questo punto vogliamo calcolare la sua traccia. Per farlo notiamo che 

\[T(Tv) = \ldots = Tv \qquad \text{Verifica banale}\]

Per cui $T$ è un proiettore. A questo punto sappiamo dall'algebra lineare che

\[ V_\rho = Ker T \oplus Im T = Ker T \oplus V_\rho^G\]

Per cui $trT = dimImT = dimV_\rho^G$. Ma dalla catena di deduzioni che abbiamo fatto

\[dimHom(V_\sigma, V_\rho) = dim(V_{\sigma^*\rho}^G) = trT = \langle \chi_{\sigma^*\rho} | 1 \rangle = \langle \chi_\sigma | \chi_\rho \rangle\]

\qed














\textsc{Dimostrazione del teorema \ref{relazione di ortogonalita}:}

A questo punto la tesi del teorema \ref{relazione di ortogonalita} segue dal lemma precedente applicato insieme al lemma di Schur. \qed


\textsc{Osservazioni:}

\begin{itemize}
\item Ricordiamo che se $\rho$ è una rappresentazione di $G$, allora $\rho$ si può scrivere in modo unico come 

\[ \rho = \dsum_i n_i \rho_i\]

Dove le $\rho_i$ sono le rappresentazioni irriducibili di $G$ e gli $n_i$ sono numeri naturali $\geq 0$. Dall'equazione scritta sopra segue subito che

\[ \chi_\rho = \dsum_i n_i \chi_{\rho_i}\]

E possiamo ottenere un'informazione utile prendendo il prodotto scalare dell'equazione precedente con il carattere di una delle rappresentazioni $\rho_i$

\[ \langle \chi_\rho | \chi_{\rho_j} \rangle = \dsum_i n_i \langle \chi_{\rho_i} | \chi_{\rho_j} \rangle \Rightarrow n_i \delta_{ij} = \langle \chi_\rho | \chi_{\rho_j} \rangle \Rightarrow n_i = \langle \chi_\rho | \chi_{\rho_i} \rangle\]

\item Caso particolare interessante del fatto precedente riguarda la rappresentazione regolare di un gruppo. Difatti come sappiamo,

\[ \chi_{\mathcal{R}}(s) = 
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]
Quindi considerando una sottorappresentazione  si ha che 
\[
\langle \chi_{\mathcal{R}} | \chi_\rho \rangle = \frac{1}{|G|}|G|\chi_{\rho}(id)=\chi_{\rho}(id)=deg(\rho)
\]
In particolare se $\rho$ è una sottorappresentazione irriducibile allora
\[ deg(\rho)=dim(Hom(\mathcal{R},\rho)) \]
Quindi ottengo una conferma del teorema precedente 
\[      
\langle \chi_{\mathcal{R}} | \chi_\rho \rangle =dim(Hom(\mathcal{R},\rho))
\]

\item Se $\rho$ e $\sigma$ sono 2 rappresentazioni irriducibili allora $$\rho \cong \sigma \Leftrightarrow \chi_{\rho}=\chi_{\sigma}$$

\item $\langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = \sum_i n_i^2$.
\item Conseguenza dell'ultima osservazione è che una rappresentazione di un gruppo $\rho$ è irriducibile $\Leftrightarrow \langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = 1$ 


\end{itemize}




\begin{cor}[Corollario del lemma \ref{lemma relazioni ortogonalita}: Lemma di Burnside]

Consideriamo un'azione di un gruppo $G$ su un insieme $I$ e consideriamo una rappresentazione dell'azione di $G$, $(\rho_I, V_{\rho_I})$. Consideriamo

\[\langle \chi_{\rho_I} | 1 \rangle = \dfrac{1}{|G|} \dsum_{s\in G} tr \rho_I(s)\]

Ma è ovvio che 

\[tr \rho_I(s) = |I^s| \qquad I^s = \{ i \in I | s \cdot i = i\} \]

Per cui lo spazio $V_{\rho_I}^G = \{\dsum a_i e_i | \text{Alcune condizioni}\}$ sarà composto da i vettori che hanno i coefficienti $a_i$ costanti su ciascuna orbita di $G$ su $I$, proprio per lasciarlo invariante. Perciò 

\[ dimV_{\rho_I}^G = \text{numero delle orbite } = |I/G|\]

E con l'affermazione precedente si ottiene appunto il lemma di Burnside

\[ |I/G| = \dfrac{1}{|G|} \dsum_{s\in G} |I^s|\]

\qed
\end{cor}



\begin{thm} Sia $G$ un gruppo finito e siano $\rho_1, \ldots , \rho_r$ le sue rappresentazioni irriducibili. Sia inoltre 

\[Cl(G)  \]

Lo spazio delle funzioni da $G$ in $\C$ costanti sulle classi di coniugio di $G$

Chiaramente $\dim Cl(G) = $ numero di classi di coniugio di $G$ $:= s$. La tesi del teorema è che $r = s$ 
dove $r$ è il numero di rappresentazioni irriducibili. 

\textsc{Osservazione:} Per questo motivo la tabella dei caratteri sarà una tabella quadrata.

\end{thm}

\textsc{Dimostrazione:}
 
Mostriamo intanto che $r \leq s$: i caratteri di $\rho_1, \ldots , \rho_r$ sono infatti ortonormali rispetto alla forma hermitiana
definita positiva $\langle . | .\rangle$, e quindi sono indipendenti (non posso avere più di $s$ vettori linearmente indipendenti 
in uno spazio vettoriale di dimensione $s$).

Verifichiamo ora che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = {0}$. Sia $f \in Cl(G)$ e $\rho$ una rappresentazione,
definiamo $T_f= \frac{1}{|G|}\dsum_s f(s)\rho(s)$ e verifichiamo che è un omomorfismo di rappresentazioni: 
$$ T_f \circ \rho(t) = \frac{1}{|G|}\dsum_s f(s)\rho(s) \rho(t) = \frac{1}{|G|}\dsum_s \rho(t) \rho(t^{-1}) f(s)\rho(s) \rho(t) =$$

$$= \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}) \rho(s) \rho(t) = \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}st) = 
\frac{1}{|G|}\rho(t) \dsum_{s'} f(s') \rho(s')= \rho(t) \circ T_f$$

Abbiamo usato il fatto che $f(s)\in \C$, quindi commuta con $\rho(g)$, e che $f$ è una funzione di classe nella 
sostituzione di $s$ con $s'= t^{-1}st$ (essendo il coniugio un automorfismo, cambia solo l'ordine della somma).

Se $\rho$ è irriducibile, $T_f= \alpha I$ è uno scalare per il lemma di Schur, $\alpha= \frac{\tr(T_f)}{\deg(\rho}$.
Si ha $$\alpha =\frac{1}{\deg(\rho)}\tr(T_f)=\frac{1}{\deg(\rho)|G|}\dsum_s f(s) \chi_\rho(s)=
\frac{1}{\deg(\rho)} \langle f | \chi_{\rho^*} \rangle = 0$$ se $f \in \langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp}$.

Generalizziamo a quando $\rho$ non è irriducibile, ossia $\rho = \sigma_1+ \ldots + \sigma_n$, con $\sigma_i$ irriducibili. Allora 
si ha $V_\rho = V_{\sigma_1}  \oplus \ldots \oplus V_{\sigma_n}$, ed essendo $T_f=0 $ su ogni $V_{\sigma_i}$, è nullo anche su $V_\rho$.

Mostriamo che questo implica $f=0$: sia $R$ la rappresentazione regolare di $G$; si ha:
$$ 0= |G| T_f (e_1) = \dsum_s f(s) R(s)(e_1) = \dsum_s f(s) e_s $$

Di conseguenza $f(s)=0 \quad \forall s \in G$ essendo gli $e_s$ una base di $V_R$.
In questo modo abbiamo mostrato che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = \{0\}$, quindi $r=s$.
\qed

















\subsection{Tabella dei caratteri}


Dato un gruppo $G$, possimo costruire la $tabella\ dei\ caratteri$ nel seguente modo:
\begin{itemize}
\item su ogni colonna mettiamo un rappresentante della classe di coniugio con sotto la cardinalità dell'orbita ovvero

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c}
\hline
$G$  & $e$ & $orb(g_1)$ & $orb(g_2)$ & \\
 & 1 & $|orb(g_1)|$ & $|orb(g_2)|$ & \\
\hline
 & &  & \\
\end{tabular}
\end{table}

\item su ogni riga mettiamo una rappresentazione irriducibile del gruppo
\item all'incrocio tra la rappresentazione $\rho_i$ e la classe di coniugio di $g_j$ inseriamo il valore di $\chi_{\rho_i(g_j)}$.


\end{itemize}



\subsection{Esempi di rappresentazioni di gruppi finiti}

\begin{exmp}[Tabella dei caratteri di $S_3$] 
La prima cosa da fare per costruire la tabella dei caratteri è vedere quanti elementi ha $S_3$, suddividerli in classi di coniugio e poi cercare le rappresentazioni irriducibili solo dopo aver fatto tutto questo. Notiamo subito che $S_3$ ha esattamente 3 classi di coniugio. La prima è ovviamente quella banale, composta solo dall'identità $e$. Poi c'è la classe delle trasposizioni $\{(1 2) ,(2 3), (1 3)\}$ che ha 3 elementi e poi ci sono i $3$cicli, ovvero $(1 2 3)$ e $(1 3 2)$. Possiamo cominciare a scrivere una tabella vuota $3\times 3$


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )  \\
 & 1 & 3 & 2 \\
\hline
 & &  & \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}



Una rappresentazione irriducibile che c'è sempre è la rappresentazione banale di grado 1, ovvero quella che manda ogni elemento nell'identità. La tabella con questa informazione diventa



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}


Un'altra rappresentazione che già conosciamo è il segno, $\epsilon$, che ricordiamo vale $(-1)^{n-1}$ dove $n$ è la lunghezza del ciclo. La tabella diventa




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

A questo punto ci sono due motivi per dire che l'ultima rappresentazione ha grado 2: il primo è che è l'unico modo di ottenere la relazione

\[ |G | = \dsum_i n_i^2 \]

Il secondo è che se fossero due rappresentazioni di grado 1 allora il gruppo avrebbe solo rappresentazioni irriducibili di grado 1 e un teorema che abbiamo fatto implicherebbe che $S_3$ sia abeliano, cosa palesemente falsa. 

Per trovare il carattere dell'ultima rappresentazione possiamo agire in più modi. Innanzitutto la tabella ora ha la forma




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 &  & \\
\hline
\end{tabular}
\end{table}


In generale ci saranno due numeri complessi $a, b$ nelle due caselle che mancano. Tuttavia noi sappiamo un sacco di teoremi che ci permettono di restringere il campo dei valori che possono avere. Per esempio noi sappiamo che 

\[\langle \rho_i | \rho_j \rangle = \delta_{ij}\]
 
Per cui imponendo che il prodotto scalare con entrambe le precedenti faccia 0 abbiamo due equazioni e due incognite, ovvero un problema risolvibile. L'altro modo è dire che

\[ \mathcal{R} = 1 + \epsilon + 2\rho\]

E dato che il carattere si comporta bene con la somma di rappresentazioni, 

\[\chi_{\mathcal{R}} = \chi_1 + \chi_\epsilon + 2 \chi_\rho  \]

Ma sappiamo anche che 

\[ \chi_{\mathcal{R}}(s) = 
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]

Per cui con agili conti riusciamo a completare la tabella








\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 & 0 & 1 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_3$}
\label{tabella caratteri s3}
\end{table}

L'ultimo modo è cercare di scomporre un'altra rappresentazione a caso di $S_3$, cercando di trovare la rappresentazione che ci manca. Per esempio ricordiamo l'azione di $S_3$ sui vettori di base di $\mathbb{R}^3$

\[ \tau(s) e_i = e_{s(i)}\]

Ricordiamo che il sottospazio di dimensione $1$ fatto dallo span del vettore $v = e_1 + e_2 + e_3$ è un sottospazio invariante in cui $\tau(s)$ è sostanzialmente l'identità. Il suo ortogonale è un altro sottospazio invariante su cui $\rho$ è irriducibile. Di conseguenza potremo scrivere

\[ \tau = 1 + \rho\]

E siamo sicuri che l'altra rappresentazione di grado 2 sia esattamente quella che stiamo cercando proprio grazie al teorema che ci dice che tutte le rappresentazioni irriducibili di un gruppo compaiono nella sua rappresentazione regolare. (Teorema \ref{thm: teorema importantissimo})

Dato che è facile calcolare il carattere di $\tau(s)$ in quanto è uguale a $Fix(s)$, possiamo scrivere

\[ Fix(s) = 1 + \chi_\rho\]

Da cui si ricava subito il carattere della rappresentazione $\rho$



\end{exmp}






\begin{exmp}[Tabella dei caratteri di $S_4$]
Facciamo la prima cosa importante: dividiamo $S_4$ in classi di coniugio. Per i soliti teoremi sugli $S_n$, le classi di coniugio saranno 
\[\{e\}, \{(a b)\}, \{(a b c)\}, \{(a b c d)\}, \{(a b)(c d)\}\]

E notiamo che sono 5. Possiamo quindi cominciare a compilare la tabella dei caratteri vuota



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


dove ho già messo la rappresentazione banale. Anche per $S_4$, essendo un gruppo simmetrico c'è la rappresentazione segno di grado 1. 




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


A questo punto bisogna fare cose a caso cercando le rappresentazioni irriducibili. Per esempio possiamo di nuovo considerare la rappresentazione per permutazioni



\[ \tau(s) e_i = e_{s(i)}\]


Che si scompone anche questa come

\[ \tau = 1 + \rho\]

Vorremmo sapere se $\rho$ è irriducibile. Potremmo invocare qualche teorema ma lo faremo con le mani calcolando il carattere di $\rho$


\[ \chi_\rho(s) = Fix(s) - 1 = 
\begin{cases}
3 \quad \text{Se } s = e \\
1 \quad \text{Se } s = (a b) \\
0 \quad \text{Se } s = (a b c) \\
-1 \quad \text{Se } s = (a b c d ), (a b) (c d)\\
\end{cases}
\]

E andando a calcolare

\[\langle\chi_\rho |\chi_\rho\rangle = \dfrac{1}{24}\left(3^2  + 6 \cdot 1^2  + 0 + (-1)^2 \cdot (3 +6 )\right) = 1\]
 

Per cui è effettivamente irriducibile.  Aggiungiamola alla tabella.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

Abbiamo appena terminato le rappresentazioni che conoscevamo di $S_4$.\\
\textbf{Ottimo consiglio:} Quando non vengono in mente altre rappresentazioni, considera due già presenti nella tabella e fanne il prodotto. Risulta utile il seguente lemma.

\begin{lemma}
Se $\rho$ e $\sigma$ sono due rappresentazioni e $deg(\rho)=1$ ( ovvero $\rho:G\rightarrow \mathbb{C}^*$), allora $\sigma$ è irriducibile $\Leftrightarrow$ $\rho\sigma$ lo è. Inoltre hanno lo stesso grado.
\end{lemma}

\textbf{Dimostrazione:} Che sia ancora a tutti gli effetti una rappresentazione si verifica esplicitamente sapendo che
\[
\forall s\in G \rho\sigma(s)=\rho(s)\sigma(s)
\]
Per dimostrare che è irriducibile si considera il fatto che
\[
\sigma \ irriducibile\ \Leftrightarrow 1=\langle\chi_{\sigma}|\chi_{\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\sigma(s)}|^2
\]
Quindi...
\[
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{S\in G}|\chi_{\rho\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho(s)}\chi_{\sigma(s)}|^2=\frac{1}{|G|}|\rho(s)\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)|^2|\chi_{\sigma(s)}|^2
\]
ed essendo $\rho(s)$ una radice $n-$esima dell'unità dove $n$ è l'ordine di $G$ si ha che 
\[1 | 
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}
|\chi_{\sigma(s)}|^2=\langle \chi_{\sigma}|\chi_{\sigma}\rangle
\]
Che abbiano lo stesso grado deriva dal fatto che
\[
\chi_{\rho\sigma}=\chi_{\rho}\chi_{\sigma}\Rightarrow deg(\rho\sigma)=\chi{\rho\sigma}(id)=\chi_{\rho}(id)\chi_{\sigma}(id)=deg(\rho)deg(\sigma)=deg(\sigma).
\] \\
Essendo $\epsilon$ di grado 1 e $\rho$ irriducibile allora anche $\rho\epsilon$ è un'altra rappresentazione irriducibile.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


E a questo punto dato che $|S_4|=24$ e che $1+1+3^2+3^2=20$ si possono avere due situazioni: $S_4$ potrebbe avere ancora 4 rappresentazioni irriducibili di grado 1 oppure solo più una di grado 2. Tuttavia abbiamo visto come $S_n$ ammetta solo due rappresentazioni irriducibili di grado 1 quindi siamo nel secondo caso.\\
Dato che ce ne manca solo una possiamo usare il trucco di prima (differenza dalla rappresentazione $R$ ) e concludere:

\begin{table}[!ht] 
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
 $\sigma$& 2&  0 & -1& 0 & 2\\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_4$}
\label{tabella caratteri s4}
\end{table}


\textbf{Ossevazione:} Guardiamo la tabella, in particolare il "minore" ottenuto considerando le prime due e l'ultima riga e le prime 3 colonne. 

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 6 & 8 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\sigma$ & 2 & 0 & 1 \\
\hline
\end{tabular}
\end{table}
Se la confrontiamo con la tabella dei caratteri di $S_3$ vediamo che sono analoghe. Intuitivamente $\rho$ in $S_3$ deriva dalla rappresentazione $\sigma$ di $S_4$ mediante un omomorfismo 
\[
S_4\rightarrow S_3
\]
che corrisponde ad una azione di $S_4$ su un insieme di 3 elementi. Tale insieme è il sottogruppo di Klein privato dell'unità ovvero
\[
\{ (12)(34),(13)(24),(14)(23)\}
\]
\\
In questo caso non è servito ma potremmo trovarci in una situazione in cui i seguenti lemmi si rivela utile
\begin{lemma}
$\rho^* $ è irriducibile $ \Leftrightarrow \rho$ è irriducibile.
\end{lemma}
Infatti $\chi_{\rho^*}=\overline{\chi_\rho} $ e quindi analogamente al lemma precedente si vede che
\[
1=\langle\chi_{\rho}|\chi_{\rho}\rangle \Leftrightarrow 1=\langle\chi_{\rho^*}|\chi_{\rho^*}\rangle
\]

\begin{lemma}
Se $\rho$ è una rappresentazione di grado $d$ di $G$, come sempre gruppo finito, allora:\\
$(a)$ $|\chi_{\rho}(s)|\leq d$ \\
$(b)$ Direttamente dal punto $(a)$ si decude che, 
\[
\chi_{\rho}(s)=d\Leftrightarrow \lambda_1,...,\lambda_d=1\Leftrightarrow \rho(s)=id
\]
dove $\lambda_1,..,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$.
\end{lemma}
\textbf{Dimostrazione:} Se $\lambda_1,..,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$ allora $\chi_{\rho}(s)=\sum_{i=1}^{d}\lambda_i$. Inoltre essendo $G$ finito $|\lambda_i|=1\forall i\in \{1,...,d\}$. Se ne deduce che
\[
|\chi_{\rho}(s)|\leq \sum_{i=1}^d |\lambda_i|=d
\]



\end{exmp}





\begin{exmp}[Tabella dei caratteri di $D_5$]

La prima cosa da fare è dividere $D_5$ in classi di coniugio


FINIRE

\end{exmp}



\subsubsection{I problemi della prima lezione visti con i nuovi strumenti}
\begin{exmp}[Problema 1 prima lezione]


\end{exmp}

\begin{exmp}[Problema 2 prima lezione]


\end{exmp}

\begin{exmp}[Problema 3 prima lezione]

Consideriamo un cubo. Scriviamo un numero su ciascuna delle facce e consideriamo l'operazione $T$ che per ogni faccia sostituisce al numero presente la media dei numeri presenti sulle 4 facce del cubo adiacenti. Vogliamo studiare il comportamento dei numeri del cubo quando questa iterazione viene compiuta molte volte.


Cerchiamo di formalizzare il problema usando la teoria della rappresentazione. Possiamo considerare l'insieme $F$ delle facce del cubo\footnote{Che ha quindi 6 elementi}. Una generica configurazione del cubo sarà esprimibile come 

\[ v = \dsum_{f \in F} a_f e_f \]

Dove $a_f \in \mathbb{C}$ e $e_f$ sono una base. L'operatore che sostituisce la media è lineare ma soprattuto commuta con le simmetrie del problema. Ora spiegherò meglio questo concetto.

Consideriamo il gruppo $G$ delle rotazioni del cubo, ovvero 

\[G = \{ g \in SO(3) | g(Cubo) \subset Cubo \} \]

\'E ovvio che il problema è invariante per simmetria, ovvero se $g \in G$, allora vale

\[ T v = g^{-1}T g v \]

Che è la formula di un cambio di base. Questo si può scrivere come 

\[gT = Tg \]

Ovvero ci dice che $\forall g \in G$ le due operazioni commutano. Le due frasi precedenti sono state dette un po' alla garibaldina in quanto non è $g$ ad agire sul cubo ma è una sua rappresentazione di grado $|F| = 6$. Di conseguenza è bene scrivere in modo formale che $\tau: G \to GL(V_\tau)$ è una rappresentazione del gruppo di rotazioni del cubo in $\mathbb{C}^6$ e questa rappresentazione commuta con un operatore $T$, ovvero

\[T\tau(g) = \tau(g) T \qquad \forall g \in G  \]


L'obiettivo che ci poniamo ora è quello di riuscire a scomporre $\tau$ come somma di rappresentazioni irriducibili in quanto una volta trovata una scomposizione 

\[ V_\tau = \bigoplus_{i = i}^n V_{\rho_i} \] 

Allora potremo usare il lemma di Schur per dire che su ogni $V_{\rho_i}$ l'operatore $T$ si comporta come scalare ovvero \emph{è più che diagonalizzato}. Per riuscire a capire qualcosa di come sono fatte le rappresentazioni di questo gruppo è opportuno prima cercare di dare una struttura più chiara a questo gruppo.

\'E possibile mostrare che QUALCUNO CHE HA VOGLIA DI FARLO LO FACCIA PLS $G \cong S_4$. A questo punto noi abbiamo una rappresentazione di grado 6 di $S_4$ che cerchiamo di scomporre come somma di rappresentazioni irriducibili. Tuttavia grazie al teorema \ref{thm: teorema importantissimo} sappiamo che tutte le sottorappresentazioni di $\tau$ saranno isomorfe alle sottorappresentazioni della rappresentazione regolare $\mathcal{R}(S_4)$, di cui abbiamo preventivamente calcolato la tabella dei caratteri \ref{tabella caratteri s4}. Dato che 


\[\tau = \dsum_i n_i\rho_i \Rightarrow \chi_\tau  = \dsum_i n_i\chi_{\rho_i}\]

Andiamo a calcolare i prodotti scalari dei caratteri delle rappresentazioni irriducibili di $S_4$ con il carattere di $\tau$ per trovare quali rappresentazioni compaiono. Per farlo calcoliamo prima il carattere di $\tau$


SCRIVI CHE NON HO VOGLIA

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline


\end{tabular}
\end{table} 



Per cui si ottiene

\[\tau = 1 + \epsilon\rho + \sigma \]

Ovvero

\[V_\tau = V_1 \oplus V_{\epsilon\rho} \oplus V_{\sigma} \]


Cerchiamo quindi di capire come sono fatti questi tre spazi che hanno rispettivaemente dimensione 1,3,2. 


SCRIVI PI\'U DETTAGLIATO CHE DEVO ANDARE A LEZIONE

\[V_1 =  span(e_1 + e_2 + ... + e_6) \]
\[V_{\epsilon\rho} = \text{Le facce opposte hanno numeri opposti}\]
\[V_{\sigma} = \text{Le facce opposte hanno numeri uguali e la somma di tutti è 0}\]

Su questi spazi è facile vedere che effettivamente $T$ è scalare. In particolare

\[
\begin{cases}
T|_{V_1} = 1 \\
T|_{V_{\epsilon\rho}} = 0 \\
T|_{V_\sigma} = -\frac{1}{2}\\
\end{cases}
\]

E quindi è evidente che $T^n \to $ su ogni faccia viene la media dei numeri che c'erano all'inizio.


\end{exmp}











\newpage
\section{Rappresentazioni reali, complesse e quaternioniche}

Ci poniamo un problema nuovo: quand'è che una rappresentazione, che abbiamo sempre definito su $\C$, funziona in modo uguale anche definendola solo su $\R$? Diamo una definizione più precisa

\begin{defn}
Diciamo che una rappresentazione $(\rho, V_\rho)$ del gruppo $G$ è reale se esiste una base di $V_\rho$ tale che

\[ \rho(g) \in M_n(\R) \ \ \forall g \in G\]

\label{def: rappr reale}
\end{defn}
Questa definizione è equivalente a chiedere che $\exists V_0 \subset V$ sottospazio vettoriale reale tale che: $V_0$ sia stabile ($G-$invariante ) e che 
\[ V = \C \otimes_\R V_0 = V_0 \oplus i V_0\]
Ne segue che $dim_\R V_0=dim_\C V$. Non è sempre detto che esista.
\begin{exmp}
Prendiamo come gruppo un gruppo ciclico, per esempio $\Z / 3\Z$\footnote{Che per i fisici è isomorfo a $C_3$}. Evidentemente tutte le rappresentazioni non banali di $G$ non sono reali, in quanto sono di grado 1 e sono le radici dell'unità.

\end{exmp}


\begin{rem}
Supponiamo di avere $\rho: G \to GL(V_\rho)$ con $V_\rho$ spazio vettoriale su $\C$ di dimensione $n$ (dimensione di $V$ pensato come un $\C-$spazio vettoriale). Allora possiamo vederlo come uno spazio vettoriale costruito sul campo dei reali e  e lavorare su quello, se il nostro obiettivo è quello di avere una rappresentazione reale. Quando lo interpreteremo in questo modo scriveremo $V_\R$ (che essendo lo stesso spazio vettoriale su $\R$ avrà dimensione $2n$). 
\end{rem}



\begin{lemma}
Sia $V$ una rappresentazione $/\C$ (tale scrittura significa $V$ visto come spazio vettoriale complesso) che sia però anche reale nel senso prima definito. Allora $V^{\R} $ NON è una rappresentazione irriducibile. 
\end{lemma}

\textsc{Dimostrazione:}

Abbiamo detto che il fatto che $\rho$ possa essere vista come una rappresentazione reale equivale all'esistenza di un $V_0$ che mi faccia  il lavoro prima detto. Bene ma allora quel $V_0$ (essendo reale) lo possiamo in realtà vedere come un sottospazio di $V^{\R}$ $G-$invariante la cui dimensione è 

\[dim_\R V_0=dim_\C V=\frac{1}{2}dim_{\R} V^{\R}\ \ \Rightarrow G\rightarrow GL(V_0) \]

è una sottorappresentazine di $G/\R$.  \qed

Ora andremo a fare una classificazione delle rappresentazioni. Vedremo che ne esistono di 3 tipi:
\begin{itemize}
\item Reali
\item Complesse
\item Quaternioniche
\end{itemize}



La classificazione verrà fatta in base all'esistenza o meno di forme bilineari di un certo tipo invarianti sotto $G$. Vediamo come farlo formalmente.


\begin{thm}

Prendiamo una $(\rho, V_\rho)$ rappresentazione $/\C$ che sia reale: allora:
\begin{enumerate}
\item $\chi_\rho(g)\in \R$ $\forall g\in G$ 
\item $V_\rho$ possiede una forma bilineare simmetrica $G-$invariante. Ovvero lo spazio delle forme bilineari simmetriche $S^2V^*\subset V\otimes V$ è tale che $(S^2V^*)^G\neq 0$.
\end{enumerate} 

\end{thm}

\textsc{Dimostrazione:} Abbiamo supposto la rappresentazione reale. Quindi la matrice è reale ed in particolare lo sarà anche la sua traccia. Quindi il primo punto è vero. Ora veniamo al secondo punto: esisterà un $V_0$ spazio vettoriale reale $G-$invariante che tensorizzato con $\C$ da $V$. Consideriamo ora una forma bilineare simmetrica $B_0$ non degenere $/\R$

\[ B_0 \in S^2 V_0^*\]

Possiamo ora renderla invariante sotto l'azione di $G$ con il solito metodo del fare la media. Consideriamo quindi $\tilde B_0$ definito come


\[ \tilde B_0(v_1, v_2) = \dfrac{1}{|G|} \dsum_{g\in G} B_0(\rho(g) v_1, \rho(g) v_2)\]




Questo ha le caratteristiche precedenti ed è anche invariante sotto $G$ (ovvero $\tilde B_0\in (S^2V_0^*)^G$). Possiamo a questo punto estenderla a forma bilineare su $V$ complessificandola in modo ovvio. Consideriamo quindi $B$

\[B \in (S^2V^*)^G \]

che definiamo sullo spazio $V = V_0 \oplus i V_0$ nel seguente modo

\[B(v_1 + i v_1', v_2 + i v_2') = \left( \tilde B_0(v_1, v_2) - \tilde B_0(v_1', v_2')\right)  + i \left( \tilde B_0(v_1', v_2) + B_0(v_1, v_2')\right)\]




\'E una banale verifica controllare che rispetta le caratteristiche richieste.
\qed


Vediamo ora il seguente lemma che ci servirà per la classificazione.

\begin{lemma}
Sia $(\rho, V_\rho)$ una rappresentazione $/\C$. Allora ogni forma bilineare non nulla $G-$invariante è non degenere (in particolare quindi ne esiste almeno una). Inoltre è unica a meno di scalari, ovvero $dim (V^*\otimes V^*)^G=1$ .
\end{lemma}

\textsc{Dimostrazione:}

Prendiamo un elemento $B$

\[B \in \left( V^* \otimes V^*\right) ^G \]

\'E un fatto di algebra che 

\[\left(V^* \otimes V^*\right)^G \cong Hom(V, V^*)^G \]

A questo punto, se $\phi: V\to V^*$ è un omomorfismo di rappresentazioni, per il lemma di Schur o $\phi$ è nullo o $\phi$ è un isomorfismo. Dato che la forma è non nulla, allora $\phi = \lambda Id$ con $\lambda \in \C$.   \qed 

Sia $V$ una rappresentazione $/\C$ irriducibile: quando è che $\exists$ una forma bilineare $G-$invariante? Ovvero quand'è che si ha $\left(V^* \otimes V^*\right)^G \neq 0$? Cerchiamo delle condizioni necessarie...
\begin{lemma} 
Sia $(\rho, V_\rho)$ una rappresentazione $/\C$ tale che sia reale: allora esiste un isomorfismo tra $V$ e $V^*$ ovvero esiste un isomorfismo tra $\rho$ e $\rho^*$ (a volte confondiamo la rappresentazione con il suo supporto).
\end{lemma}

\textsc{Dimostrazione:}

\[\rho \simeq \rho^* \Leftrightarrow \chi_\rho(g)=\chi_{\rho^*}(g)\ \forall g\in G\]
ma dato che 
\[\chi_{\rho^*}=\overline{\chi_\rho}\]
allora
\[ \chi_\rho(g)=\chi_{\rho^*}(g)\ \forall g\in G\Leftrightarrow \chi_\rho(g)=\overline{\chi_\rho}(g)\ \forall g\in G\]
e questo è vero $\Leftrightarrow $ $\chi_\rho$ è una funzione reale. E ciò è vero se la rappresentazione è definibile $/\R$. \qed 

\begin{lemma}
Sia $(\rho, V_\rho)$ una rappresentazione $/\C$: allora
\begin{itemize}
\item  $B \in (V^* \otimes V^*)^G \Rightarrow B \in S^2V^* \vee B \in \bigwedge ^2 V^*$
\end{itemize}
Definiamo 

\[ m_\rho = \dfrac{1}{|G|} \dsum_{g \in G} \chi_{\rho}(g^2)\]

l'indicatore di Frobenius-Schur. Allora
\begin{itemize}
\item  $m_\rho \in \{-1, 0, 1 \}$
\item{ \begin{enumerate}

\item se $m_\rho = 0 \Rightarrow (V^*\otimes V^*)^G = 0$
\item se $m_\rho = 1 \Rightarrow (S^2 V^*)^G \neq 0$
\item se $m_\rho = -1 \Rightarrow (\bigwedge^2 V^*)^G \neq 0$
\end{enumerate}
}

\end{itemize}
\end{lemma}



\textsc{Dimostrazione:}

Abbiamo visto che $V^*\otimes V^*$ si può decomporre come somma diretta delle potenze simmetriche e alternanti ovvero
\[V^*\otimes V^*=S^2V^*\oplus  \bigwedge ^2 V^*\]
e inoltre per il lemma precedente sappiamo che $V^*\otimes V^*$ ha dimensione 1. 
DEVO FINIRE DI SCRIVERLA 

\begin{defn}[Classificazione sull'indice di Frobenius]
Sia $\rho:G\rightarrow GL(V)$ una rappresentazione $/\C$: essa è detta
\begin{itemize}
\item \textbf{reale} se $m_\rho = 1$  
\item \textbf{complessa} se $m_\rho = 0$ 
\item \textbf{quaternionica} se $m_\rho = -1$
\end{itemize}
\end{defn}


\subsection{Quaternioni}

Ovviamente la parola quaternionica ha a che fare con il corpo dei quaternioni. Vediamo un po' di caratteristiche interessanti di questo oggetto.

Il corpo $\HH$ si può vedere come

\[\HH = \R \oplus i \R \oplus j \R \oplus k \R \]

Con $i,j,k$ unità immaginarie che rispettano le seguenti regole

\[ 
\begin{cases}
i^2 = j^2 = k^2 = -1 \\
ij = - ji = k \\
jk = -kj = i \\
ki = - ik = j \\
\end{cases}
\]


Vediamo un po' di proprietà interessanti. Per esempio se consideriamo 

\[ Q_8 =  \{\pm 1, \pm i, \pm j, \pm k \}\]

allora questo insieme è un gruppo se munito della moltiplicazione. Possiamo andare a vedere la tabella dei caratteri di questo gruppo. 


Per farlo possiamo cercare i sottogruppi normali $H_i$ di $Q_8$ e quozientare rispetto a quelli per cercare in modo facile delle rappresentazioni di grado 1. L'ultima rappresentazione che si trova deve avere dimensione $2$ perché $\sum n_i^2 = |G|$. Vedremo poco sotto che cosa significa questa rappresentazione.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 1 & 2 & 2 & 2 \\
$Q_8$ & 1 & -1 & $\pm i$ & $\pm j$ & $\pm k$ \\
\hline
$\rho_1$ & 1 & 1 & 1 & 1 & 1 \\
\hline
$\rho_i$ & 1 & 1 & 1 & -1 & -1 \\
\hline
$\rho_j$ & 1 & 1 & -1 & 1 & -1 \\
\hline
$\rho_k$ & 1 & 1 & -1 & -1 & 1 \\
\hline
$\rho_\HH$ & 2 & -2 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $Q_8$}
\label{tab: caratteri q8}
\end{table}













\'E interessante notare che la tabella dei caratteri di $Q_8$ è uguale a quella di $D_4$, ma i due gruppi non sono isomorfi. Questo ci ricorda che la tabella dei caratteri dice tanto di un gruppo ma non tutto.


COSE RANDOM SCRITTE DI FRETTA PERCH\'E devo andare a lezione

%~ \[\HH = \left\{ \left \left(\begin{array}{cc} z & w \\ -\overline{w} & \overline z \end{array}\right) \right| z, w \in \C \right\} \]




Matrici di spin di Pauli


\[ 
1_\HH = 
\left(
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right)
\qquad
i_\HH = 
\left(
\begin{array}{cc}
i & 0 \\
0 & -i \\
\end{array}
\right)
\qquad
j_\HH = 
\left(
\begin{array}{cc}
0 & 1 \\
-1 & 0 \\
\end{array}
\right)
\qquad
k_\HH =
\left(
\begin{array}{cc}
0 & i \\
i & 0 \\
\end{array}
\right)
\qquad
\]



\begin{lemma}
Sia $(\rho, V_\rho)$ una rappresentazione irriducibile su $\C$. Allora $\rho$ è reale secondo la definizione \ref{def: rappr reale} $\Leftrightarrow m_\rho = 1$ 
\end{lemma}

\textsc{Dimostrazione:} $\Rightarrow)$ già fatto.

$\Leftarrow)$ Sia $B \in (S^2V_\rho^*)^G$, con $dim_\C V_\rho = n$. Noi stiamo cercando un certo $V_0 \subset V_\rho$ spazio vettoriale su $\R$ tale che $V_\rho = V_0 \oplus i V_0$ 

Prendiamo ora una certa forma $h : V_\rho \times V_\rho \to \C$ hermitiana, definita positiva e $G-$invariante. Questa sicuramente esiste, è stato dimostrato nel teorema METTI IL LINK,

Definiamo a questo punto un endomorfismo di $V_\rho$ che chiamiamo $\phi: V_\rho \to V_\rho$, definito come


\[ B(x, y) = h(\phi(x), y)\] 

Questa definizione ha senso per $y$ fissato (per Riesz).

Che proprietà ha $\phi$? Possiamo notare che $\phi$ è $G$-equivariante, ovvero vale

\[ \phi(\rho(g)x) = \rho(g) \phi(x)\]

Mostriamolo rapidamente

\[ h(\phi(\rho(g)x) , y) = B(\rho(g)x, y) = B(x, \rho(g^{-1} ) y) = h (\phi(x), \rho(g^{-1} ) y) = h(\rho(g)x, y)\]

Questo non è male, in quanto se $\phi$ fosse lineare avremmo subito che $\phi$ è un omomorfismo di rappresentazioni irriducibili e potremmo usare Schur. Tuttavia

\[ \phi(z_1 x_1 + z_2 x_2) = \overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho\]

SCRIVI PERCH\'E

E quindi purtroppo $\phi$ non è davvero lineare. Però dato che in mezzo c'è  solo il coniugio, possiamo provare a vedere cosa fa $\phi^2$

\[\phi^2(z_1 x_1 + z_2 x_2) = \phi(\overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2))  = z_1 \phi^2(x_1) + z_2 \phi^2(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho \] 


E quindi effettivamente $\phi^2$ è un omomorfismo di rappresentazioni irriducibili. Per questo motivo possiamo applicare Schur e concludere che 

\[ \phi^ 2 = \lambda Id_{V_\rho} \qquad \lambda \in \C\]


Cosa possiamo dire su $\lambda$? Il claim è che sia $\lambda \in \R$ e $\lambda > 0$. Vediamo come si mostra

\[h(\phi(x), y) = B(x, y) = B(y, x) = h(\phi(y), x) = \overline{h(x, \phi(y))} \]
usando questo fatto possiamo considerare 

\[ \lambda h(x, y) = h(\phi^2(x), y) = \overline{h(\phi(x), \phi(y))} = h(x, \phi^2(y)) = \overline{\lambda} h(x, y) \qquad \forall x, y \in V_\rho\]

E questo ci dice ovviamente che $\lambda \in \R$. Per mostrare ora che $\lambda > 0$ dobbiamo sfruttare il fatto che la nostra forma hermitiana sia definita positiva. Per questo motivo andiamo a considerare

\[\lambda h(x, x) = h (\phi^2(x), x) = \overline{h(\phi(x), \phi(x))} \Rightarrow \lambda = \dfrac{\overline{h(\phi(x), \phi(x))}}{ h(x,x)} \qquad \forall x \neq 0 \in V_\rho\]

E dato che $h $ è definita positiva si ha anche $\lambda > 0$


A questo punto possiamo (a meno di riscalare) scegliere $\lambda = 1$, ovvero $\phi^2 = Id$. A questo punto ci piacerebbe tornare a fare cose con $\phi$ e non $\phi^2$. Notiamo che se ci restringiamo a spazi vettoriali su $\R$, allora anche $\phi$ è lineare in quanto il coniugio non ci dà fastidio. Dato che quindi $\phi$ è un endomorfismo di uno spazio vettoriale reale tale che $\phi^2=1$, allora $\phi$ è diagonalizzabile e ha solo gli autovalori $\pm 1$.  Per questo motivo possiamo scomporre lo spazio di partenza $V_\rho = V_+ \oplus V_-$, con ovvia notazione per gli autospazi.


A questo punto ci manca poco. $V_+$ e $V_-$ sono sottospazi reali del nostro spazio di partenza. Se mostriamo che sono isomorfi, abbiamo trovato la nostra scomposizione dello spazio $V_\rho$ in due spazi 

\[ V_\rho = V_0 \oplus iV_0\]


Proprio per questo motivo è intelligente notare che vale

\[ i V_+ = V_- \]

Mostriamo perché con il solito trucco della doppia inclusione. Prendiamo per esempio $x \in V_+$. Allora 

\[ \phi(ix) = - i \phi(x) = -i x \]

Ovvero il vettore $ix$ è autovettore di $\phi$ con autovalore $-1$. Applicando due volte questo ragionamento si ottiene facilmente

\[ V_+ \cong V_- \qquad (i V_+ = V_-)\]

Per cui a questo punto abbiamo finito \qed



Ora vorremmo effettivamente capire il perché dei nomi dati nella classificazione delle rappresentazioni come reali, complesse e quaternioniche. Per questo motivo ci servono un paio di concetti di algebra.


\begin{defn}[Algebra]
Un'algebra su $\R$ è uno spazio vettoriale reale $A$ dotato di una moltiplicazione $\cdot : A \times A \to A$ che sia associativa e bilineare.
Inoltre imponiamo che vi sia un elemento neutro rispetto a questa moltiplicazione. Quest'ultima richiesta non fa parte della
più generale definizione di algebra (le algebre che la soddisfano si dicono \emph{unitarie}), ma noi la inseriamo nella definizione
poiché in questo corso non tratteremo mai algebre non unitarie.
\end{defn}


\begin{defn}[Algebra di divisione]
Un algebra di divisione è un'algebra in cui ogni elemento escluso lo $0$ possiede un inverso moltiplicativo.
\end{defn}

\begin{exmp}
Gli esempi più standard di algebra di divisione su $\R$ di dimensione finita sono i campi $\R$ e $\C$ come spazi vettoriali reali.
Un esempio più sofisticato è dato dal corpo dei quaternioni $\HH$ (ovviamente visto come spazio vettoriale su $\R$).
\end{exmp}


\begin{rem}
Consideriamo una rappresentazione irriducibile $\rho:G\to GL(V_\rho)$ con $V_\rho$ spazo vettoriale su $\R$. Allora l'insieme degli endomorfismi
di $\rho$ 
\[ End_G(V_\rho)\]
è un'algebra di divisione su $R$ se dotato della composizione.
Infatti per lemma di Schur (in particolare la prima parte dell'enunciato, che vale su ogni campo) ogni elemento di $End_G(V_\rho)$ o è la funzione nulla oppure è un isomorfismo, quindi ammette inverso.
Se $\dim V_\rho = n$ allora $End_G(V_\rho)$, essendo contenuto in $End(V_\rho)$, ha dimensione finita, in particolare $\dim End_G(V_\rho)\le n^2$.
Vedremo tuttavia che vale una limitazione molto più forte.
\end{rem}


Presentiamo ora un sorprendente teorema che afferma che non ci sono altre algebre di divisione di dimensione finita su $\R$ oltre a quelle che abbiamo elencato come esempi, ovvero $\R, \C, \HH$.
\begin{thm}[di Frobenius]
\label{thm: frobenius}
Sia $A$ un'algebra di divisione su $\R$ di dimensione finita. Allora si ha 
\[A \cong \R \quad \vee \quad  A \cong \C \quad \vee \quad A  \cong \HH \]
\end{thm}

\textsc{Dimostrazione:}
Indichiamo con $\bm{1}\in A$ l'elemento neutro rispetto alla moltiplicazione di $A$ (occhio: è un vettore di $A$, non un numero reale).
Il sottospazio vettoriale generato da $\bm{1}$ è chiaramente isomorfo ad $\R$ come $\R-$spazio vettoriale, ma in
realtà lo è anche come sottoalgebra: infatti se $a,b\in\R$ allora
\[a\bm{1} \cdot b\bm{1} = ab (\bm{1}\cdot \bm{1}) = ab\bm{1} \]
dove nel primo passaggio abbiamo usato la bilinearità e nel secondo il fatto che $\bm{1}$ è elemento neutro.
Dunque, con un lieve abuso di notazione, possiamo scrivere $\R \subseteq A$, intendendo per $\R$ proprio il sottospazio generato da $\bm{1}$.
Se $\dim A = 1$ allora sarebbe $A=\R$. D'ora in poi supponiamo $\dim A > 1$.

Vogliamo ora mostrare che esiste una sottoalgebra di $A$ isomorfa a $\C$.
Sia $\alpha\in A \setminus\R$ e indichiamo con $A[\alpha]$ la sottoalgebra generata da $\alpha$, ovvero consideriamo l'insieme
\[A[\alpha] = \left\{ \dsum_{n = 0} ^N a_n \alpha^n |\ N\in\mathbb{N}, \ a_i \in \R,\ \alpha \in A   \right\} \]
L'algebra $A[\alpha]$, essendo contenuta in $A$, ha necessariamente dimensione finita, quindi esisterà un certo $N$ per cui gli elementi
$\alpha^0, \alpha^1, \dots \alpha^N$ sono linearmente dipendenti. Più precisamente prendiamo il minimo $N$ per cui questo avviene.
Allora esistono dei coefficienti $a_n$ reali tali che
\[ \dsum_{n=0}^N a_n \alpha^n = 0 \]
Ovvero il polinomio a coefficienti reali
\[ p(x) = \dsum_{n = 0}^N a_n x^n \] 
è annullato da $\alpha$.
Se potessimo scomporre in modo non banale $p(x) = p_1(x)p_2(x)$ allora avremmo che $0 = p_1(\alpha)p_2(\alpha)$, dunque $\alpha$ annullerebbe uno dei due fattori (qui
stiamo usando che $A$ è un'algebra unitaria), il che sarebbe assurdo vista l'ipotesi di minimalità di $N$.
Dunque $p(x)$ deve essere irriducibile. Ma su $\R$ i polinomi irriducibili possono solo avere grado $1$ o $2$.
Vediamo rapidamente perché non può avere grado $1$.
Supponiamo per assurdo che sia
\[p(x) = a_0 + a_1 x\]
Ma ciò vorrebbe dire che $a_0+a_1\cdot\alpha=0$, ovvero $\alpha=-a_0/a_1 \in \R$, ma noi avevamo assunto $\alpha\not\in\R$.
Di conseguenza $p(x)$ ha grado esattamente $2$.
Non è difficile mostrare che $A[\alpha]$ ha esattamente dimensione $2$: se $\alpha^2$ si scrive in termini di potenze inferiori lo faranno anche tutte le potenze successive,
dunque ogni elemento di $A[\alpha]$ si può scrivere nella forma $x + \alpha y$ con $x,y$ reali e $\{1,\alpha\}$ è base di $A[\alpha]$, essendo i due elementi indipendenti.
A meno di moltiplicare $p(x)$ per una costante per renderlo monico, possiamo scrivere 
\[ p(x) = (x-s)^2+t^2\]
con $s,t$ reali, $t\neq 0$. Se definiamo
\[i = \frac{\alpha-s}{t}\]
è facile osservare che $i^2 = -1$. Ora $\{1, i\}$ è base di $A[\alpha]$ (essendo $1$ e $i$ linearmente indipendenti) e il nostro $i$
gioca esattamente lo stesso ruolo dell'unità immaginaria in $\C$. A questo punto è immediato esibire un isomorfismo
\[ A[\alpha] \cong \C \]
Se $\dim A =2 $ abbiamo finito. Supponiamo d'ora in poi $\dim A > 2$ e scriviamo $\C\subset A$ identificando $\C$ con la
sottoalgebra $A[\alpha]$. Definiamo ora un'applicazione $\varphi:A\to A$ nel modo seguente:
\[ \varphi(x) = -ixi = ixi^{-1} \]
Osserviamo che $\varphi$ è $\R-$lineare, infatti presi $a,b\in\R$ e $x,y\in A$ si ha:
\[ \varphi(ax+by) = i(ax+by)i^{-1} = i(ax)i^{-1} +i(by)i^{-1} = a\varphi(x) + b\varphi(y)\]
Inoltre osserviamo che $\varphi^2$ è l'identità. Questo implica che possiamo decomporre
\[ A = A_+ \oplus A_-\] dove $A_+$ e $A_-$ sono gli autospazi relativi rispettivamente agli autovalori $1$ e $-1$.
In particolare gli elementi di $A_+$ sono esattamente quelli che commutano con $i$.
Vogliamo ora mostrare che $A_+ = \C$.
Sia $\beta\in A_+$. Imitando il ragionamento fatto prima con $\alpha$, possiamo considerare il polinomio a coefficienti complessi di minimo grado
che si annulla in $\beta$. Come fatto prima osserviamo che deve essere irriducibile (questo passaggio richiede in realtà qualche cautela,
ma tutto funziona come deve poiché $\beta$ commuta con tutti gli elementi di $\C$. Se avessimo preso $\beta\in A_-$ il ragionamento sarebbe errato).
Ma gli unici polinomi irriducibili a coefficienti in $\C$ sono quelli di grado $1$, quindi $\beta\in\C$.

Abbiamo dunque stabilito che $A_+ = \C$. Dato che $\dim A > 2$ possiamo prendere $z\in A_-$ non nullo.
Consideriamo l'applicazione $\psi_z:A\to A$ definita da $\psi_z(x) = zx$.
Osserviamo che valgono le seguenti implicazioni:
\[ x\in A_+ \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = -zx = -\psi_z(x)\]
\[ x\in A_- \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = (-z)(-x) = \psi_z(x)\]
ovvero $\psi_z$ scambia $A_+$ e $A_-$. Inoltre $\psi_z$ è bigettiva e lineare, dunque concludiamo che $A_+\cong A_-$ come $\R-$spazi vettoriali e in particolare $\dim A = 4$.

Con un ragionamento simile a quello che avevamo fatto per $\alpha$, osserviamo che $z^2$ è nel sottospazio generato da $1$ e $z$. Inoltre $z^2=\psi_z(z)\in A_+$.
Visto che $Span(1,z) \cap A_+ = \R$ deve essere per forza $z^2\in\R$.

Se fosse $z^2 \ge 0$ allora potremmo scrivere $z^2=r^2$ per qualche $r\in\R$.
Visto che $r$ e $z$ commutano sarebbe allora $(z-r)(z+r)=0$, dunque uno dei due fattori sarebbe $0$, assurdo perché $z\not\in\R$. Pertanto $z^2 < 0$ e possiamo definire
\[j = \frac{z}{\sqrt{-z^2}}\]
così $j^2 = -1$. Infine definiamo $k = ij$.

Ora $k$ e $j$ sono indipendenti e sono in $A_-$, dunque formano una base di $A_-$. Allora $\{1,i,j,k\}$ è base per $A$ ed è facile
verificare che questi quattro elementi rispettano le stesse regole di moltiplicazione dei quaternioni, pertanto si conclude che $A\cong\HH$ e la tesi è dimostrata.\qed

Dal teorema precedente si ha immediatamente che se $G\to GL(V)$ è una rappresentazione irriducibile su $\R$ allora $End_G(V)$ è isomorfo a uno
tra $\R, \C, \HH$.

\begin{prop}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora:
\begin{itemize}
\item $End_G(V) \cong \R$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 1$
\item $End_G(V) \cong \C$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 2$
\item $End_G(V) \cong \HH$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 4$
\end{itemize}
\end{prop}
\textsc{Dimostrazione: }
Basta ricordare che $\dim End_G(V) = \langle\chi_\rho|\chi_\rho\rangle$ e la tesi segue subito
dal teorema di Frobenius.
\qed

\begin{lemma}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora $\dim V^G = \langle\chi_\rho|\chi_1\rangle$
\end{lemma}
\textsc{Dimostrazione: }
Definiamo un'applicazione lineare $R:V\to V$ nel seguente modo:
\[ R = \frac{1}{|G|} \sum_{g\in G}{\rho(g)} \]
Se prendiamo $v\in V$ e $s\in G$ allora
\[ \rho(s) R(v) = \rho(s)\frac{1}{|G|} \sum_{g\in G}{\rho(g)v} = \frac{1}{|G|} \sum_{g\in G}{\rho(sg)v} = R(v)\]
ovvero $R(v)\in V^G$.
Inoltre se $w\in V^G$ allora $R(w)=w$. Di conseguenza $R^2 = R$ (cioè $R$ è una cosiddetta \emph{proiezione})
e quindi $R$ si diagonalizza con autovalori $0$ e $1$. Inoltre l'autospazio relativo a $1$ è proprio $V^G$.
Ciò implica che
\[ \dim V^G = tr(R) = \frac{1}{|G|} \sum_{g\in G}{\chi_\rho(g)} = \langle\chi_\rho|\chi_1\rangle \]
\qed









\newpage
\section{Rappresentazioni indotte}
Supponiamo di avere $\rho:H\rightarrow GL(V)$ una rappresentazione di $H<G$ con $G$ gruppo. Il nostro aim è quello di definire una $\widetilde{\rho}:G\rightarrow GL(V)$ rappresentazione di $G$ che estende $\rho$.

\begin{defn}Indichiamo con $induzione\ da\ H\ in \ G\ di W$ il seguente spazio vettoriale:
\[Ind_H^G(W)=\{f:G\rightarrow W: \rho(h)\circ f(gh)=f(g)\ \forall (g,h)\in G\times H\}   \]
\end{defn}

\textbf{Osservazione:} $Ind_H^G(W)$ è lo spazio vettoriale di una rappresentazione di $G$.
Si consideri $N=\{f:G\rightarrow W\}$: definisco
\[\rho_N:G\rightarrow GL(N) \textup{ tale che }(\rho_N(g)f)(g')=f(g^{-1}g')\]
Si verifica che $\rho_N$ è una rappresentazione di $G$ su $N$. Notiamo innanzitutto che $Ind_H^G(W)\subseteq N$ e che tale sottospazio è $\rho_N-$invariante poichè detta $f\in Ind_H^G(W)$ si ha che
\[ \rho_N(g)(f)(g')=f(g^{-1}g')=\rho(h)f(g^{-1}g'h)=\rho(h)\rho_N(g)(f)(g'h)\]
ovvero $\rho_N(g)f\in Ind_H^G(W)$. Questo ci dice che $Ind_H^G(W)$ è una sottorappresentazione di $\rho_N$ chr chiamiamo $\rho_{ind}:G\rightarrow GL(Ind_H^G(W))$.

\begin{exmp}
$H=\{e\}$ e $W=\C$ con $\rho:H\rightarrow GL(\C)$: allora
\[Ind_H^G(W)=\{f:G\rightarrow \C:\rho(e)f(gh)=f(g)\}=\{f:G\rightarrow \C\}\]
\end{exmp}

\textbf{Notazione:} Spesso indicheremo lo spazio vettoriale $\{f:G\rightarrow \C\}$ con $\C[G]=\bigoplus_{g\in G}\C e_g$ dove gli $\{e_g\}_{g\in G}$ sono una base di $\{f:G\rightarrow \C\}$. 
In tal modo risulta che 
\[\{f:G\rightarrow \C\}=\C[G]\otimes W\]
sempre nel senso che se prendo $\{e_g\}_{g\in G}$ una base di $\C[G]$ e $\{w_i\}\in W$ una base di $W$ allora 
\[e_g\otimes w_i:G\rightarrow W \textup{ tale che } g\mapsto w_i \textup{ e } g'\mapsto 0\ \forall g'\neq g\]

Torniamo a $\rho_{ind}$: ricapitolando $\rho_{ind}:G\mapsto GL(Ind_H^G)$ tale che $g\mapsto \rho_{ind}(g)$ dove $\rho_{ind}(g)f(g')=f(g^{-1}g')$. Vediamo ora un po' delle sue caratteristiche. Poniamo per snellire la scrittura $V=Ind_H^G(W)$. Ricordandoci il nostro aim, di certo vogliamo che $W$ sia un sotospazio di $V$.

\begin{defn}
Data $f\in \{f:\C\rightarrow W\}$ (che contiene $V$) definiamo il supporto di $f$ 

\[Supp(f)=\{f\in G:f(g)\neq 0\}.\]
\end{defn}

\begin{defn}
Dato $g\in G$ definiamo $V_g=\{f\in V : Supp(f)\subseteq gH \}$.
\end{defn}

\textbf{Osservazione:} se $g'\in gH$, allora $g'H=gH\Rightarrow V_g=V_{g'}$. Quindi più che dipendere dagli elementi di $G$, gli insiemi sopra definiti dipendono dalle classi laterali di $H$ in $G$. Perciò ora parleremo non più di $V_g$ bensì di $V_{gH}$.\\
\textbf{Osservazione:} dato che le classsi laterali di $H$ sono una partizione di $G$, dette $\{g_iH\}_{i\in |G/H|}$ i rappresentanti delle classi laterali $gH$ al variare di $g$ in $G$ allora 
\[G=\bigsqcup_{i=1}^{|G/H|} g_iH\] 
e quindi ogni funzione la posso scrivere come combinazione lineare di $f_i\in V_{g_iH}$.
\[\Rightarrow V=\bigotimes_{i=1}^{|G/H|} V_{g_iH}\]
Voglio ora rintracciare $W$, in quanto sono interessata a trovare un sottospazio isomorfo a $W$ dento $V$. Notiamo innanzitutto che la scrittura precedente non è detto che sia una decomposizione di rappresentazioni, ma possiamo vedere subito che si comporta bene sotto l'azione di $\rho_{ind}$. 
\begin{lemma}
$\rho_{ind}$ permuta le varie $V_{gH}$ nel senso che $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. 
\end{lemma}
\textsc{Dimostrazione:} Data $f\in V$, definisco
\[f_i(g)=\begin{cases}
f(g)& \textup{se } g\in g_iH\\
0& \textup{altrimenti}
\end{cases}\Rightarrow f=\sum_{i=1}^{|G/H|} f_i\ \textup{ e per come le ho definite } f_i\in V_{g_iH}\]
Sia ora $f\in V_{g'H}$ e fissiamo un $g\in G$ allora
\[\rho_{ind}(g)(f)(x)=f(g^{-1}x)\Rightarrow \rho_{ind}(g)(f)(x)\neq 0\leftrightarrow g^{-1}x\in Supp(f)\subseteq g'H\leftrightarrow x\in gSupp(f)\subseteq gg'H\]
Ovvero $x\in Supp(\rho_{ind}(g)(f))\Leftrightarrow x\in gSupp(f)$. Ciò implica che
\[ V_{g'H} \begin{matrix}
\overset{\rho_{ind}(g)}{\longrightarrow}\\ 
\underset{\rho_{ind}(g^{-1})}{\longleftarrow}
\end{matrix} V_{gg'H} \]
Essendo $\rho_{ind}(g),\rho_{ind}(g^{-1})\in GL(V)$ allora seguono iseguenti fatti:
\begin{enumerate}
\item $\rho_{ind}(g)V_{g'H}\subseteq V_{gg'H}$;
\item $\rho_{ind}(g^{-1})V_{gg'H}\subseteq V_{g'H}$;
\item $V_{g'H}\cong V_{gg'H}$, ovvero sono due spazi vettoriali isomorfi;
\item $\rho_{ind}(g)^{-1}=\rho_{ind}(g^{-1})$;
\end{enumerate}
Quindi $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. \qed
\\ 
\begin{cor}
\[\forall i,j\in |G/H|\ \ \ V_{g_iH}\cong V_{g_jH}.\]
In particolare hanno tutti la stessa dimensione e quindi
\[dim(V)=|G/H|dim(V_{eH})=dim(Ind_H^G(W))\]
\end{cor}
Concentriamoci ora su $V_{eH}=\{f\in V:supp(f)\subseteq H\}$. Sia $h\in H$: allora $\rho_{ind}(h):V_{eH}\rightarrow V_{eH}$ in quanto $eH=hH$. Ciò significa che $\rho_{ind}|_{H}$ definisce una rappresentazione di $H$ in $V_{eH}$.
\begin{lemma}
La rappresentazione appena trovate di $H$ su $V_{eH}$ è naturalmente isomorfa a\\ $\rho:H\rightarrow GL(W)$.
\end{lemma}
\textsc{Dimostrazione:} Considero la seguente funzione: 
\[\Phi:V_{eH}\rightarrow W\ \textup{tale che } f\mapsto f(e)\]
ovvero la funzione che associa ad un $f$ la valutazione della stessa nell'elemento neutro. Verifichiamo innanzitutto che si tratta di un omomorfismo di rappresentazione. Ricordiamo chi è $V_{eH}$:
\[V_{eH}=\{f:G\rightarrow W: \rho(h)f(gh)=f(g)\ \forall h,g\ \ \textup{e } Supp(f)\subseteq H\}\]
$\Phi$ è un omo. di rappr. sse $\Phi(\rho_{ind}(h)f)=\rho(h)\Phi(f)\ \forall h\in H$.
\[\Phi(\rho_{ind}(h)f))=\rho(h)\Phi(f) \Leftrightarrow (\rho_{ind}(h)f)(e)=\rho(h)(f(e))\Leftrightarrow f(h^{-1}e)=f(h^{-1})=\rho(h)f(e)\]
ma ciò è vero in quanto le $f\in Ind_H^G(W)$. \\
Verifichiamo che $\Phi$ è iniettiva. $Supp(f)\subseteq H$ e $\forall h\in H\ f(h)=\rho(h^{-1}f(e))$: questo mi dice che una volta che si è fissato il valore di $f(e)$ allora la funzione è univocamente determinata su $H$. D'altra parte so che in $G\setminus H$ la funzione è identicamente nulla $\Rightarrow$ quindi è univacamente determinata su tutto $G$. Allora detti $\Phi$ è iniettiva.\\
Vediamo ora che è anche surgettiva. Sia $w\in W$ vogliamo $f\in V_{eH}$ che valutata in $e$ dia $w$. Definiamo $f_w:G\rightarrow W$ nel seguente modo: 
\[f_w:=\begin{cases}
\rho(h^{-1})(w)& \textup{ se } h\in H\\
0& \forall g\in G\setminus H
\end{cases}\]
Si verifica che tale funzione soddisfa le proprietà richieste. \qed \\
Dunque possiamo ora concludere il seguente teorema
\begin{thm} Dette $g_1H,...,g_{|G/H|}H$ i rappresentanti delle classi laterali allora 
\[Ind_H^G(W)=\bigoplus_{i=1}^{|G/H|}\rho_{ind}(g_i)V_{eH}\bigl( =\bigoplus_{i=1}^{|G/H|} V_{g_iH}\bigr) \]
Inoltre $\rho_{ind}|_H$ definisce una rappresentazione di $H$ su $V_{eH}$ isomorfa a $\rho$. In particolare quindi 
\[dim(Ind_H^G(W))=|G/H|dim(W)\] 
\end{thm}
Vediamo ora degli esempi.
\begin{enumerate}
\item Abbiamo visto prima $H=\{e\}$ $W=\C$ allora $Ind_H^G(W)=\C[G]$ è la rappresentazione regolare di $G$. 
\item $H\neq \{e\},\subseteq G$ e $\rho:H\rightarrow GL(W)$ la rappresentazione banale con $W=\C$. Allora
\[Ind_H^G(\C)=\{f:G\rightarrow \C:\rho(h)f(gh)=f(gh)=f(g)\ \forall g,h\}\Rightarrow f \textup{ è costante sulle classi laterali di} H\]
\[\Rightarrow Ind_H^G(\C)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di }\ H\}=\C[G/H]\]
\begin{tikzpicture}
\matrix(m)[matrix of math nodes,row sep=3em,column sep=3em,minimum width=3em]
{
f:G& \C\\
G/H& \\};
\path[-stealth]
(m-1-1) edge (m-1-2)
(m-1-1) edge node [left] {$\pi$} (m-2-1)
(m-2-1) edge node [right] {$\ f$} (m-1-2);
\end{tikzpicture}\\
$G/H$ è un insieme finito dove $G$ agisce per moltiplicazione a sinistra ($g(g'H)=gg'H$). Quindi $Ind_H^G(W)$ è la rappresentazione per permutazione associata all'azione di $G$ in $G/H$. \\
\textbf{Recall:} $X$ è un $G-$insieme, sia $X= \bigsqcup X_i$ la decomposizione in $G-$orbite: allora 
\[\C[X]=\bigoplus_{i=1}^{\#-orbite} \C[X_i]\] 
\item $H\subseteq G$ e $\rho:H\rightarrow GL(W): \rho(h)=id\ \forall h\in H$. Nel caso in cui $W$ avesse dimensione 1 si torna all'esempio 1. Tuttavia anche se $dim(W)>1$, $Ind_H^G(W)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di } H\}=\C[G/H]\otimes W$ 
\end{enumerate}
\textbf{Esercizio:} Supponiamo che $W=W_1\bigoplus W_2$ una rappresentazione di H non riducibile: allora 
\[Ind_H^G(W)=Ind_H^G(W_1)\bigoplus Ind_H^G(W_2)\].


\subsection{Formula di aggiunzione}
Vediamo ora una proprietà universale di queste rappresentazioni indotte. Prima abbiamo definito passando da $\rho$ a $\rho_{ind}$ sostanzialmente una applicazione
\[Ind:Rappr(H)\rightarrow Rappr(G)\]
e abbiamo anche "l'inversa" ovvero la restrizione della rappresentazione ad H
\[Res:Rappr(G)\rightarrow Rappr(H)\]
e risulta che $Res_H^G(V)=V$ visto però come una rappresentazione di $H$ ($\rho_{res}=\rho|_H$). Che legame c'è tra queste due applicazioni? 
\begin{thm}
Sia $\rho:H\rightarrow GL(W)$ una rappresentazione di $H$ e sia $\sigma:G\rightarrow GL(U)$ una rappresentazione di $G$. Allora
\[Hom_H(W,Res_H^G(U))\cong Hom_G(Ind_H^G(W),U)\]
ovvero che ogni $\phi:W\rightarrow Res_H^G(U)$ omomorfismo di $H-$rappresentazione si estende in modo unico a un $\overset{\sim}{\phi}:Ind_H^G(W)\rightarrow U$ omomorfismo di $G-$rappresentazione
\end{thm}
\textsc{Dimostrazione:} sia $\phi:W\rightarrow Res_H^G(U)$ un omomorfismo di $H-$rappresentazione e sia $V:=Ind_H^G(W)=\bigotimes_{i=1}^{\#-orbite} \rho_{ind}(g_i)V_{eH}$. Essendo $V_{eH}\cong W$ come $H-$rappresentazioni, allora $\phi:V_{eH}\rightarrow Res_H^G(U)$: voglio estenderlo a $V$. Essendo $V$ definito come una somma diretta è sufficiente definire l'omomorfismo sui blocchi e per questo considero $V_{g_iH}$: ho che
\[V_{g_iH}\overset{\rho_{g_i^{-1}}}{\rightarrow} V_{eH}\overset{\phi}{\rightarrow} Res_H^G(U)\overset{\sigma(g)}{\rightarrow} U\Rightarrow \sigma(g)\phi\rho_{ind}(g_i^{-1}):V_{g_iH}\rightarrow U\] 
Incollando tutte queste applicazioni ottengo $\overset{\sim}{\phi}:Ind_H^G(W)\rightarrow U$ dove
$\overset{\sim}{\phi}(v)=\sigma_{g}\circ \phi\circ \rho_{ind}(g^{-1})(v)$ con $v\in V_{gH}$. Si verifica che $\overset{\sim}{\phi}\in Hom_G(Ind_H^G(W),U)$. \\
Vediamo ora che questa estensione è unica. Siano $\overset{\sim}{\phi_1}$ e $\overset{\sim}{\phi_2}$ due estensioni di $\phi$: valutiamole nello stesso elemento $v\in V_{gH}$.
\[\overset{\sim}{\phi_1}(\rho_{ind}(g^{-1})v)=\phi(\rho_{ind}(g^{-1})(v))=\overset{\sim}{\phi_2}(\rho_{ind}(g^{-1})v)\]
dove l'uguaglianza è vera perchè $\rho_{ind}(g^{-1})(v)\in V_{eH}$. Quindi coincidendo sui singoli blocchi le due estensioni coincidono anche su $V$. E quindi l'estensione è unica. \qed















































\end{document}
