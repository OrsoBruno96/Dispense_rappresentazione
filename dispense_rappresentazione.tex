
\documentclass[11pt]{article}


\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[a4paper]{geometry}
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{subfig}
\usepackage{array}
\usepackage{xy}
\usepackage{multicol}
%\usepackage{slashbox}
\usepackage{fancyhdr}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[T1,OT1]{fontenc} 
\usepackage[nohug,small]{diagrams}
\usepackage{bm}


\usepackage{grffile}
\usepackage{tikz}
\usepackage{pgf,tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc}

\usetikzlibrary{arrows}
\topmargin 0cm
\oddsidemargin 0cm
\evensidemargin 0cm
\textwidth 16.5cm
\textheight	23.5cm
\marginparwidth 2cm
\marginparpush 2cm



\title{Dispense del corso di Teoria della Rappresentazione}
\author{Fabio Zoratti}
\date{\today}



\makeindex

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{post}[thm]{Postulato}
\newtheorem*{cor}{Corollario}

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{prob}{Problema}[section]
\newtheorem{hint}{Suggerimento}[section]
\newtheorem{sol}{Soluzione}[section]
\newtheorem*{rem}{Osservazione}

\theoremstyle{remark}
\newtheorem*{note}{Nota}





\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}



\newcommand{\tridiag}[6]{
	  \begin{diagram}
	  #1 & \rTo^{#2}  & #3        \\
	     & \rdTo_{#6} & \dTo>{#4}   \\
	     &          & #5
	  \end{diagram}
}  
\newcommand{\quaddiag}[8]{
	\begin{diagram}
	#1     & \rTo^{#2} & #3 \\
	\dTo<{#6} &         & \dTo>{#4} \\
	#7     & \rTo^{#8} & #5
	\end{diagram}
}
















\begin{document}
\maketitle
\tableofcontents



\newpage
\section{Teoria dei gruppi}

\begin{defn}[Gruppo] Un gruppo è un insieme dotato di un'operazione binaria $\cdot : G\times G \to G$ che gode delle seguenti proprietà:
\begin{enumerate}
	\item Associatività: presi comunque $a,b,c\in G$ vale che $(a\cdot b)\cdot c = a\cdot(b\cdot c)$
	\item Esiste $e\in G$, chiamato \emph{unità}, o \emph{identità}, o \emph{elemento neutro}, tale che $\forall a\in G$ vale $e\cdot a = a = a\cdot e$
	\item Per ogni $a\in G$ esiste un $a'$ tale che $a'\cdot a$ e $a\cdot a'$ sono unità, ovvero si comportano come l'elemento $e$ al punto precedente.
		  Un tale $a'$ si dice \emph{inverso} di $a$.
\end{enumerate}
Per comodità di solito si omette il puntino. Se $G$ è finito, $card(G) = n$, si dice che $G$ ha \emph{ordine} $n$.
\end{defn}

\paragraph{Esempi}
\begin{enumerate}
	\item $\mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}$ con l'operazione di somma.
	\item $\mathbb{Q}^*, \mathbb{R}^*, \mathbb{C}^*$ con l'operazione di moltiplicazione (senza lo 0).
	\item $GL_n(\mathbb{R})$ oppure $GL(V)$
	\item $f:I\to I $ biunivoca, con $I$ insieme e con l'operazione di composizione. Nel caso in cui $I$ sia un insieme finito, tanto vale scegliere $I = \{1,2,3,\ldots, n\}$. In tal caso questo gruppo si chiama $S_n$.
\end{enumerate}

\paragraph{Alcuni teoremi elementari}
\begin{enumerate}
	\item L'unità $e$ è unica.
	
	Dimostrazione: supponiamo che $e$ ed $e'$ siano entrambe unità. Allora vale
	
	\[e = ee' = e' \]
	
	\item Dato $a\in G$, l'inverso di $a$ è unico (e usualmente si denota con $a^{-1}$).

	Dimostrazione: supponiamo che $a', a''$ siano entrambi inversi di $a$. Allora
		
	\[(a' a)a'' = a'(aa'') \implies e a'' = a' e \implies a'' = a' \]
	
	\item Dati $a_1, a_2, \ldots, a_n$, il prodotto $a_1\cdot a_2 \cdots\cdot a_n$ è ben definito senza bisogno di parentesi.
	\item Se $ab = e$, allora anche $ba = e$, dunque $a$ e $b$ sono uno l'inverso dell'altro.

	Dimostrazione: $ba = bae = babb^{-1} = beb^{-1} = bb^{-1} = e$.
	
	\item Dato un intero positivo $k$ e un elemento $a\in G$, definiamo $a^k=\underbrace{a\cdot a \cdots\cdot a}_{k\text{ volte}}$.
	Inoltre poniamo $a^0 = e$ e infine $a^{-k} = (a^{-1})^k$, così abbiamo definito le potenze con esponente in $\Z$.
	Non è difficile dimostrare che, se $k,h$ sono interi (non necessariamente positivi), valgono le usuali proprietà: 
	\[a^{k+h} = a^k \cdot a^h \quad\quad\quad (a^k)^h = a^{kh} \]
	Però non è vero in generale che $(ab)^k = a^kb^k$ (sarebbe vero se l'operazione fosse commutativa). Osserviamo infine che 
	\[ (ab)^{-1} = b^{-1}a^{-1}\]
	infatti $(ab)(b^{-1} a^{-1}) = a(bb^{-1})a^{-1} = aea^{-1} = aa^{-1} = e$.
\end{enumerate}



\begin{defn}[Sottogruppo]
Sia $G$ un gruppo, $H\subseteq G$ si dice sottogruppo di $G$ se:
\begin{itemize}
	\item $e\in H$
	\item $x,y\in H \implies xy\in H$
	\item $x\in H \implies x^{-1}\in H$
\end{itemize}
e si indica $H \leq G$. In altre parole $H$ è sottogruppo se,
ereditando l'operazione di $G$, è esso stesso un gruppo.
\end{defn}

\begin{exmp}[Sottogruppo generato da un elemento]
Sia $G$ un gruppo e $a$ un suo elemento. L'insieme delle potenze di $a$, ovvero $\{a^k | k\in\Z\}$, è un sottogruppo di $G$,
che di solito viene denotato con $\langle a\rangle$.
\end{exmp}

\begin{rem}
Se $G$ è gruppo, $a\in G$ ed esiste un intero $n>0$ tale che $a^n=e$, allora tutti gli elementi di $\langle a\rangle$ sono della forma
$a^k$ per qualche $0\le k < n$. Infatti se si considera un qualsiasi $a^s$ con $s\in\Z$, si può scrivere $s=nq+r$ con $0\le r<n$.
Allora \[a^s=a^{nq+r} = (a^n)^qa^r = e^qa^r = a^r\]
Se $n$ è il minimo intero positivo tale che $a^n=e$, allora si dice che $a$ ha \emph{ordine} $n$. In tal caso è facile verificare che
l'insieme $\langle a\rangle$ contiene esattamente $n$ elementi distinti, ovvero $a^0, a^1, \dots a^{n-1}$.
Infatti, se fosse $a^i=a^j$ con $0\le i<j<n$, allora $a^{j-i}=e$, che sarebbe assurdo siccome $0<j-i<n$.

Se $\langle a\rangle$ è finito (il che è certo se ad esempio $G$ è finito) allora di sicuro esiste $n>0$ tale che $a^n=e$. Infatti basta prendere $0\le i < j$
tali che $a^i = a^j$ e osservare che $a^{j-i} = e$. Questi $i$ e $j$ esistono per forza perché se tutte le potenze fossero distinte allora $\langle a\rangle$ sarebbe infinito.
\end{rem}


\begin{defn}[Sottogruppo normale]
Sia $G$ un gruppo, $H \leq G$ si dice \emph{normale} in $G$ se
\[
	\forall h\in H, \forall g\in G\qquad ghg^{-1}\in H
\]
e si indica $H \trianglelefteq G$.
\end{defn}

\begin{defn}[Laterale]
	Sia $G$ un gruppo e $H<G$ un suo sottogruppo, definiamo \textit{laterale destro} o \textit{classe laterale destra} di $H$
	un sottoinsieme di $G$ del tipo
	\[
		gH=\{ gh\ |\ h\in H\}
	\]
\end{defn}

\begin{defn}[Quoziente]\label{defn:quoziente}
	Sia $G$ un gruppo, $H<G$, chiamiamo \textit{quoziente} di $G$ per $H$ l'insieme delle classi laterali di $H$, che indicheremo con $G/H$, ovvero
	\[
		G/H=\{gH\ |\ g\in G\}
	\]
	dove vengono identificati gli insiemi uguali (infatti non è detto che se $g,g'\in G$, con $g\neq g'$, allora $gH\neq g'H$).
\end{defn}

\begin{rem}
	Dato $G$ un gruppo, $H<G$, non è difficile mostrare che tutte le classi laterali di $H$ in $G$ hanno la stessa cardinalità, in particolare hanno tutte la cardinalità della classe $eH$, ma $eH=H$ come insieme, quindi tutte le classi laterali di $H$ hanno la stessa cardinalità di $H$.
\end{rem}


\begin{thm}
	Il quoziente di un gruppo $G$ per un suo sottogruppo $H$ fornisce una partizione di $G$: per ogni $g\in G$ esiste un unico $H$-laterale destro $g'H$ tale che $g\in g'H$. 
\end{thm}
\begin{proof}
	Si osserva che $g\in gH$ visto che $gH=\{gh\ |\ h\in H\}$ e che $e\in H$, se si avesse che $g\in \alpha H$ allora $g=\alpha h_1$ per qualche $h_1$. Si osserva allora che i due laterali coinciderebbero:
	\[
		\alpha H=\{ \alpha h\ |\ h\in H\} = \{ \alpha h_1 h\ |\ h\in H \} = \{ gh\ |\ h\in H\} = gH
	\]
\end{proof}

\begin{thm}[Teorema di Lagrange]
	Sia $G$ un gruppo finito, $H<G$, allora $|H|$ divide $|G|$ e, in particolare, $\displaystyle |G/H|=\frac{|G|}{|H|}$; il numero $|G/H|$ viene chiamato \textit{indice} di $H$ in $G$.
\end{thm}


\begin{defn}[Gruppo quoziente]
	Sia $G$ gruppo, $H\trianglelefteq G$ (osservare che si richiede che il sottogruppo sia \textit{normale}), allora chiameremo \textit{gruppo quoziente} di $G$ su $H$ l'insieme quoziente come l'abbiamo definito \eqref{defn:quoziente} munito della seguente operazione:
	\[
		(g_1H)\cdot(g_2H)=g_1g_2H
	\]
	Non riportiamo la dimostrazione del fatto che l'operazione così definita rispetti effettivamente gli assiomi dei gruppi.
\end{defn}

\begin{rem}
	Attenzione a non farsi ingannare: ci si può chiedere se, dato $G$ gruppo con $H,K\trianglelefteq G$ tali che $H\cong K$, si possa concludere che $G/H\cong G/K$. Questo in generale è \textbf{falso}! Come esempio si può prendere $G=\Z$, $H=5\Z$ e $K=7\Z$, dove
	\[
		5\Z=\{ 5t|\ t\in \Z\}\qquad 7\Z=\{ 7t|\ t\in \Z\}
	\]
	Infatti evidentemente $H\cong \Z\cong K$, ma $G/H\cong \Z_5$ mentre $G/K\cong \Z_7$\footnote{Esistono controesempi anche con gruppi finiti.}.
\end{rem}


\begin{defn}[Classi di coniugio]
Sia $G$ un gruppo, $x \in G$, la classe di coniugio di $x$ è l'insieme $\{ gxg^{-1} | g\in G \}$. Si dimostra facilmente che le classi di coniugio di tutti gli elementi di $G$ formano una partizione del gruppo stesso. Si osserva inoltre che un sottogruppo è normale se e solo se è unione di classi di coniugio (\textsc{Attenzione:} è raro che unendo a caso classi di coniugio si ottenga un sottogruppo).
\end{defn}


\begin{exmp}[Le classi di coniugio di $GL_n(\C)$]
Nel caso del gruppo $GL_n(\C)$ due matrici stanno nella stessa classe di coniugio se e solo se sono simili, quindi per ogni classe di coniugio esiste un rappresentante canonico che è la forma di Jordan di una qualsiasi matrice nella classe (con opportune convenzioni sull'ordine dei blocchi e degli autovalori).
\end{exmp}

\begin{defn}[Centro di un gruppo]
	Sia $G$ un gruppo, il \textit{centro} di $G$ si indica con $Z(G)$ ed è il sottoinsieme degli elementi che commutano con tutto $G$:
	\[
		Z(G)=\{ h\in G\ |\ hg=gh\ \forall g\in G \}
	\]
	\`E immediato verificare che $Z(G)$ è un sottogruppo normale di $G$.

\end{defn}

\begin{defn}[Prodotto diretto di gruppi]
Siano $G$ e $H$ gruppi. Si definisce prodotto diretto di $G$ e $H$ il gruppo formato dall'insieme $G \times H = \{ (g, h) | g \in G, h \in H\}$ con l'operazione componente per componente, ovvero separatemente per i due gruppi di partenza.
\end{defn}


\begin{defn}[Omomorfismo (isomorfismo) di gruppi]
Siano $G$ ed $H$ gruppi, un'applicazione $\varphi:G\to H$ si dice \textit{omomorfismo di gruppi} se
\[
	\forall g_1,g_2\in G\qquad \varphi(g_1 g_2)=\varphi(g_1)\varphi(g_2)
\]
dove la prima moltiplicazione è fatta in $G$ mentre la seconda in $H$.
Se $\varphi$ è bigettiva, allora si dice \textit{isomorfismo}, e i due gruppi si dicono \emph{isomorfi}.
Indichiamo con $Hom(G,H)$ l'insieme degli omomorfismi da $G$ ad $H$.
\end{defn}

\begin{defn}
	Siano $G$ e $H$ gruppi, $f:G\to H$ un omomorfismo di gruppi, allora definiamo
	\begin{gather*}
		Ker f = \{g\in G\ |\ f(g)=e_H\}\\
		Imm f =\{ h\in H\ |\ \exists g\in G\text{ t.c. }f(g)=h\}
	\end{gather*}
	Non è difficile verificare che sia $Ker f$ che $Imm f$ sono sempre sottogruppi rispettivamente di $G$ e di $H$, inoltre si può osserevare che $Ker f$ è un sottogruppo normale di $G$.
\end{defn}

\begin{rem}
	Non è difficile dimostrare che, dati $G$ e $H$ due gruppi e $f:G\to H$ un omomorfismo di gruppi, esso è \textit{iniettivo} se e solo se $Ker f = \{e\}$.
\end{rem}


\begin{thm}[Primo teorema di omomorfismo]\label{alg:primo_teo_omo}
	Dati due gruppi $G$ e $H$ e un omomorfismo $f:G\to H$, vale che
	\[ G/Ker f \cong Imm f\]
\end{thm}
\begin{rem}
	Se la $f$ del teorema precedente è iniettiva, allora $G/Ker f\cong G$ e quindi $G\cong Imm f$. Invece se $f$ è surgettiva, allora $G/Ker f\cong H$.
\end{rem}



\begin{defn}[Azione di un gruppo su un insieme] Sia $G$ un gruppo e $I$ un insieme. Chiamiamo azione $a$ di $G$ su $I$ una funzione $a:G\times I \to I$ che rispetti la regola di composizione, ovvero che se $h,g\in G$ e $i \in I$, valga
\[ a(h,a(g,i)) = a(hg, i) \]
Normalmente si usa una notazione abbreviata in cui invece di scrivere $a(g,i)$ si scrive direttamente $g\cdot i$ o addirittura $gi$


\label{defn:azione}
\end{defn}


\begin{defn}[Azione transitiva]
Un'azione di un gruppo $G$ su un insieme $I\neq \emptyset$ si dice \textit{transitiva} se $\forall\ i,j\in I\ \exists s\in G$ t.c. $j=s\cdot i$.
\label{defn:azione transitiva}
\end{defn}


\begin{defn}[Orbita di un elemento]
Sia $G$ un gruppo che agisce sull'insieme $I$, dato $x\in I$ si chiama \textit{orbita} di $x$ in $G$ l'insieme $\Orb_{G}(x)=\{ g\cdot x\ |\ g\in G \}$, se il gruppo utilizzato è chiaro si può scrivere semplicemente $\Orb(x)$. Si osserva subito che un'azione è transitiva se e solo se induce una unica orbita.
\label{defn:orbita}
\end{defn}


\begin{rem} Le orbite di un gruppo $G$ sull'insieme $I$ formano una partizione dell'insieme. La verifica non è difficile. Vale inoltre la formula $|\Orb_G(x)| \cdot |\Stab_G(x)| = |G|$
\end{rem}



\begin{rem}
	Un gruppo $G$ può agire su se stesso per coniugio, ovvero dati $g\in G$ (qui $G$ è pensato come gruppo che agisce) e $x\in G$ ($G$ pensato come insieme),
	si pone $g\cdot x = gxg^{-1}$. Non è difficile verificare che si tratta davvero di una azione.
	Osserviamo che le classi di coniugio sono le orbite degli elementi generate mediante l'azione per conugio.
\end{rem}




\begin{defn}[Azione semplicemente transitiva]
Un'azione di $G$ su un insieme $I\neq \emptyset$ si dice \textit{semplicemente transitiva}
se presi comunque $i,j\in I$ esiste un unico $s\in G$ tale che $j=s\cdot i$.
\end{defn}


\begin{defn}[Funzione $G$ equivariante]
Dato un gruppo $G$ che agisce su due insiemi $I$ e $J$, una funzione $\phi: I \to J$ si dice $G$ equivariante se 
\[ \phi(s \cdot_I i) = s \cdot_J \phi(i) \qquad \forall s \in G, \ \ \forall i \in I \]
\end{defn}





\newpage
\subsection{Proprietà dei gruppi ciclici}

\begin{defn}[Gruppo ciclico] Un gruppo $G$ si dice ciclico se esiste un elemento $a\in G$ tale che ogni
elemento di $G$ è una potenza di $a$, ovvero $G=\langle a\rangle$. Si dice che $a$ è un generatore di $G$.
\end{defn}

\begin{rem}
Sia $G$ un gruppo ciclico di cardinalità $n$ e generatore $a$. Allora $n$ è il più piccolo intero positivo tale che $a^n = e$,
e ogni elemento di $G$ si scrive in modo unico come $a^k$ con $0\le k < n$.
\end{rem}

\begin{exmp}[Radici dell'unità]
Dato $n>0$ intero, l'insieme $\mu_n\subset \C^*$ delle radici $n$-esime dell'unità è un gruppo ciclico con $n$ elementi.
\end{exmp}

\begin{rem} Se $n$ è un intero positivo esiste (a meno di isomorfismo) un unico gruppo ciclico di cardinalità $n$.
Abbiamo già visto che esiste (basta considerare $\mu_n$),
inoltre dati due gruppi ciclici di cardinalità $n$ e generatori rispettivamente $a$ e $b$ è immediato costruire un
isomorfismo $f:\langle a\rangle\to\langle b\rangle$ ponendo $f(a^k) = b^k$ per $0\le k < n$.
\end{rem}


\begin{prop} Sia $C_n$ un gruppo ciclico di cardinalità $n$. Allora
\[ n = card(Hom(C_n,\C^*))\]
\end{prop}
\begin{proof} Sia $a$ un generatore di $C_n$. Fissato $\omega\in\mu_n$ posso definire
una funzione $f:C_n\to\C^*$ ponendo $f(a^k) = \omega^k$ per $0\le k < n$.
Verifichiamo che $f\in Hom(C_n, \C^*)$. A tal fine prendiamo due elementi di $C_n$, che sono della forma $a^k, a^h$ per certi interi $0\le k,h < n$.
\[f(a^k \cdot a^h) = f(a^{k+h}) = \omega^{k+h} = \omega^k \omega^h = f(a^k)f(a^h)\]
Dunque $f$ è omomorfismo. Variando la scelta di $\omega\in\mu_n$ si producono effettivamente $n$ omomorfismi differenti (infatti se $\omega$ cambia allora cambia anche $f(a)$).
Mostriamo che non ci sono altri omomorfismi oltre a questi.
Sia $f\in Hom(C_n,\C^*)$. Visto che $a^n=e$, deve valere $f(a)^n = f(a^n) = 1$. Allora $f(a)$ deve essere una radice $n$-esima
dell'unità, che chiamiamo $\omega$. A questo punto il fatto che $f$ è omomorfismo implica che $f(a^k) = \omega^k$ per ogni intero $k$.
\end{proof}


\subsection{Proprietà dei gruppi abeliani}
\begin{defn}[Gruppo abeliano] Un gruppo $G$ si dice abeliano se l'operazione di gruppo è commutativa, cioè $\quad\forall a,b\in G\quad ab=ba$.
\end{defn}

\begin{rem} Un gruppo ciclico è sempre abeliano.
\end{rem}

Potrebbe essere utile conoscere il seguente risultato, la cui dimostrazione richiederebbe una conoscenza più approfondita della teoria dei gruppi.
\begin{thm}Ogni gruppo abeliano finito è isomorfo al prodotto diretto di gruppi ciclici.
\end{thm}

\begin{rem} Sia $G$ un gruppo abeliano. Allora 
\[ |G| = card(Hom(G,\C^*))\]

La dimostrazione si ottiene ricordando che $G$ è prodotto diretto di gruppi ciclici e facendo un ragionamento simile a quello
della proposizione analoga per gruppi ciclici.
Se invece $G$ non è abeliano allora nella formula precedente all'uguale va sostituito un $>$.
\end{rem}




\subsection{Proprietà del gruppi simmetrici}
Il gruppo simmetrico $S_n$ è stato introdotto come l'insieme delle funzioni bigettive da $\{1,2,\dots,n\}$ in sé, dotato dell'operazione di composizione.
Dunque $S_n$ agisce in modo naturale su $\{1,2,\dots,n\}$, permutandone gli elementi. Per descrivere un elemento $\sigma \in S_n$ 
è spesso conveniente usare la notazione di prodotto di cicli disgiunti, che ora descriviamo informalmente.

Si comincia a costruire la lista $(1, \sigma(1), \sigma^2(1), \dots)$. Visto che abbiamo a disposizione un numero finito di elementi,
ad un certo punto sarà $\sigma^k(1) = 1$. Allora se scriviamo $(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ tutti i numeri tra le 
parentesi saranno diversi tra loro (questo segue dal fatto che $\sigma$ è bigettiva). Inoltre ognuno dei numeri scritti viene mandato da $\sigma$
nel numero immediatamente successivo nella lista, e l'ultimo numero viene mandato nel primo. Quello che abbiamo appena scritto è un \emph{ciclo}.
\`E anche possibile che la lista sia semplicemente $(1)$, il che vorrebbe dire che $1$ viene lasciato fisso da $\sigma$.
Se avessimo cominciato il procedimento con $\sigma(1)$ al posto di $1$ avremmo ottenuto $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$, che
descrive ugualmente bene il modo in cui $\sigma$ sposta gli elementi scritti. Anche se la scrittura è diversa, per noi
$(1, \sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1))$ e $(\sigma(1), \sigma^2(1), \dots, \sigma^{k-1}(1), 1)$ sono esattamente lo stesso ciclo,
e un ragionamento analogo vale per gli altri numeri facenti parte della lista: non importa da quale si parte.
Può darsi che non tutti i numeri da $1$ a $n$ compaiano nel ciclo appena scritto: in tal caso si prende un numero ancora non scritto e si ricomincia da capo
da lui, creando una nuovo ciclo, e si continua così finché non sono stati scritti tutti i numeri.
Alla fine ci ritroviamo un elenco di cicli che sono necessariamente disgiunti per via della bigettività di $\sigma$.
\`E facile convincersi che in questo modo si descrive completamente $\sigma$. Inoltre a meno di variare
l'ordine con cui sono scritti i cicli e di cambiare i ``punti di partenza'' dei singoli cicli questa scrittura come cicli disgiunti è unica.
Riassumiamo quanto detto nel seguente teorema:

\begin{thm}
Ogni elemento $\sigma \in S_n$ si scrive in modo unico come prodotto di cicli disgiunti a meno dell'ordine dei fattori e a meno di 
cambiare il modo in cui i singoli cicli sono presentati.
\end{thm}

Spesso nella scrittura in cicli disgiunti si tralasciano i cicli di lunghezza uno. Ad esempio $(3,5,7)(4,1)\in S_{12}$ ha perfettamente senso:
i numeri $1,3,4,5,7$ vengono ``spostati'' da $\sigma$ nel modo descritto e tutti gli altri vengono lasciati fissi.

I cicli, più che essere delle liste, vanno pensati come elementi di $S_n$, ovvero come funzioni bigettive da $\{1,\dots,n\}$ in sé.
In quanto tali possono essere moltiplicati, nel senso di composizione delle funzioni. Ad esempio
\[(1,2,3)\cdot(3,5)\]
chiaramente non è la scrittura come prodotto di cicli disgiunti di un elemento di $S_n$, in quanto appunto i due cicli scritti
non sono disgiunti. Ma il loro prodotto ha perfettamente senso, e usando la stessa convenzione
che si usa di solito per la composizione di funzioni vanno fatti ``agire'' da destra a sinistra.
Ad esempio il prodotto scritto manda il numero $5$ nel numero $1$ (infatti il ciclo a destra manda $5$ in $3$, il quale viene mandato in $1$ dal ciclo
a sinistra). Se lo volessimo scrivere come prodotto di cicli disgiunti otterremmo:
\[(1,2,3,5)\]

\begin{thm}
Ogni elemento $\sigma \in S_n$ si può scrivere come prodotto di \emph{trasposizioni}, ovvero cicli di lunghezza $2$, non necessariamente disgiunti.
\end{thm}
\begin{proof}
Considerato il teorema sulla decomposizione in cicli, basta mostrare la tesi nel caso in cui $\sigma$ è un ciclo.
Supponiamo $\sigma = (a_1, a_2, \dots, a_k)$. Allora è facile verificare che
\[\sigma = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Qualcuno potrebbe essere turbato dal caso in cui il ciclo ha lunghezza $1$, ossia $\sigma$ è l'identità.
In tal caso possiamo dire che $\sigma$ è il prodotto di un insieme vuoto di trasposizioni.
Chi fosse ancora turbato potrebbe scrivere, almeno nel caso $n\ge 2$, $\sigma = (1,2)(1,2)$.
\end{proof}

Osserviamo che il teorema precedente assicura solo l'esistenza di una scrittura come prodotto di trasposizioni
ma non l'unicità. In effetti questa non sussiste, infatti se in fondo ad un prodotto di trasposizioni aggiungo $(1,2)(1,2)$
allora il risultato non cambia. Per avere un esempio leggermente più sofisticato:
\[(2,1)(2,3) = (1,3)(1,2)\]
Tuttavia quello che non cambia è la parità del numero di trasposizioni, come precisato dal seguente teorema.
\begin{thm}
Siano $\tau_1, \tau_2, \dots, \tau_t, \sigma_1, \sigma_2, \dots, \sigma_s$ trasposizioni in $S_n$. Supponiamo che
\[\tau_1\tau_2\dots\tau_t = \sigma_1\sigma_2\dots\sigma_s\]
Allora $s\equiv t \mod 2$.
\end{thm}
\begin{proof}[Cenno di dimostrazione]
Definiamo la seguente funzione $f:S_n\to\N$:
\[f(\rho) = card\left(\left\{\quad(a,b)\in\{1,\dots,n\}^2 \quad | \quad a < b, \quad\rho(a) > \rho(b) \quad\right\}\right)\]
Non è difficile verificare che se $\tau\in S_n$ è una trasposizione allora $f(\rho)$ e $f(\tau\rho)$ hanno parità diversa.
Il risultato segue immediatamente visto che $f(\tau_1\tau_2\dots\tau_t) = f(\sigma_1\sigma_2\dots\sigma_s)$.
\end{proof}

\begin{defn}[Segno di una permutazione]
Il teorema appena visto permette di definire il \emph{segno} di ogni elemento $\sigma\in S_n$, che si pone uguale a $1$
se $\sigma$ si scrive come prodotto di un numero pari di trasposizioni, si pone uguale a $-1$ altrimenti.
\end{defn}

\begin{prop}
Il segno di un ciclo di lunghezza $k$ è esattamente $(-1)^{k-1}$
\end{prop}
\begin{proof}
Abbiamo già visto un modo in cui un ciclo di lunghezza $k$ si può scrivere come prodotto di trasposizioni:
\[(a_1, a_2, \dots, a_k) = (a_1, a_k)\cdot (a_1, a_{k-1}) \cdot \dots \cdot (a_1, a_2)\]
Dunque la tesi segue immediatamente.
\end{proof}

\begin{defn}
L'insieme degli elementi di $S_n$ aventi segno $+1$ è un sottogruppo di $S_n$,
chiamato \emph{gruppo alterno} e indicato con $A_n$.
\end{defn}


\subsection{Proprietà dei gruppi diedrali}

\begin{defn}[Gruppo diedrale]
Consideriamo in $\R^2$ un poligono regolare di $n$ lati con centro nell'origine.
L'insieme $D_n$ delle isometrie di $\R^2$ che mandano il poligono in sé è un gruppo con l'operazione di composizione.
Si verifica che questo gruppo ha $2n$ elementi, di cui $n$ rotazioni (ovvero elementi di $O_2(\R)$ con determinante $1$)
e $n$ riflessioni (ovvero elementi di $O_2(\R)$ con determinante $-1$).
Inoltre, detta $\rho$ una rotazione di $2\pi/n$ (che ha ordine $n$, e per inverso ha $\rho^{n-1}$) e $\sigma$ una qualunque riflessione (che ha ordine $2$), esse generano
il gruppo $D_n$, che si può presentare nel seguente modo: $$D_n=\langle\rho,\sigma|\rho^n=\sigma^2=id,\ \sigma\rho\sigma=\rho^{-1}\rangle$$
\end{defn}

\begin{rem}
 Le $n$ potenze distinte di $\rho$ sono tutte e sole le rotazioni di $D_n$, mentre gli elementi della forma $\sigma\rho^{i},\ i=0,1,..,n-1$ 
 sono tutte e sole le riflessioni. 
\end{rem}

\begin{rem}
 Si dimostra facilmente che la relazione $\sigma\rho\sigma=\rho^{-1}$ è verificata da qualsiasi rotazione $\rho$
 e qualsiasi riflessione $\sigma$.
\end{rem}











\newpage
\section{Algebra lineare}
In questa sezione diamo alcune definizioni e teoremi di algebra lineare che sono stati utilizzati nel corso o che sono utili per avere una visione d'insieme di certi argomenti. Non saranno presenti le dimostrazioni che possono essere trovate su molti libri di algebra lineare.
\begin{thm}[Diagonalizzazione simultanea]
\label{thm:diag_sim}
	Date due matrici $M, N\in \mathcal{M}(n,n,\K)$, diremo che sono \textit{simultaneamente diagonalizzabili} se esiste una base comune di autovettori per entrambe.\\
	Date $M, N\in \mathcal{M}(n,n,\K)$, se esse commutano e sono entrambe diagonalizzabili allora sono simultaneamente diagonalizzabili.
\end{thm}
\begin{cor}
	Date $M_1,\ldots,M_k \in \mathcal{M}(n,n,\K)$, se $M_iM_j=M_jM_i\ \forall\ i, j$ e ogni $M_i$ è diagonalizzabile, allora esiste una base comune di autovettori per tutte quante.
\end{cor}


\begin{defn}[Ideale di un endomorfismo]
	Se $p(x)=a_n x^n+\ldots+a_0$, allora scriviamo $p(f)$ per intendere $a_nf^n+\ldots+a_0f^0$ dove $f^0=Id$ e $f^k=\underbrace{f\circ\ldots\circ f}_{k \text{ volte}}$.\\
	Sia $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$. Definiamo \textit{ideale di $f$} l'insieme
	\[
		I(f)=\left\{ p(x)\in \K[x]\ |\ p(f)=0 \right\}
	\]
	

\end{defn}


\begin{thm}[Teorema di decomposizione primaria]
\label{thm:dec_primaria}
	Siano $V$ un $\K$-spazio vettoriale, $f:V\to V$ un endomorfismo di $V$ e $q(x)\in I(f)$. Sia $q=q_1\cdot\ldots\cdot q_k$ tale che $MCD(q_i,q_j)=1\ \forall\ i\neq j$, allora $V=Ker(q_1(f))\oplus\dots\oplus Ker(q_k(f))$ e gli addendi sono $f$-invarianti.\\
	In particolare se $f$ è triangolabile e $\lambda_1,\ldots,\lambda_k$ sono gli autovalori di $f$ con molteplicità algebrica rispettivamente $\alpha_1,\ldots,\alpha_k$, allora $V=Ker\left((f-\lambda_1 Id)^{\alpha_1}\right)\oplus\dots\oplus Ker\left((f-\lambda_k Id)^{\alpha_k}\right)$.
\end{thm}

\begin{thm}[Forma canonica di Jordan]
	Sia $M\in \mathcal{M}(n,n,\K)$ una matrice triangolabile, siano $\lambda_1,\ldots,\lambda_k$ i suoi autovalori, allora $M$ è simile alla sua \textit{forma canonica di Jordan} che è nella forma
	\begin{align*}
		&\begin{pmatrix}
			J_1 & & \\
			& \ddots & \\
			& & J_t
		\end{pmatrix}		
		&\text{ dove }J_i=\begin{pmatrix}
		                  	\lambda & 1 & & & \\
							& \lambda & 1 & & \\
		                  	& & \ddots & \ddots & \\
		                  	& & & \ddots & 1 & \\
		                  	& & & & \lambda
		                  \end{pmatrix} \text{ per qualche }\lambda \in \left\{ \lambda_1,\ldots,\lambda_k \right\}
	\end{align*}
	La dimensione e il numero di blocchi di ciascun tipo sono univocamente determinati dalla matrice $M$, ne segue che la forma canonica di Jordan è unica a meno di permutazione dei blocchi e dunque, scelta una convenzione sull'ordine dei blocchi, essa è un sistema completo di invarianti per similitudine: due matrici sono simili se e solo se hanno la stessa forma di Jordan.

\end{thm}
\begin{defn}[Forma hermitiana]
Siano $V,W$ due $\C$-spazi vettoriali, una funzione $h:V\times V\to W$ si dice \textit{forma hermitiana} se $\forall v,w,z\in V,\ \forall \alpha \in \C$ vale che
\begin{gather*}
	h(v,w) = \overline{h(w,v)}\\
	h(\alpha v, w) = \alpha h(v,w)\\
	h(v+z,w)=h(v,w)+h(z,w)
\end{gather*}
\end{defn}

\begin{defn}
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{definita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V, v\neq 0$ si ha che $\phi(v,v)>0$ (rispettivamente $\phi(v,v)<0$), ossarvare che $\phi(v,v)\in \R\ \forall\ v\in V$, dunque ha senso chiedere che sia maggiore o minore di $0$.\newline
	Una forma hermitiana $\phi:V\times V\to \C$ è \textit{semidefinita positiva} (rispettivamente \textit{negativa}) se $\forall\ v\in V$ si ha che $\phi(v,v)\geq 0$ (rispettivamente $\phi(v,v)\leq 0$)
\end{defn}

\begin{thm}
	Ogni forma hermitiana definita positiva su uno spazio vettoriale $V$ di dimensione finita ammette una \textit{base ortonormale}, ovvero esiste una base $\{v_1,\ldots,v_n\}$ di $V$ tale che $\phi(v_i,v_j)=\delta_{ij}$. 
\end{thm}




\newpage
\section{Algebra multilineare}
\subsection{Alcune generalizzazioni di algebra lineare}

\begin{defn}[Base di uno spazio vettoriale]
Sia $V$ un $\K-$spazio vettoriale e $I$ un insieme; una base di $V$ è una funzione $e: I \to V$ tale che 
per ogni $v \in V$ esiste un'unica funzione $a: I \to \K$ a supporto finito per cui vale $v=\sum_{i\in I}a_i e_i$.
Questa definizione è compatibile con la definizione di base come insieme di vettori generatori linearmente indipendenti.

Spesso useremo una notazione del tipo $\{e_i\}_{i\in I}$ per indicare una base di uno spazio vettoriale $V$. Ciò sottintende
una funzione $e:I\to V$ che manda $i\to e_i$, in accordo con la definizione che abbiamo appena dato.
\end{defn}

Alcuni dei risultati a cui arriveremo sono validi anche per $I$ infiniti, ma per semplicità consideriamo solo
spazi vettoriali finitamente generati (per cui esiste una base $e: I \to V$ con $I$ insieme finito).
Questo primo lemma dovrebbe essere noto a chiunque abbia un minimo di familiarità con l'algebra lineare:
\begin{lemma}
Siano $V,W$ dei $\K-$spazi vettoriali, sia $e:I\to V$ una base di $V$ e $f: I \to W$ una funzione. Allora $\exists!\  \phi: V \to W$ lineare tale che
\[\phi(e_i) = f_i \]
Inoltre $\phi$ è un isomorfismo $\Leftrightarrow$ $f$ è una base.
\end{lemma}

\begin{lemma}
Dato $I$ insieme, esiste uno spazio vettoriale $V$ con base una certa $e:I\to V$.
\end{lemma}
\begin{proof}
Definisco il seguente insieme, che è in modo naturale un $\C-$spazio vettoriale:
\[ \C^I = \{ v:I\to\C \quad|\quad v \text{ ha supporto finito}\}\]
Ora è facile osservare che $e:I\to\C^I$ definita da $e_i(j) = \delta_{i,j}$ è una base.
\end{proof}


\subsection{Prodotto tensoriale}

\begin{defn}[Prodotto tensoriale]
   Siano $V, W$ due $\mathbb{C}$-spazi vettoriali. Si dice prodotto tensore di $V$ e $W$, 
   e si indica come $V\otimes W$, uno spazio vettoriale con una funzione bilineare 
   $\otimes: V \times W \to V\otimes W$ tale che per ogni data funzione bilineare $h: V\times W \to  Z$,
   esiste unica $\phi: V\otimes W \to Z$ lineare per cui $\phi(v \otimes w)=h(v,w)$. Ovvero questa $\phi$ fa commutare il diagramma:
   \[\tridiag{V\times W}{ \otimes }{V \otimes W}{\phi}{Z}{h}\]
   Questa proprietà viene detta proprietà universale del prodotto tensoriale e la funzione $\otimes: V \times W \to V\otimes W$
   viene detta funzione universale.
\label{defn:prodotto tensoriale}
\end{defn}

\begin{exmp} $V=W=V\otimes W = \C$ e come $\otimes$ prendo il prodotto ovvero $v\otimes w=vw$.
\end{exmp}
\begin{exmp} $W$ un qualsiasi $\K-$spazio vettoriale, $V=\K$. Allora posso prendere $\K\otimes W = W$ e come $\otimes$ prendo il prodotto per scalari $\alpha\otimes w=\alpha w$.

Infatti se $h:\K\times W\to Z$ è una funzione bilineare, allora mi basta definire $\phi:W\to Z$ ponendo $\phi(w) = h(1, w)$. Si vede facilmente
che così il diagramma commuta e non esiste nessun'altra $\phi$ che fa commutare il diagramma.
\end{exmp}

\begin{prop}
Se ho due prodotti tensoriali $V \otimes W$ e $V \overline{\otimes} W$, allora esiste un unico isomorfismo 
$\phi: V \otimes W \to V \overline{\otimes} W$ tale che, fissati comunque $v\in V, w\in W$, valga
\[ \phi (v\otimes w) = v \overline{\otimes} w\]
\end{prop}
\begin{proof}
Considero i seguenti due diagrammi:
\[
\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ V \overline{\otimes} W }{\overline{\otimes}} \qquad
\tridiag{V\times W}{ \overline{\otimes} }{V \overline{\otimes} W}{}{V \otimes W}{\otimes}
\]
La proprietà universale per il prodotto tensore $\otimes$ dice che $\exists !\phi:V\otimes W\rightarrow V\overline{\otimes}W$ lineare che fa commutare il primo diagramma: analogamente $\exists !\psi:V\overline{\otimes}W\rightarrow V\otimes W$ lineare che fa commutare il secondo diagramma. Ora consideriamo i seguenti due diagrammi:
\[
\begin{diagram}
            &                            & V\otimes W               \\
            & \ruTo^{\otimes}            & \dTo>{\phi}              \\
 V\times W  & \rTo^{\overline{\otimes}}  &  V\overline{\otimes} W   \\
            & \rdTo_{\otimes}            & \dTo>{\psi}              \\
            &                            & V\otimes W               \\
\end{diagram}
\qquad
\begin{diagram}
            &                            & V\overline{\otimes} W    \\
            & \ruTo^{\overline{\otimes}} & \dTo>{\psi}              \\
 V\times W  & \rTo^{\otimes}             &  V\otimes W              \\
            & \rdTo_{\overline{\otimes}} & \dTo>{\phi}              \\
            &                            & V\overline{\otimes} W    \\
\end{diagram}
\]
Ma noi conosciamo già un'applicazione lineare che fa commutare i due diagrammi ``più grandi'': l'identità. Quindi per unicità possiamo concludere che $\phi\psi=\psi\phi=id$, ovvero $\phi$ e $\psi$ sono una l'inversa dell'altra e in particolare $\phi$ è un isomorfismo.
\end{proof}

Si può dimostrare inoltre che dati due spazi vettoriali $V$ e $W$ esiste sempre 
un loro prodotto tensoriale, dunque abbiamo il seguente risultato:
\begin{thm}
$V\otimes W$ esiste ed è unico a meno di isomorfismo.
\end{thm}


\begin{note}
\`E importante notare che non tutti gli elementi $z \in V \otimes W$ si scrivono come $z = v \otimes w$. In particolare, per fare un esempio concreto che mostra che questa cosa non funziona, prendiamo $W = V^*$. Vedremo fra poco che $V\otimes V^*$ è canonicamente isomorfo allo spazio delle applicazioni bilineari da $V$ in $\C$, che sappiamo scriverlo come matrici $n\times n$. Tuttavia se un elemento si scrive in termini di matrici come $z = v\otimes w$, allora la matrice associata a $z$ in una base avrà rango al massimo 1, ben lontano da coprire tutto lo spazio.
\end{note}


\begin{prop}
L'insieme degli elementi di $V\otimes W$ della forma $v\otimes w$ con $v\in V, w\in W$ genera tutto lo spazio $V\otimes W$.
\end{prop}

\begin{proof} Consideriamo il sottospazio generato dagli elementi della forma $v\otimes w$:
\[\langle\{v\otimes w:v\in V,w\in W\}\rangle=\langle A\rangle\]
Vogliamo quindi dimostrare che $\langle A\rangle=V\otimes W$. Consideriamo il diagramma:
\[\tridiag{V\times W}{ \otimes }{V \otimes W}{  }{ \langle A\rangle }{h}\]
dove $h(v,w)=v\otimes w$ è ovviamente un'applicazione bilineare. Quindi $\exists !\psi:V\otimes W\rightarrow \langle A\rangle$ che fa commutare il diagramma. Detta $i:\langle A\rangle\rightarrow V\otimes W$ l'inclusione consideriamo il seguente diagramma:
\[
\begin{diagram}
            &                       & V\otimes W               \\
            & \ruTo^{\otimes}       & \dTo>{\psi}              \\
 V\times W  & \rTo^{h}              & \langle A \rangle   \\
            & \rdTo_{i\circ h}      & \dTo>{i}              \\
            &                       & V\times W               \\
\end{diagram}
\]
tuttavia il diagramma grande commuta anche con $id_{V\otimes W}$ e quindi $i\circ\psi=id_{V\otimes W}\Rightarrow i$ è surgettiva $\Rightarrow \langle A\rangle=V\otimes W$.
\end{proof}


\begin{defn}[Prodotto tensoriale di mappe lineari]
Date $f:V \to V'$ e $g:W \to W'$ funzioni lineari, si definisce prodotto tensoriale tra $f$ e $g$ l'unica funzione lineare $f \otimes g : V \otimes W \to V' \otimes W'$ tale che $(f \otimes g)(v \otimes w)=f(v) \otimes g(w)$ $\forall v\in V, w\in W$.

Una funzione con tale proprietà esiste ed è unica poiché l'applicazione $V\times W \to V'\otimes W'$ che manda $(v,w)$ in $f(v)\otimes g(w)$ è bilineare.
\end{defn}

\begin{rem}
$id_V \otimes id_W = id_{V\otimes W}$
\end{rem}


\begin{prop}
Siano $V$ e $W$ $\K-$spazi vettoriali e sia $\{e_i\}_{i\in I}$ una base di $V$. Allora ogni elemento di $V\otimes W$ si scrive in modo unico come:
\[ \sum_{i\in I} e_i \otimes w_i\]
con $w_i\in W$.
\end{prop}
\begin{proof}
Sia $x$ un elemento di $V\otimes W$.
Visto che gli elementi della forma $v\otimes w$ generano l'intero spazio $V\otimes W$, possiamo scrivere
\[ x = \sum_{j=1}^N v_j\otimes w_j \]
scegliendo opportunamente $N\in \N$, $v_j\in V$, $w_j\in W$ per $j=1\dots N$. Visto che $\{e_i\}_{i\in I}$ è base di $V$:
\[ x = \sum_{j=1}^N v_j\otimes w_j  = \sum_{j=1}^N \left(\sum_{i\in I} a_{i,j}e_i\right)\otimes w_j = \sum_{i,j} a_{i,j}(e_i\otimes w_j) = \sum_{i\in I} e_i\otimes\tilde w_i\]
dove si è posto $\tilde w_i = \sum_j a_{i,j}e_j$.
Quindi siamo riusciti ad ottenere una scrittura del tipo che volevamo. Resta da mostrare che questa scrittura è unica,
ovvero che gli elementi $\tilde w_i$ sono univocamente determinati.

Consideriamo la base $\{e_i^*\}_{i\in I}$ di $V^*$ duale rispetto a $\{e_i\}_{i\in I}$. Fissato un $k\in I$ possiamo considerare l'applicazione
lineare $e_k^*\otimes id_W: V\otimes W \to \K\otimes W$. Valutandola in $x$ otteniamo:
\[ (e_k^*\otimes id_W) \left(\sum_{i\in I} e_i\otimes\tilde w_i \right) = \sum_{i\in I} e_k^*(e_i)\otimes\tilde w_k = 1\otimes \tilde w_k\]
Ma $\K\otimes W$ è isomorfo in modo naturale a $W$, e questo isomorfismo porta l'elemento $1\otimes \tilde w_k$ in $\tilde w_k$.
Quest'ultimo, pertanto, è univocamente determinato.
\end{proof}

\begin{prop}
Se $\{e_i\}_{i\in I}$ è una base di $V$ e $\{f_j\}_{j\in J}$ è una base di $W$ allora $\{e_i \otimes f_j\}_{(i,j)\in I\times J}$ è una base di $V \otimes W$.
\end{prop}
\begin{proof}
Dato $x\in V\otimes W$ sappiamo che si può scrivere in modo unico $x = \sum_{i\in I} e_i\otimes w_i$.
Ora, per ogni $i\in I$, il vettore $w_i$ si scrive in modo unico come $w_i = \sum_{j\in J} a_{i,j}f_j$.
Da questo segue abbastanza facilmente che $x$ si scrive in modo unico come $\sum_{i,j} a_{i,j}(e_i\otimes f_j)$.
\end{proof}

\begin{cor}
$\dim(V \otimes W) = \dim V \cdot \dim W$
\end{cor}


\begin{prop}
Siano $V$ e $W$ spazi vettoriali. Allora $V^*\otimes W$ è isomorfo allo spazio vettoriale $\Hom(V,W)$ delle applicazioni lineari da $V$ a $W$.
\end{prop}
\begin{proof}
Definiamo $\theta: V^*\times W \to \Hom(V,W)$ come la funzione che manda la coppia $(f, w)\in V^*\times W$ nella funzione  $h_{f,w}$ definita da:
$h_{f,w}(v) = f(v)w$ per ogni $v\in V$. Non è difficile osservare che $\theta$ è bilineare, quindi induce una funzione lineare
$\phi : V^*\otimes W \to \Hom(V,W)$ tale che, dati comunque $f\in V^*, w\in W, v\in V$, soddisfa $\phi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\phi$ è un isomorfismo di spazi vettoriali.

Fissiamo $\{v_i\}_{i\in I}$ una base di $V$, $\{v_i^*\}_{i\in I}$ la base duale, $\{w_j\}_{j\in J}$ una base di $W$.
Ora abbiamo 
\[\phi(v_i^*\otimes w_j)(v_k) = \delta_{i,k} w_j\]
Ciò vuol dire che, se scriviamo la matrice dell'applicazione $\phi(v_i^*\otimes w_j):V\to W$ secondo le basi date, questa presenta un $1$ all'incrocio
tra l'$i-$esima colonna e la $j-$esima riga, mentre è nulla altrove.
Dunque $\phi$ manda la base $\{v_i^*\otimes w_j\}_{(i,j)\in I\times J}$ dello spazio $V^*\otimes W$ in una base dello spazio $\Hom(V,W)$,
quindi è un isomorfismo.
\end{proof}

\begin{rem}
In particolare, ponendo $W=V$, otteniamo che $\End(V)$ è isomorfo a $V^*\otimes V$.

Esiste un'applicazione in un certo senso ``naturale'' $t:V^*\otimes V \to \K$, dove $\K$ è il campo degli scalari, definita da
$t(f\otimes v) = f(v)$.
Se indichiamo con $\phi$ l'isomorfismo $V^*\otimes V\to \End(V)$ che abbiamo definito nel corso della precedente dimostrazione,
otteniamo una funzione lineare $\tr = t\circ \phi: \End(V)\to \K$. Non è difficile vedere che questa $\tr$ che abbiamo appena definito 
coincide con la classica funzione ``traccia''. Il modo in cui l'abbiamo definita noi rende evidente il fatto che la traccia non dipende
dalla base scelta per scrivere la matrice di un endomorfismo.
\end{rem}


\begin{thm}
Se $f:V\to V$ e $g:W\to W$ sono endomorfismi di spazi vettoriali, allora vale la formula
\[\tr(f\otimes g) = \tr(f) \tr(g)  \]
\label{thm: tracciaprodotto}
\end{thm}
\begin{proof}
Sia $\{v_i\}_{i\in I}$ una base di $V$, $\{w_j\}_{j\in J}$ una base di $W$, e come al solito indichiamo gli elementi delle basi duali aggiungendo un asterisco.
\begin{align*}
 \tr(f\otimes g) &= \sum_{(i,j)\in I\times J} (v_i^*\otimes w_j^*) (f\otimes g)(v_i\otimes w_j) =\\
                 &= \sum_{(i,j)\in I\times J} v_i^*(f(v_i))\cdot w_j^*(g(w_j)) =\\
                 &= \left(\sum_{i\in I} v_i^*(f(v_i))\right) \cdot \left(\sum_{j\in J} w_j^*(g(w_j))\right) =\\
                 &= \tr(f) \cdot \tr(g)
\end{align*}
\end{proof}
\begin{proof}[Dimostrazione alternativa per $\C-$spazi vettoriali]
Iniziamo a considerare il caso in cui sia $f$ che $g$ siano diagonalizzabili: prendendo due basi $a:I\rightarrow V$ , $b:J\rightarrow W$ di autovettori rispettivamente per $f$ e per $g$, si verifica facilmente la verità della proposizione nella base indotta su $V\otimes W$ (ovvero in quella formata dagli $a_i\otimes b_j$).

Ora, essendo la traccia una funzione continua e le matrice diagonalizzabili dense nello spazio delle matrici, la proprietà affermata dal lemma si estende al caso generale per continuità.
\end{proof}

\begin{thm}
Sia $V$ spazio vettoriale. Allora $V^*\otimes V^*$ è isomorfo allo spazio vettoriale delle forme bilineari $V\times V\to \C$.
\end{thm}


\subsection{Prodotto esterno e prodotto simmetrico}

\begin{defn}[Applicazione $n-$lineare simmetrica/alternante]
 Una applicazione $\phi: V^n \to Z$ si dice $n-$lineare se è lineare in ogni componente dopo aver fissato le altre $n-1$.

 Inoltre $\phi$ si dice simmetrica se $\phi(v_{s(1)},\ldots,v_{s(n)})=\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$, mentre si dice 
 alternante se $\phi(v_{s(1)},\ldots,v_{s(n)})=\mathrm{sgn}(s)\phi(v_1,\ldots,v_n)$ per ogni permutazione $s \in S_n$.
\end{defn}

\begin{prop}
  Un'applicazione $n-$lineare $h: V^n \to W$ è alternante se e solo se, presi comunque $v_1,\dots,v_n\in V$
  non tutti distinti tra loro, si ha che $h(v_1,\dots,v_n)=0$.
\end{prop}

\begin{prop}
  Sia $h: V^n \to W$ un'applicazione $n-$lineare alternante. Se i vettori $v_1,\dots,v_n$ sono linearmente dipendenti allora
  $h(v_1,\dots,v_n)=0$.
\end{prop}


\begin{defn}[Prodotto esterno]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto esterno è uno spazio vettoriale indicato con $\bigwedge^n V$
dotato di una funzione $n-$lineare alternante $\wedge: V^n \to \bigwedge^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1\wedge v_2\wedge\ldots\wedge v_n \in \bigwedge^n V$, tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare alternante, 
esiste un'unica $\phi: \bigwedge^n V \to Z $ lineare per cui vale $\phi(v_1\wedge v_2\wedge \ldots \wedge v_n)=h(v_1,\ldots,v_n)$.
\label{defn:prodotto esterno}
\end{defn}

\begin{defn}[Prodotto simmetrico]
Sia $n$ un intero positivo, $V$ uno spazio vettoriale. Un prodotto simmetrico è uno spazio vettoriale indicato con $S^n V$
dotato di una funzione $n-$lineare simmetrica $V^n \to \bigwedge^n V$ che manda $(v_1,\ldots,v_n)$ in 
$v_1 v_2\ldots v_n \in S^n V$, tale che presa comunque una funzione $h: V^n \to Z$ $n-$lineare simmetrica, 
esiste un'unica $\phi: S^n V \to Z $ lineare per cui vale $\phi(v_1 v_2 \ldots v_n)=h(v_1,\ldots,v_n)$.
\label{defn:prodotto simmetrico}
\end{defn}


\begin{prop}[Proprietà dei prodotti esterni e simmetrici]
Sia $V$ uno spazio vettoriale e $n$ un numero naturale.
\begin{itemize}
\item $\bigwedge^nV$ e $S^nV$ esistono.
\item $\bigwedge^nV$ e $S^nV$ sono unici a meno di un unico isomorfismo.
\item $\bigwedge^nV$ è generato dai vettori del tipo $v_1\wedge \dots \wedge v_n$, e analogamente 
      $S^nV$ è generato dai vettori del tipo $v_1\dots v_n$.
\end{itemize}
\end{prop}


\begin{defn}[Potenza simmetrica e potenza esterna di un'applicazione lineare]
Sia $f: V\to V$ un endomorfismo di uno spazio vettoriale. Definiamo 
$\bigwedge^k f : \bigwedge^k V \to \bigwedge^k V$ la funzione che manda $v_1 \wedge \ldots \wedge v_n$ in $f(v_1) \wedge \ldots \wedge f(v_n)$

In modo analogo si definisce la potenza simmetrica.
\end{defn}


\begin{rem} Siano $V$, $W$, $Z$ spazi vettoriali.
\begin{itemize}
   \item $\bigwedge^k Id_V = Id_{\bigwedge^kV}$
   \item Date due applicazioni lineari $f:V\to W$ e $g:W\to Z$ allora $\bigwedge^k(g\circ f) = \bigwedge^kg\circ\bigwedge^kf$
\end{itemize}
\end{rem}

\begin{thm}[Dimensione del prodotto esterno]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme
\[E=\{e_{i_1} \wedge e_{i_2}\wedge \ldots \wedge e_{i_k} \quad|\quad 1 \leq i_1 < i_2 <\ldots< i_k \leq n\}\]
è una base di $\bigwedge^k V$ di cardinalità $|E|= \binom {n}{k}$.
\label{thm:prodotto esterno}
\end{thm}


\begin{thm}[Dimensione del prodotto simmetrico]
Sia $V$ uno spazio vettoriale di dimensione $n$, $\{e_i\}_{i=1}^n$ una base di $V$ e $k$ un intero positivo.
Allora l'insieme 
\[E=\{e_{i_1} \wedge e_{i_2}\wedge\ldots \wedge e_{i_k} \quad|\quad 1 \leq i_1 \leq i_2 \leq\ldots\leq i_k \leq n\}\]
è una base di $S^k V$ di cardinalità $|E|= \binom {n+k-1}{k}$.
\label{thm:prodotto simmetrico}
\end{thm}

Sia $V$ uno spazio vettoriale di dimensione $n$. Allora sappiamo che:
\begin{itemize}
\item $\dim V\otimes V = n^2$
\item $\dim S^2V = \frac{n(n+1)}{2}$
\item $\dim \bigwedge^2V = \frac{n(n-1)}{2}$
\end{itemize}
Possiamo definire in modo abbastanza naturale un'applicazione lineare $V\otimes V\to \bigwedge^2V\oplus S^2V$,
mandando il vettore $v\otimes w$ nella coppia $(v\wedge w, vw)$. In effetti questa applicazione è un isomorfismo.
\begin{proof}
Per uguaglianza delle dimensioni degli spazi di partenza e di arrivo, mi basta dimostrare che è surgettiva.
Per ogni coppia $v\in V$, $w\in W$ abbiamo che:
\[\frac{v\otimes w + w\otimes v}{2} \to (0, vw)\]
\[\frac{v\otimes w - w\otimes v}{2} \to (v\wedge w, 0)\]
Quindi $\bigwedge^2V$ e $S^2V$ sono contenuti nell'immagine. Ma da questo segue subito la tesi.
\end{proof}


\begin{thm}
Sia $V$ spazio vettoriale. Allora $S^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari simmetriche $V\times V\to \C$.
\end{thm}
\begin{thm}
Sia $V$ spazio vettoriale. Allora $\bigwedge^2V^*$ è isomorfo allo spazio vettoriale delle forme bilineari antisimmetriche $V\times V\to \C$.
\end{thm}


\begin{prop}
Sia $f: V \to V$ un endomorfismo di uno spazio vettoriale. Allora vale
\[ 
\begin{cases}
\tr(\bigwedge^2 f ) = \dfrac{(\tr(f))^2 - \tr(f^2)}{2} \\
\tr(S^2 f ) = \dfrac{(\tr(f))^2 + \tr(f^2)}{2} \\
\end{cases}
\]
\label{thm:tracciasymalt}
\end{prop}


\textbf{Dimostrazione:} siano $e_1,..,e_n$ una base di $V$ allora $\exists a_{ij}: f(e_j)=\sum_i a_{ij}e_i$. Se $i<j$
\[ e_i\wedge e_j\rightarrow \bigwedge^2f(e_i\wedge e_j)=f(e_i)\wedge f(e_j)=(\sum_k a_{ki}e_k)\wedge(\sum_l a_{lj}e_l)=\sum_{k,l}a_{ki}a_{lj}(e_k\wedge e_l)=\sum_{k<l}(a_{ki}a_{lj}-a_{li}a_{kj})(e_k\wedge e_l)\]
\[\Rightarrow tr(\bigwedge^2f)=\sum_{i<j}(a_{ii}a_{jj}-a_{ij}a_{ji})=\frac{1}{2}\sum_{i,j}(a_{ii}a_{jj}-a_{ij}a_{ji})= \dfrac{(\tr(f))^2 - \tr(f^2)}{2} \]
Per $S^2f$ si dimostra in maniera analoga. \qed













\newpage
\section{Prime proprietà delle rappresentazioni}

\begin{defn}[Rappresentazione]
	Sia $G$ un gruppo e $V$ uno spazio vettoriale. Una funzione $\rho: G \to GL(V)$ che manda ciascun elemento del gruppo in un'applicazione lineare invertibile di $V$
	si dice rappresentazione di $G$ se è un omomorfismo di gruppi, ovvero in parole semplici deve rispettare la regola di composizione. In formule, se $s, t \in G$ deve valere
	\[ \rho(st) v = \rho(s)\rho(t) v \qquad \forall v \in V, \quad \forall s,t \in G\]
\end{defn}

La dimensione di $V$ viene detta grado della rappresentazione.
Per il momento consideriamo solo rappresentazioni su spazi vettoriali complessi, ovvero supponiamo che $V$ sia $\C-$spazio vettoriale.

Con un piccolo abuso di linguaggio, spesso indicheremo con il nome di ``rappresentazione'' direttamente $V$, se è chiaro quale sia 
la rappresentazione $\rho$ che stiamo considerando. Spesso indicheremo gli spazi vettoriali
su cui è definita una rappresentazione $\rho$ con nomi del tipo $V_\rho$, dove il pedice ci ricorda la rappresentazione che stiamo considerando.

\begin{rem}
Detto in altri termini, una rappresentazione di un gruppo $G$ è un'azione lineare di $G$ su uno spazio vettoriale $V$.
Se non c'è ambiguità (cioè se è chiaro quale rappresentazione stiamo considerando)
possiamo scrivere semplicemente $g\cdot v$ in luogo di $\rho(g)v$ per allegerire la notazione.
\end{rem}

\begin{rem}
$\rho(G)$ è evidentemente un sottogruppo di $GL(V)$, quindi esistono sempre inversi, potenze e valgono tutte le cose che sono vere per i gruppi.
\end{rem}


\begin{exmp}
   La rappresentazione banale, di grado qualsiasi, indicata con $\rho_1$ che manda qualsiasi elemento di $g$ nell'identità di $GL(V_{\rho_1})$, ovvero
	\[ \rho(s ) = id_{V_{\rho_1}} \qquad \forall s \in G\]
\end{exmp}
\begin{exmp}
   Dato $S_n$, il segno di un elemento $s\in S_n$ è una rappresentazione di grado 1. Infatti si ha $sgn(st) = sgn(s) sgn(t)$.
\end{exmp}
\begin{exmp}
   L'azione naturale di $S_n$ sui vettori della base. Prendiamo quindi $G = S_n$ e uno spazio vettoriale di dimensione $n$, che quindi è isomorfo a $\C^n$. Prendiamo la base canonica di $\C^n$ e la indichiamo con $\{e_i\}_{i=1}^n$. Descriviamo la rappresentazione $\rho: S_n \to GL(\C^n)$ dicendo cosa fa agli elementi della base: per linearità si estenderà a tutto lo spazio.
	\[ \rho(s) e_i = e_{s(i)}\]
	Notare che in questo caso $\deg(\rho) = n$. Notiamo inoltre che se rappresentiamo nella base canonica le matrici associate a $\rho(s)$ queste matrici sono unitarie. Inoltre, ogni colonna (e anche ogni riga) contiene esattamente un 1 e tutti gli altri sono 0.
	
	Prendiamo come esempio $S_3$ e vediamo cosa succede. Notiamo innanzitutto che $ |S_3| = 3! = 6$
	FINISCI DI SCRIVERE
\end{exmp}







\begin{prop}
Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\forall g \in G$ la matrice $\rho(g)$ ammette una base di autovettori in $V_\rho$, ovvero è diagonalizzabile. Inoltre, tutti gli autovalori di $\rho(g)$ sono radici $n-$esime dell'unità.

\begin{note} Per ogni matrice in generale la base è diversa, quindi le varie matrici in generale \emph{non} sono simultaneamente diagonalizzabili.
Però se $G$ è abeliano tutte le matrici $\rho(s)$ sono simultaneamente diagonalizzabili. 
\end{note}
\label{prop:diagonalizzabilita rappresentazioni}
\end{prop}

\begin{proof} Se $G$ è un gruppo finito, allora esiste un intero positivo $k$ tale che $g^k = e$. Dato che $\rho:G\to GL(V_\rho)$ mantiene queste proprietà in quanto omomorfismo, dovrà essere $\rho(g)^k = id$.

Visto che il polinomio minimo di $\rho(g)$ non ha radici multiple, con il teorema di decomposizione primaria \eqref{thm:dec_primaria} si mostra facilmente che $\rho(g)$ è diagonalizzabile. Inoltre da questa formula è anche evidente che tutti gli autovalori di $\rho(g)$ hanno modulo $1$ e in particolare saranno radici $k-$esime dell'unità.

Ricordiamo un teorema di algebra lineare per mostrare che se $G$ è abeliano allora
tutte le matrici $\rho(g)$ sono simultaneamente diagonalizzabili:
due endomorfismi di uno spazio vettoriale diagonalizzabili sono simultaneamente diagonalizzabili se e solo se commutano tra loro.
\end{proof}




\begin{defn}[Omomorfismo di rappresentazioni]
Siano $\rho$ e $\sigma$ due rappresentazioni di $G$ su $V_{\rho}$ e $V_{\sigma}$ rispettivamente. Un omomorfismo di spazi vettorali $\varphi:V_{\rho}\to V_{\sigma}$ si dice \textit{omomorfismo di rappresentazioni} se
\[
	\forall\ a\in G, \forall\ v\in V_{\rho}\quad \varphi(\rho(a)(v)) = \sigma(a)(\varphi(v))
\]
oppure equivalentemente
\[
	\forall\ a\in G\quad \varphi\circ \rho(a) = \sigma(a)\circ \varphi
\]


%Siano $G, H$ due gruppi e $\rho: G \to V_\rho$ e $\sigma: H \to V_\sigma$ due loro rappresentazioni. Una funzione lineare da $V_\rho \to V_\sigma$ \footnote{Ovvero un omomorfismo da $V_\rho$ a $V_\sigma$} è un omomorfismo di rappresentazioni se rispetta la regola di composizione


%\[ \qquad \forall v,w \in V_\rho, V_\sigma\]


\end{defn}



\begin{defn}[Rappresentazioni isomorfe]
Due rappresentazioni si dicono \textit{isomorfe} se esiste un omomorfismo di rappresentazioni tra di loro che è anche bigettivo.
\end{defn}




\paragraph{Rappresentazioni di grado 1}\label{par:rappr_deg_1}
Dato un gruppo $G$, le sue rappresentazioni di grado $1$ sono per definizione
omomorfismi che vanno da $G$ all'insieme degli isomorfismi di $\C-$spazi vettoriali di dimensione $1$.
Senza perdere di generalità possiamo supporre che lo spazio vettoriale sia proprio $\C$. Dunque le
rappresentazioni di grado $1$ non sono altro che omomorfismi $G\to\C^*$.
\begin{thm}
Gli omomorfismi $G\to\C^*$ sono rappresentazioni di $G$ tra loro non isomorfe.
\end{thm}
\begin{proof}
Siano $\rho:G\to\C^*$ e $\sigma:G\to\C^*$ rappresentazioni isomorfe. Allora
esiste $\varphi:\C\to\C$ isomorfismo di $\C$ (ovvero $\varphi\in\C^*$) tale che per ogni $g\in G$ vale $\varphi \rho(g) = \sigma(g) \varphi$.
Visto che $\C^*$ è commutativo abbiamo allora che $\rho(g) = \sigma(g)$ per ogni $g\in G$, il che vuol dire che 
in realtà $\rho$ e $\sigma$ sono proprio la stessa rappresentazione.
\end{proof}

Negli esercizi sarà necessario trovare le possibili rappresentazioni di un gruppo $G$ (che qui supporremo finito), un buon punto di partenza è cercare per prima cosa le rappresentazioni di grado 1. Per fare questo c'è un metodo generale (indicheremo con $\rho$ la rappresentazione cercata e con $\mu_m$ il sottoinsieme di $\C$ che contiene le radici $m$-esime dell'unità):\footnote{stiamo cercando un omomorfismo di gruppi da $G$ a $GL(\C)=\C^*$}:
\begin{enumerate}
	\item cercare i generatori del gruppo $G$, che indicheremo con $g_1, \ldots, g_k$
	\item per ogni generatore $g_i$ trovare il suo ordine $n_i$ (ovvero il minimo intero $n_i$ tale che $g_i^{n_i}=e$)
	\item imporre che $\rho(g_i)\in \mu_{n_i}$ per ogni $i=1,\ldots,k$
	\item imporre infine che $\rho:G\to GL(\C)$ sia veramente un omomorfismo di gruppi (per ora abbiamo solo posto delle condizioni necessarie), per fare ciò bisogna controllare che le \textit{relazioni} con cui può essere presentato il gruppo siano rispettare nell'immagine\footnote{questo in parole povere significa che tutte le regole con cui vengono moltiplicati gli elementi devono essere rispettate, per dare una spiegazione formale servirebbero i prodotti liberi che però non sono necessari per questo corso}.
\end{enumerate}
Questo definisce un omomorfismo da $G$ a $\C^*$: dato $h\in G$ t.c. $h=g_{i_1}^{a_1}\cdots g_{i_t}^{a_t}$, allora $\rho(h) = \rho(g_{i_1})^{a_1}\cdots \rho(g_{i_t})^{a_t}$.
\begin{rem}
	Non è detto che tutte le rappresentazioni che si ottengono siano non isomorfe, questo metodo solamente le produce tutte.
\end{rem}


\begin{exmp}[Rappresentazioni di grado 1 di $C_n$]
Dato $G=C_n$, prendiamo un suo generatore $g$ (un qualsiasi elemento di ordine $n$), imponiamo che $\rho(g)\in \mu_n$ (ovvero $\rho(g)=e^{2k\pi i/n}$ per qualche $k\in \{0,\ldots,n-1\}$). Visto che $\forall h\in G\ \exists k\in \N$ t.c. $h=g^k$, ho già definito $\rho$ su ogni elemento di $G$.\newline
Ora abbiamo definito tutte le rappresentazioni di $C_n$ di grado 1, ma sono tutte distinte? In effetti mostriamo che in questo case il processo che abbiamo operato ha prodotto tutte rappresentazioni non isomorfe. Supponiamo di avere due rappresentazioni $\rho_1, \rho_2$ trovate con il metodo descritto sopra, supponiamo inoltre di avere $\varphi:\rho_1\to\rho_2$ un isomorfismo di rappresentazioni, vediamo che questo è assurdo: se fosse un isomorfismo di rappresentazioni dovrebbe valere che $\forall x\in \C, \forall g\in G$ $\varphi( \rho_1(g)x ) = \rho_2(g)( \varphi(x) )$, ma $\rho_1(g)$ è la moltiplicazione per uno scalare e $\varphi$ è lineare, quindi vorrebbe dire che $\rho_1(g)\varphi( x ) = \rho_2(g)( \varphi(x) )$, preso $x\neq 0$ si ottiene che $\rho_1(g)=\rho_2(g)\forall g\in G$, e questo è assurdo perchè differiscono almeno su un generatore di $G$.
\end{exmp}

L'esempio di prima era particolarmente semplice quindi non abbiamo dovuto faticare troppo, però il procedimento descritto è abbastanza laborioso con gruppi più complicati, vediamo un altro metodo che, conoscendo un quoziente ciclico di $G$, fornisce alcune rappresentazioni di grado 1 del gruppo.
\subparagraph{}
Sia $G$ un gruppo, $H\trianglelefteq G$ t.c. $G/H$ sia ciclico, allora trovando le rappresentazioni di $G/H$ di grado 1 siamo capaci di ricostruire delle rappresentazioni di grado 1 di $G$: consideriamo $\pi:G\to G/H$ la proiezione al quoziente, ovvero $\pi(g)=gH$, e un omomorfismo di gruppi $\rho:G/H\to \C^*$ (la rappresentazione di $G/H$), allora come omomorfismo $\sigma:G\to GL(\C)$ (ovvero una rappresentazione di $G$) prendiamo l'omomorfismo che fa commutare il seguente diagramma:
\[\tridiag G \pi {G/H} \rho {GL(\C)} \sigma\]
ovvero $\sigma(g)=\rho(\pi(g))$.

Questo metodo è particolarmente potente perché, per ogni rappresentazione $\sigma:G\to GL(\C)$, l'immagine è un gruppo ciclico\footnote{\'E abbastanza facile convincersene ricordando che ogni elemeno di $G$ deve essere mandato in una radice dell'unità.}, quindi, in virtù del primo teorema di omomorfismo\eqref{alg:primo_teo_omo}, l'immagine di una qualsiasi rappresentazione $\sigma$ di grado 1 è un quoziente ciclico di $G$.
Questo implica che in realtà il metodo esposto è in grado di trovare \textbf{tutte} le rappresentazioni di $G$ di grado $1$.


\begin{exmp}[Rappresentazioni di grado 1 di $S_3$]

\end{exmp}

\begin{exmp}[Rappresentazioni di grado 1 di $C_n \times C_n$]


(generalizzazione a prodotto di $C_{n_i}$)
\end{exmp}

\begin{rem}
	\'E possibile generalizzare ulteriormente il secondo procedimento descritto. Sia $G$ un gruppo, $H\trianglelefteq G$ tale che conosciamo le rappresentazioni (irriducibili) di $G/H$, allora riusciamo a ricostruire delle rappresentazioni di $G$ allo stesso modo: sia $\rho:G/H\to GL(V)$ rappresentazione di $G/H$, $\pi:G\to G/H$ la proiezione al quoziente, si considera lo stesso omomorfismo $\sigma$ di prima: $\sigma(g) = \rho(\pi(g))$.
\end{rem}













\newpage
\subsection{Operazioni con le rappresentazioni}

\begin{defn}[Somma di rappresentazioni]
  Date due diverse rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \ \sigma: G \to GL(V_\sigma)$ si può definire la rappresentazione somma $\rho + \sigma$ definita sullo spazio vettoriale $V_\rho \oplus V_\sigma$ definita in modo ovvio

  \[ (\rho + \sigma)(g) v = \rho(g) \pi_{V_\rho}(v) + \sigma(g) \pi_{V_\sigma}(v) \qquad \forall v \in V_\rho \oplus V_\sigma \]

  Questa definizione ha senso, infatti proiettando $v$ sui due spazi di partenza le due rappresentazioni sono definite e rimangono nello spazio di partenza. Si può poi fare la somma se riportiamo il tutto nello spazio più grande.
\label{defn:somma di rappresentazioni}
\end{defn}
Matricialmente $(\rho+\sigma)_g$ si rappresenta come
\[ [(\rho+\sigma)_g]= \begin{bmatrix}
[\rho_g] & 0\\ 
0 & [\sigma_g]
\end{bmatrix} \]
dove si intende ovviamente che la base ha i primi vettori in $V_{\rho}$, i secondi in $V_{\sigma}$.\\
\textsc{Osservazioni:}

\begin{enumerate}
\item $\deg(\rho+\sigma)=\deg(\rho)+\deg(\sigma)$
\item $\rho + \sigma \cong \sigma + \rho$
\item $\rho + (\sigma + \tau) \cong (\rho + \sigma ) + \tau$
\item Esiste l'elemento neutro che è la rappresentazione di grado 0 ma non esiste l'inverso.
\item se $\deg(\rho)=1$ allora $\rho(g^{-1})=\rho(g^{-1})^t=\rho^*(g)\Rightarrow \rho^*(g)=\rho(g)^{-1}$.

\end{enumerate}





\begin{defn}[Prodotto di rappresentazioni]
  Date due rappresentazioni dello stesso gruppo $G$, $\rho: G \to GL(V_\rho), \sigma: G \to GL(V_\sigma)$ possiamo definire il prodotto di rappresentazioni che si indica con $\rho \otimes \sigma$  ma anche con $\rho\sigma$\footnote{Quest'ultima notazione può portare a confusione in quanto può essere scambiata con la composizione se le due rappresentazioni sono definite sullo stesso spazio, quindi cercheremo di evitarla.} definita sullo spazio $V_\rho \otimes V_\sigma$ tale che

  \[ \rho \otimes \sigma(g) (v \otimes w) = \rho(g) v \otimes \sigma(g) w \qquad \forall v \in V_\rho, w \in V_\sigma\]

  Non è restrittivo dare la definizione solo per gli elementi di $V_\rho \otimes V_\sigma$ decomponibili, in quanto sappiamo che sono una base dello spazio. Gli altri si otterranno per linearità.
  
\label{defn:prodotto di rappresentazioni}
\end{defn}


\textsc{Osservazioni:}


\begin{enumerate}
\item $1\otimes \rho \cong \rho$
\item $\rho \otimes \sigma \cong \sigma \otimes \rho$
\item $0 \otimes \rho \cong 0$
\item $\rho \otimes (\sigma \otimes \tau) \cong (\rho \otimes \sigma)\otimes \tau$
\item $\rho \otimes (\sigma_1 + \sigma_2) \cong \rho \otimes \sigma_1 + \rho \otimes \sigma_2$

\end{enumerate}





\begin{defn}[Rappresentazione duale]
Sia $\rho$ una rappresentazione di $G$ su $V_\rho$. Allora la rappresentazione duale $\rho^*$ è la rappresentazione di $G$ su $V_\rho ^*$ tale che $\rho^*(s)=\rho(s^{-1})^t$
\label{defn:rappresentazione duale}
\end{defn}

\begin{defn}[Definizione equivalente di duale]
Sia $\rho$ una rappresentazione di $G$ su $V$. Definiamo la rappresentazione duale $\rho^*$ come l'unica rappresentazione tale che
\[\rho_g^*(f)(\rho_g(v))=f(v)\ \ \forall g\in G,\forall v\in V,\forall f\in V^*\]
\end{defn}

\begin{note}
$\rho^*(s)=\rho(s^{-1})^t=\left(\rho(s)^{-1}\right)^t=\left(\rho(s)^t\right)^{-1}$. Inoltre, notare che la presenza di inverso e trasposto fa in modo che $\rho^*(s)$ sia una rappresentazione.
\end{note}


\textsc{Osservazione:} vale
\[ (\rho + \sigma)^* \cong \rho^* + \sigma^* \]
\begin{proof}
Consideriamo la funzione $\Theta : (V_\rho \oplus V_\sigma)^*\to V_\rho ^* \oplus V_\sigma ^*$ definita da
\[ \Theta(f) = (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) \]
per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$, dove $\imath_{V_\rho}$ e $\imath_{V_\rho}$ sono le immersioni di $V_\rho$ e $V_\sigma$ dentro la loro somma diretta.
\`E facile osservare che si tratta di un'applicazione lineare. Consideriamo anche
la funzione $\Xi: V_\rho ^* \oplus V_\sigma ^* \to (V_\rho \oplus V_\sigma)^*$ definita da
\[ \Xi(h,k) = h\circ\pi_{V_\rho} + k\circ\pi_{V_\sigma} \]
per ogni coppia di funzionali $h\in V_\rho ^*$, $k\in V_\sigma ^*$,
dove $\pi_{V_\rho}$ e $\pi_{V_\sigma}$ indicano come al solito le proiezioni sui sottospazi. 
Anche $\Xi$ è lineare, inoltre è facile osservare che $\Theta$ e $\Xi$ sono una l'inversa dell'altra. Dunque sono degli 
isomorfismi. Resta da mostrare che $\Theta$ è omomorfismo di rappresentazioni. Prendiamo $g\in G$: dobbiamo mostrare che 
\[ (\rho^*+\sigma^*)(g) \circ \Theta = \Theta \circ (\rho + \sigma)^*(g) \]
ovvero che per ogni funzionale $f\in (V_\rho \oplus V_\sigma)^*$ vale
\[ [(\rho^*+\sigma^*)(g)] (f\circ \imath_{V_\rho}, f\circ \imath_{V_\sigma}) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che si riscrive:
\[ ([\rho^*(g)](f\circ \imath_{V_\rho}), [\sigma^*(g)](f\circ \imath_{V_\sigma})) = ([(\rho + \sigma)^*(g)f]\circ \imath_{V_\rho}, [(\rho + \sigma)^*(g)f]\circ \imath_{V_\sigma}) \]
che è equivalente a:
\[ (f\circ \imath_{V_\rho} \circ [\rho(g^{-1})], f\circ \imath_{V_\sigma}\circ[\sigma(g^{-1})]) = (f\circ [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho} , f\circ[(\rho+\sigma)(g^{-1})]\circ \imath_{V_\sigma}) \]
Per ottenere la tesi basta osservare che valgono
\[ \imath_{V_\rho} \circ [\rho(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\rho}\]
\[\imath_{V_\sigma} \circ [\sigma(g^{-1})] = [(\rho+\sigma)(g^{-1})] \circ \imath_{V_\sigma}\]
Notiamo infine che l'isomorfismo trovato è canonico, ovvero non dipende da alcuna scelta delle basi.
\end{proof}





\begin{defn}[Rappresentazione regolare]
  Consideriamo un gruppo $G$, per semplicità finito, e consideriamo uno spazio vettoriale $V_\rho$ di dimensione $|G|$ su $\C$. Una base di questo spazio ha sicuramente dimensione $|G|$. Possiamo indicare gli elementi della base con $e_g, \ \ \forall g \in G$. Un generico vettore di questo spazio si scrive quindi come

  \[ v = \dsum_{g \in G} a_g e_g \]

  Dove $a_g$ sono dei numeri complessi. Possiamo definire una rappresentazione di $G$ su questo spazio in questo modo:

  \[ \rho(h)v = \rho(h) \dsum_{g \in G} a_g e_g = \dsum_{g\in G} a_g \rho(h) e_g := \dsum_{g \in G} a_g e_{gh} \]

  Notare che questa definizione ha senso in quanto essendo $G$ un gruppo, $gh \in G$ e quindi sicuramente $e_{gh}$ è un elemento della base. Questa particolare rappresentazione di $G$ si chiama \emph{rappresentazione regolare} di $G$


  
  \label{defn:rappresentazione regolare}
\end{defn}



\subsection{Sottospazi invarianti e scomposizione delle rappresentazioni}



\begin{defn}[Sottorappresentazione]
Sia $\rho$ una rappresentazione di $G$ su $V_{\rho}$, una sottorappresentazione di $\rho$ è un sottospazio vettoriale $W\subseteq V_{\rho}$ tale che $\rho(s)(W)\subseteq W\ \forall\ s\in G$. Posso definire una rappresentazione $\sigma$ con $V_{\sigma}=W$ e $\sigma(s)=\rho(s)|_W$ (la indicherò con $\sigma\subseteq \rho$).
\end{defn}

\textbf{Notazione:} $V_{\rho}^G=\{v\in V_{\rho}: \rho(s)v=v\ \forall s\in G\}$.


\begin{defn}[Rappresentazione irriducibile]
Una rappresentazione $\rho$ di $G$ è \textit{irriducibile} se
\begin{enumerate}
	\item $\rho \neq 0$ ($\deg(\rho) \geq 1$)
	\item $\rho$ non ha sottorappresentazioni non banali (diverse da 0 e $V_{\rho}$).
\end{enumerate}

\end{defn}


\begin{defn}[Rappresentazione completamente riducibile]
Una rappresentazione si dice completamente riducibile se si può scrivere come somma di rappresentazioni irriducibili.
\end{defn}


\begin{rem}
Attenzione al gioco di parole in italiano: una rappresentazione irriducibile è completamente riducibile. Il nome della definizione può in effetti portare a confusione.
\end{rem}


\begin{rem} Normalmente la cosa che si fa più spesso in teoria della rappresentazione è cercare di scomporre la rappresentazione di un gruppo come somma di rappresentazioni irriducibili. Vedremo quindi adesso diversi teoremi che ci aiuteranno in questi problemi.

\end{rem}



\begin{exmp}[Rappresentazione regolare di $S_3$]


\end{exmp}



\begin{thm}[Le rappresentazioni di un gruppo finito sono completamente riducibili]
  Sia $G$ un gruppo finito e $\rho: G \to GL(V_\rho)$ una sua rappresentazione. Allora $\rho$ è completamente riducibile.
  \label{thm:gruppo finito completamente riducibile}
\end{thm}
Per dimostrare questo teorema ci servono diversi lemmi che enunciamo e andiamo a dimostrare. Finiti i lemmi seguirà la dimostrazione.


\begin{prop}[Prodotto hermitiano invariante] Sia $G$ un gruppo finito e $\rho: G \to V_\rho$ una sua rappresentazione. Allora lo spazio vettoriale $V_\rho$ ammette una forma hermitiana invariante sotto l'azione di $G$, ovvero un prodotto tale che $h(v,w) = h(\rho(g) v, \rho(g) w) \ \forall v,w\in V_\rho, \forall g \in G$
\label{thm:esistenza hermitiana}
\end{prop}

\textsc{Dimostrazione:}

Lo spazio $V_\rho$ ammette sicuramente una forma hermitiana, che chiamiamo $h$. Ora andiamo a fare una sorta di media per trasformare questa forma in una invariante. Consideriamo quindi

\[h_G(v, w) := \dfrac{1}{|G|} \dsum_{g \in G} h(\rho(g)v,\rho(g) w) \]

\'E abbastanza facile mostrare adesso che effettivamente $h_G$ è invariante sotto l'azione di $G$. Infatti,

\[ h_G(\rho(h)v, \rho(h) w) = \dsum_{g \in G} h(\rho(gh)v , \rho(gh) w)\]

Ma dato che $G$ è un gruppo, questo vuol dire solo \emph{far partire la somma da un indice diverso.} Di conseguenza $h_G$ è $G$-invariante \qed


\begin{lemma}
Sia $h: V_\rho \times V_\rho \to \C$ una forma hermitiana definita positiva e invariante per $\rho: G \to GL(V_\rho)$ e sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora se $W^\perp$ è l'ortogonale di $W$, $\rho|_{W^\perp}: G \to GL(W^\perp)$ è una sottorappresentazione.

\end{lemma}

\textsc{Dimostrazione:}

Per noti teoremi di algebra lineare sappiamo che

\[ V_\rho = W \oplus W^\perp \]

Di conseguenza un generico vettore di $V_\rho$ si potrà scrivere come somma di $w_1 + w_2$, con $w_1 \in W$ e $w_2 \in W^\perp$. Inoltre sappiamo che $\rho_{|_W}$ è una sottorappresentazione. Mostriamo che anche $\rho_{|_W^\perp}$ è una sottorappresentazione: quello che dobbiamo mostrare è che $\rho(g) w_2 \in W^\perp \ \forall g \in G$. Dato che abbiamo un prodotto hermitiano la cosa più facile da verificare è che $w_2$ sia ortogonale a $W$. Consideriamo quindi l'espressione

\[ 0 = h(w_1, w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp\]

Ma noi sappiamo che $h$ è $G$-invariante, quindi

\[ 0 = h(w_1, w_2) = h(\rho(g) w_1, \rho(g) w_2) \qquad \forall w_1 \in W, \forall w_2 \in W^\perp \]

E dato che $\rho(g) W = W$ per ipotesi, abbiamo mostrato che $\rho(w_2) \in W^\perp$. \qed




\begin{lemma}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo finito $G$. Sia $\rho|_W: G \to GL(W)$ una sottorappresentazione di $\rho$. Allora esiste una sottorappresentazione $\sigma: G \to GL(W')$ tale che

\[\rho = \rho|_W + \sigma \]
\end{lemma}


\textsc{Dimostrazione:} La tesi segue dai due lemmi precedenti. L'ipotesi di gruppo finito si usa per l'esistenza della forma hermitiana invariante. \qed



\begin{rem} Notare che il teorema precedente è falso per gruppi infiniti. Un esempio si può costruire prendendo $G=\Z$, $V=\C^2$, e come rappresentazione
\begin{align*}
	\rho:&\Z\to GL(\C^2)\\
	&k\to M^k
\end{align*}
dove $M=\begin{pmatrix}
        	1 & 1\\
        	0 & 1
        \end{pmatrix}$ è scritta nella base canonica.\newline
Si vede subito che una sottorappresentazine è $Span(e_1)$ essendo $e_1$ autovettore per ogni $\rho(k)$, ma non esiste un suo complementare $G$-invariante: se esistesse avrebbe dimesione 1, quindi avremmo diagonalizzato $\rho(k)\ \forall k\in \Z$, ma sappiamo che tali endomorfismi non sono diagonalizzabili. 
\end{rem}


\textsc{Dimostrazione del teorema \ref{thm:gruppo finito completamente riducibile}:}

Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di $G$. Se $\rho$ è irriducibile, allora è completamente riducibile e quindi segue la tesi. Se invece esiste un sottospazio invariante $W$, allora per i lemmi precedenti esiste $Z \subset V_\rho$ tale che $V_\rho = W \oplus Z$ e tale che $Z$ sia una sottorappresentazione. Per induzione si procede fino ad ottenere la tesi. \qed





\begin{thm} Se $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ sono rappresentazioni di $G$ e $f: V_\rho \to V_\sigma$ è un omomorfismo di rappresentazioni, allora $Im(f)$ è una sottorappresentazione di $\sigma$ e $Ker(f)$ è una sottorappresentazione di $V_\rho$
\end{thm}
\begin{proof}
Se $v\in Ker(f)$ allora per la definizione di omomorfismo di rappresentazioni ho che $\forall s\in G$ $f(\rho(s)v)=\sigma(s)f(v)=0$ e quindi $\rho(s)v\in Ker(f)$. Allo stesso modo, se $w\in Im(f)$ allora $w=f(v)$ per qualche $v\in V_\rho$ e quindi sempre per la definizione di omomorfismo di rappresentazione $\sigma(s)w=\sigma(s)f(v)=f(\rho(s)v)\in Im(f)$
\end{proof}




\begin{thm}Sia $G$ un gruppo abeliano finito. Allora ogni rappresentazione di $G$ è isomorfa alla somma di rappresentazioni di grado 1.
\end{thm}
\begin{proof}
	\'E una conseguenza immediata della proposizione \eqref{prop:diagonalizzabilita rappresentazioni}: sia $\rho:G\to GL(V_{\rho})$, con $dim V_{\rho} = n$, dato che $G$ è finito $\forall g\in G$ $\rho(g)$ è diagonalizzabile inoltre, visto che è abeliano, si sfrutta l'osservazione alla fine della proposizione per dedurre che le $\rho(g)$ sono simultaneamente diagonalizzabili. A questo punto il teorema è dimostrato: sia $\{v_1,\ldots,v_n\}$ una base comune di autovettori, per ogni $1\leq i\leq n$ $Span(v_i)$ è una sottorappresentazione di grado 1 di $G$ (perchè invariante per $G$), visto che $V_{\rho}=Span(v_1)\oplus\ldots\oplus Span(v_n)$, allora $\rho\cong\rho_1+\ldots+\rho_n$, dove $\rho_i:G\to GL(Span(v_i))$.
\end{proof}


\begin{prop} La rappresentazione regolare $\mathcal{R}$ di $C_n$ è isomorfa alla somma delle $n$ rappresentazioni irriducibili di grado 1 di $C_n$.

\end{prop}


\begin{lemma}
Date $\rho_1, \rho_2, \sigma$ rappresentazioni di $G$, allora

\[Hom(\rho_1 + \rho_2, \sigma) \cong Hom(\rho_1, \sigma) \oplus Hom(\rho_2, \sigma)\]

\end{lemma}

\textsc{Dimostrazione:}


% Consideriamo il lato sinistro dell'uguaglianza. Un generico elemento $\phi \in Hom(\rho_1 + \rho_2, \sigma)$ è una mappa                                                                         %
%                                                                                                                                                                                                  %
% \[ \phi: V_{\rho_1} \oplus V_{\rho_2} \to V_\sigma \]                                                                                                                                            %
%                                                                                                                                                                                                  %
% che commuti con la rappresentazione, ovvero tale che                                                                                                                                             %
%                                                                                                                                                                                                  %
% \[ \phi\left( (\rho_1 + \rho_2 ) (g) v\right) = \sigma(g) \phi(v) \qquad \forall v \in V_{\rho_1} \oplus V_{\rho_2} \ , \forall g \in G\]                                                        %
%                                                                                                                                                                                                  %
%                                                                                                                                                                                                  %
% Esibiamo una mappa $\Xi: Hom(\rho_1 + \rho_2, \sigma) \to Hom(\rho_1, \sigma) \oplus Hom(\rho_2, \sigma)$ nel modo più sensato e mostriamo che è un isomorfismo. In particolare, definiamo $\Xi$ %
%                                                                                                                                                                                                  %
%                                                                                                                                                                                                  %
% \[ \Xi(\phi)(v) =  \phi\left( \pi_{V_{\rho_1} } (v)\right) + \phi\left( \pi_{V_{\rho_2}} (v) \right)\]                                                                                           %


Consideriamo il lato destro dell'uguaglianza. Un elemento $\phi_i \in Hom(\rho_i, \sigma)$ si potrà scrivere

\[
\phi_i :V_{\rho_i} \to V_\sigma | \quad \phi_i\left( \rho_i(g) v\right) = \sigma(g) \phi_i(v) \qquad \forall v \in V_{\rho_i}, \ \forall g \in G 
\]

Esibiamo ora una mappa 
\begin{align*}
\Xi :  Hom(\rho_1, \sigma) \oplus Hom(\rho_2, \sigma) &\to Hom(\rho_1 + \rho_2, \sigma) \\
\phi_1 , \phi_2                                       &\to \phi_1 + \phi_2 \\
\end{align*}
 in modo ovvio e mostriamo che è un isomorfismo. 

\[ \Xi (\phi_1(v) + \phi_2(v)) = (\phi_1 + \phi_2) (v) \]


Innanzitutto la funzione $\phi_1 + \phi_2 : V_{\rho_1} \oplus V_{\rho_2}$ è effettivamente un omomorfismo di rappresentazioni e il motivo è che le due rappresentazioni in un certo senso agiscono separatamente sui due spazi senza interferire, quindi la verifica è immediata. 

$\Xi$ è inettiva in quanto per l'appunto le due rappresentazioni agiscono in modo separato sui due spazi e quindi affinché due coppie di $\phi_i$ diano lo stesso risultato, devono essere uguali le coppie.

$\Xi$ inoltre è suriettiva in quanto PERCHÉ? 

\qed


\begin{thm}[Lemma di Schur]
Siano $\rho: G \to GL(V_\rho)$ e $\sigma: G \to GL(V_\sigma)$ due rappresentazioni irriducibili di $G$ gruppo finito e $\phi:V_\rho \to V_\sigma$ un omomorfismo di rappresentazioni, allora $\phi$ è un isomorfismo oppure è identicamente nullo. Se poi $f:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni e lo spazio vettoriale $V_\rho$ è su $\C$ o su un campo algebricamente chiuso $\K$, allora $f$ è una moltiplicazione per scalare.
\end{thm}
\begin{proof}
Supponiamo che $\phi\neq 0$, allora sappiamo che $Ker(\phi)\subseteq V_\rho$ è una sottorappresentazione di $\rho$, ma $\rho$ è irriducibile e quindi $Ker(\phi)=0\Rightarrow \phi$ iniettiva. Ma anche $Im(\phi)\subseteq V_{\sigma}$ è una sottorappresentazione di $\sigma$ e, non essendo nulla ed essendo $\sigma$ irriducibile, coincide con tutto $V_\sigma \Rightarrow \phi$ suriettiva, da cui $\phi$ è un isomorfismo.
Consideriamo ora $f$: sia $\lambda$ un autovalore di $f$, che esiste perché $G$ è finito e stiamo lavorando su $\C$, allora $f-\lambda Id:V_\rho\to V_\rho$ è un omomorfismo di rappresentazioni. Ma non è iniettivo, perché c'è almeno un autovettore relativo a $\lambda$, e quindi per la prima parte del lemma di Schur ho che $f-\lambda Id$ è identicamente nullo, da cui ricaviamo che $f$ è la moltiplicazione per uno scalare ($\lambda$).
\end{proof}

Come immediato corollario, notiamo che, prese due rappresentazioni irriducibili $\rho$ e $\sigma$, si ha che $\dim\left(\Hom(\rho,\sigma)\right)$ è uguale a 1 se $\rho\simeq\sigma$, ed è uguale a 0 se $\rho\not\simeq\sigma$. Generalizzando, se $\rho$ si scompone in somma di rappresentazioni irriducibili come $\rho=\sum n_i\rho_i$, allora $\dim\left(\Hom(\rho,\rho_i)\right)=n_i$.



\begin{thm}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione e 

\[\rho = \dsum_{i=1}^N n_i \rho_i \]

una sua scomposizione come somma di rappresentazioni irriducibili a due a due non isomorfe. Allora la scomposizione è unica.
\end{thm}






\begin{thm}
Sia $\mathcal{R}$ la rappresentazione regolare di $G$, un gruppo finito, e sia 
\[ \mathcal{R} = \dsum_{i=1}^Nn_i \rho_i,\]
con $\rho_i$ irriducibili e a due a due non isomorfe. Allora ogni rappresentazione irriducibile di $G$ è isomorfa ad una $\rho_i$. Inoltre $n_i = \deg(\rho_i).$ 
\label{thm: teorema importantissimo}
\end{thm}
\begin{proof}
Per il corollario del Lemma di Schur, la tesi è equivalente al seguente fatto: se $\rho$ è una rappresentazione irriducibile di $G$, allora $\dim\left(\Hom(\mathcal{R},\rho)\right)=\deg\rho$ (in realtà non useremo l'ipotesi di irriducibilità di $\rho$). Costruiamo dunque un isomorfismo tra gli spazi vettoriali $\Hom(\mathcal{R},\rho)$ e $V_\rho$.

Chiamando $e_g$ l'elemento della base di $V_\mathcal{R}$ associato a $g\in G$, notiamo preliminarmente che, presa $\varphi\in\Hom(\mathcal{R},\rho)$, vale $\varphi(e_g)=\varphi(\mathcal{R}(g)e_{1})=\rho(g)\varphi(e_1)$, dove con $1$ si indica l'elemento neutro di $G$. Quindi, il valore di $\varphi(e_1)$ determina completamente $\varphi(e_g)$ per ogni $g$, e quindi determina completamente $\varphi$.

Sia allora $\Phi\colon\Hom(\mathcal{R},\rho)\to V_\rho$ tale che $\Phi(\varphi)=\varphi(e_{1})$. Per quanto appena visto, $\Phi$ è iniettiva. D'altra parte, lo stesso ragionamento ci permette anche di dimostrare facilmente che $\Phi$ è suriettiva: per ogni $v\in V_\rho$, la funzione $\varphi_v$ tale che $\varphi_v(e_g)=\rho(g)v$ è tale che $\Phi(\varphi_v)=\varphi_v(e_1)=\rho(1)v=v$. La linearità di $\Phi$ consente di concludere.
\end{proof}


\begin{cor}
Se $G$ è abeliano allora ha $|G|$ rappresentazioni irriducibili di grado 1 e $\mathcal{R}$ è la somma di queste.
\end{cor}


\begin{cor}
Sia $G$ un gruppo finito. $G$ ha un numero finito di rappresentazioni irriducibili, a meno di isomorfismi. Inoltre
\[|G| = \dsum n_i^2\]
\end{cor}






\newpage
\subsection{Alcuni isomorfismi notevoli}
Sia $G$ un gruppo e siano $\rho:G\to GL(V)$ e $\sigma:G\to GL(W)$ sue rappresentazioni.
Sappiamo bene che l'insieme delle applicazioni lineari da $V$ in $W$,
che denotiamo con $\Hom(V, W)$, è uno spazio vettoriale.
Quello che vogliamo fare ora è rendere tale spazio vettoriale una rappresentazione di $G$,
ovvero vogliamo definire un'azione lineare di $G$ su $\Hom(V, W)$. 
Il modo naturale di farlo è il seguente:
sia $\varphi\in\Hom(V, W)$. Definiamo $g\cdot \varphi\in\Hom(V, W)$ come la funzione che fa commutare il diagramma:
\[ \quaddiag V \varphi W {\sigma(g)} W {\rho(g)} V {g\cdot \varphi} \]
Ovvero esplicitamente: $g\cdot\varphi = \sigma(g) \circ \varphi\circ \rho(g^{-1})$

La verifica che si tratti di un'azione di gruppo è molto semplice, inoltre è evidente dalla definizione che l'azione è lineare:
dunque abbiamo davvero dotato $\Hom(V, W)$ di una struttura di $G-$rappresentazione.

Sappiamo già che $\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come spazi vettoriali.
Quello che vogliamo vedere è che sono isomorfi anche come $G-$rappresentazioni.
Ovviamente la rappresentazione che si considera su $V^*\otimes W$ è la rappresentazione $\rho^*\otimes\sigma$, che per semplicità indichiamo con $\tau$.

\begin{thm}
$\Hom(V, W)$ e $V^*\otimes W$ sono isomorfi come $G-$rappresentazioni.
\label{thm: isov*w}
\end{thm}
\begin{proof}
Conosciamo già un isomomorfismo di spazi vettoriali $\Psi:V^*\otimes W \to \Hom(V, W)$, costruito in
maniera tale che per ogni $f\in V^*, w\in W, v\in V$ si abbia $\Psi(f\otimes w)(v) = f(v)w$.
Mostriamo che $\Psi$ è omomorfismo di rappresentazioni, ovvero che fissato $g\in G$ il seguente diagramma commuta:
\[ \quaddiag {V^*\otimes W} \Psi {\Hom(V, W)} {g} {\Hom(V, W)} {\tau(g)} {V^*\otimes W} \Psi \]
Ovvero dobbiamo controllare se
\[ g\circ \Psi = \Psi \circ \tau(g)\]
Possiamo limitarci a fare le verifiche sugli elementi decomponibili di $V^*\otimes W$, in
quanto essi generano tutto lo spazio. Prendiamo dunque $f\in V^*$, $w\in W$ e consideriamo $f\otimes w$. Vogliamo mostrare che
\[ g(\Psi(f\otimes w)) = \Psi(\tau(g)(f\otimes w))\]
Quindi prendiamo $v\in V$ e vediamo dove viene mandato dalla funzione a sinistra:
\begin{align*}
         & g(\Psi(f\otimes w))\ (v)\\
=  \qquad& \sigma(g)\ \Psi(f\otimes w)\ (\rho(g^{-1})\ v)\\
=  \qquad& \sigma(g)\ f(\rho(g^{-1})\ v)\ w\\
=  \qquad& f(\rho(g^{-1})\ v)\ \sigma(g)\ w
\end{align*}
Mentre dalla funzione a destra viene mandato in:
\begin{align*}
         & \Psi(\tau(g)(f\otimes w))\ (v)\\
=  \qquad& \Psi([f\circ\rho(g^{-1})] \otimes [\sigma(g)w])\ (v)\\
=  \qquad& f(\rho(g^{-1})v)\ \sigma(g)\ w
\end{align*}
Avendo ottenuto lo stesso risultato in entrambi i casi, si ha la tesi.
\end{proof}




\newpage
\section{Teoria dei caratteri}


\begin{defn}
Sia $\rho: G \to GL(V_\rho)$ una rappresentazione di un gruppo $G$. Definiamo carattere di $\rho$ la funzione che associa ad ogni elemento del gruppo $G$ la traccia della matrice associata all'elemento, ovvero

\[\chi_\rho(s) := \tr(\rho(s)) \qquad \forall s \in G \]
Notare che $\chi_{\rho}$ è una funzione che va dal gruppo in $\C$, ovvero $\chi_{\rho}: G \to \C$

\end{defn}

Vediamo delle proprietà elementari del carattere

\textsc{Osservazioni:}
\begin{enumerate}
	\item Se $deg(\rho) = 1$ allora il carattere di $s$ è uguale a $\rho(s)$
	\item $\chi_{\rho_1} = deg(\rho)$. \footnote{Al solito $\rho_1$ è la rappresentazione che manda ogni elemento nell'identità di $V_\rho$}\\
	Questo è vero poichè $[\rho_1]=I_n\Rightarrow \tr(\rho_1)=n$ ed $n=deg(\rho)$.
	\item $\chi_{\rho + \sigma}(s) = \chi_\rho(s) + \chi_\sigma(s)$.\\ 
	Questo è dovuto al fatto che la somma di rappresentazioni si può scrivere come matrice a blocchi. Una volta scritto così è evidente il risultato.
	\item $\chi_{\rho\sigma}(s) = \chi_\rho(s)\chi_\sigma(s)$. In particolare dunque $\chi_{\rho^2}=(\chi_\rho)^2$.\\ 
	Questo deriva immediatamente dal teorema \ref{thm: tracciaprodotto}.
	\item $\chi_{\rho}(s^{-1})=\overline{\chi_{\rho}(s)}$\\
Essendo $G$ un gruppo finito, $\forall s\in G\ \rho(s)^n = id$ dove $n=|G|$: dunque tutti gli autovalori di $\rho(s)$ sono radici ennesime dell'unità e $\rho(s)$ è diagonalizzabile\footnote{Si veda la proposizione \ref{prop:diagonalizzabilita rappresentazioni}}. In tale base è evidente che:
$$\chi_{\rho}(s^{-1})=\tr(\rho (s^{-1}))=\tr(\rho (s)^{-1})=\sum_i\lambda_i^{-1}=\sum_i\overline{\lambda_i}=\overline{\tr(\rho(s))}=\overline{\chi_{\rho}(s)}$$
in quanto, avendo gli autovalori modulo 1, l'inverso coincide con il coniugio.  	
	\item $\chi_{\rho^*}(s)\footnote{Ricordiamo che $\rho^*(s) = (\rho(s)^{-1})^*$} = \overline{\chi_\rho(s)}$.\\
		Per l'osservazione precedente vale che
		$$\chi_{\rho^*}(s)=\tr(^t\rho(s^{-1}))=\tr(\rho(s^{-1}))=\overline{\tr(\rho(s))}=\overline{\chi_\rho(s)}$$
	\item $\chi_{\rho}(hsh^{-1})=\chi_{\rho}(s)$ ovvero $\chi_\rho$ è costante sulle classi di coniugio di $G$. La motivazione è semplice: se due elementi sono coniugati tra loro questo significa che le matrici corrispondenti saranno simili e la traccia è un invariante di similitudine.
	
Di conseguenza, non sarà necessario calcolare il carattere per ogni elemento del gruppo ma basterà farlo per le classi di coniugio di $G$.

Le funzioni che costanti sulle classi di coniugio di un gruppo vengono dette $funzioni\ di\ classe$. L'insieme delle funzioni di classe di un gruppo viene normalmente indicato con $Cl(G)$ e si verifica che esso è un sottospazio di $\mathbb{C}^G$.
	\item Supponiamo di avere una rappresentazione per permutazioni. Sia $I$ un insieme finito e $G$ un gruppo allora 
$$\chi_{\rho_{I}}(s)=\#punti\ fissi\ di\ \rho_I(s)=|I^s|$$
dove $I^s:=\{i\in I| s\circ i=i\}$. La veridicità di questo fatto si vede scrivendo esplicitamente la matrice che rappresenta $\rho_I(s)$.
	\item Consideriamo la rappresentazione per permutazioni regolare $R$. Calcoliamone il carattere:
	\[ \chi_{\mathcal{R}}(s) = \begin{cases}
|G| \qquad \text{se } s=id \\
0 \qquad \text{se } s\neq id\\
\end{cases} \]
semplicemente perchè $s\circ g=g\Leftrightarrow s=id$.
\end{enumerate}
\textbf{Esempio:} $G=S_3$, $I=\{1,2,3\}$. Allora

\[ \chi_{\rho_I}(s) = \begin{cases}
3 \qquad \text{se } s=id \\
1 \qquad \text{se } s\ \text{è una trasposizione}\\
0 \qquad \text{se } s\ \text{è un treciclo}\\
\end{cases} \]
Ricordandoci che $\chi_{\rho_I}=\chi_{1+\rho}$ si ha che 
\[ \chi_{\rho}(s) = \begin{cases}
2 \qquad \ \ \text{se } s=id \\
0 \qquad \ \ \text{se } s\ \text{è una trasposizione}\\
-1\qquad \text{se } s\ \text{è un treciclo}\\
\end{cases} \]

\begin{defn}[Prodotto hermitiano dei caratteri]

\[ \langle f | g \rangle = \dfrac{1}{|G|} \dsum_{s \in G} f(s)\overline{ g(s)} \]

\end{defn}


\begin{thm}[Relazioni di ortogonalità]
Se $\rho$ e $\sigma$ sono rappresentazioni irriducibili di $G$, allora vale

\[\langle \chi_{\rho}|\chi_{\sigma} \rangle = \begin{cases}
1 \qquad \text{se } \rho \cong \sigma \\
0 \qquad \text{altrimenti }\\
\end{cases} \]
\label{relazione di ortogonalita}
\end{thm}

Per dimostrare questo teorema abbiamo bisogno di un lemma che ora enunciamo e dimostriamo.




\begin{lemma}
Se $(\rho, V_\rho)$ e $(\sigma, V_\sigma)$ sono rappresentazioni \footnote{Non necessariamente irriducibili} di $G$, allora vale

\[ \langle \chi_\sigma | \chi_\rho \rangle  = dim(Hom (\rho, \sigma))\] 
\label{lemma relazioni ortogonalita}
\end{lemma}
\begin{proof}
L'idea principale per dimostrare questo lemma è di ridurci al caso più facile in cui una delle due rappresentazioni è quella banale. Per farlo notiamo un paio di cose
\[ \langle \chi_\sigma | \chi_\rho \rangle = \dfrac{1}{|G|} \dsum_{s\in G} \chi_\sigma(s) \overline{\chi_\rho(s)} = \dfrac{1}{|G|} \dsum_{s\in G} {\chi_{\rho^*}(s)} \chi_{\sigma}(s) = \dfrac{1}{|G|} \dsum_{s\in G} \chi_{\rho^*\sigma}(s)  = \langle \chi_{\rho^*\sigma}  | 1 \rangle\]
Siamo passati da due rappresentazioni ad una sola. In particolare lo spazio vettoriale su cui agisce questa rappresentazione è 
\[ V_{\rho^* \sigma} = V_{\rho}^* \otimes V_\sigma \cong \Hom(V_\rho, V_\sigma)\]
dove l'isomorfismo di rappresentazioni è quello dimostrato nel teorema \ref{thm: isov*w}.
In particolare abbiamo che
\[ (V_{\rho^*\sigma})^G \cong Hom(V_\rho, V_\sigma)^G \cong Hom(\rho, \sigma)\]
Per cui dato che noi stiamo cercando $dim(Hom(\rho, \sigma))$, basterà trovare $dim(V_{\rho^*\sigma})^G$.
\begin{lemma}
Sia $\rho$ una rappresentazione di un gruppo finito $G$. Allora
$dim(V_{\rho})^G=\tr(T)$ dove
\[ T = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s).\]
\end{lemma}

Prima di iniziare la dimostrazione diamo una giustificazione della scelta della applicazione lineare $T$: questa è stata dettata dal fatto che
\[ \tr(T)=\tr(\dfrac{1}{|G|} \dsum_{s\in G} \rho(s))= \dfrac{1}{|G|} \dsum_{s\in G} \tr(\rho(s))= \langle \chi_\rho| 1 \rangle \]
Si nota anzitutto che 

\[ \rho(t) T(v) = \dfrac{1}{|G|} \dsum_{s\in G} \rho(t)\rho(s) v = \dfrac{1}{|G|} \dsum_{s\in G} \rho(s) = T(v)\]

ovvero $ImT\subseteq V_\rho^G$. D'altra parte se $v \in V_\rho^G$ allora è chiaro che $Tv = v$ (basta applicare la definizione). Per cui $ImT = V_\rho^G$. 

Inoltre 

\[T(Tv) = \ldots = Tv \qquad \text{verifica banale}\]

Per cui $T$ è un proiettore. A questo punto sappiamo dall'algebra lineare che

\[ V_\rho = Ker T \oplus Im T = Ker T \oplus V_\rho^G\]

Per cui $trT = dimImT = dimV_\rho^G$. Ma dalla catena di deduzioni che abbiamo fatto

\[dim(Hom(V_\rho, V_\sigma)) = dim(V_{\rho^*\sigma}^G) = trT = \langle \chi_{\rho^*\sigma} | 1 \rangle = \langle \chi_\sigma | \chi_\rho \rangle\]

\end{proof}

\textbf{Osservazione:}
\[\langle \chi_\sigma | \chi_\rho \rangle=\overline{\langle \chi_\rho | \chi_\sigma \rangle}\]
ovvero per quanto appena dimostrato
\[ dim(Hom(V_\rho, V_\sigma))=\overline{dim(Hom(V_\sigma, V_\rho))}\]
tuttavia essendo dei numeri naturali deduciamo che le eguaglianze sussistono anche senza il coniugio.












\textsc{Dimostrazione del teorema \ref{relazione di ortogonalita}:}

A questo punto la tesi del teorema \ref{relazione di ortogonalita} segue dal lemma precedente applicato insieme al lemma di Schur. \qed


\textsc{Osservazioni:}

\begin{itemize}
\item Ricordiamo che se $\rho$ è una rappresentazione di $G$, allora $\rho$ si può scrivere in modo unico come 

\[ \rho = \dsum_i n_i \rho_i\]

Dove le $\rho_i$ sono le rappresentazioni irriducibili di $G$ e gli $n_i$ sono numeri naturali $\geq 0$. Dall'equazione scritta sopra segue subito che

\[ \chi_\rho = \dsum_i n_i \chi_{\rho_i}\]

E possiamo ottenere un'informazione utile prendendo il prodotto scalare dell'equazione precedente con il carattere di una delle rappresentazioni $\rho_i$

\[ \langle \chi_\rho | \chi_{\rho_j} \rangle = \dsum_i n_i \langle \chi_{\rho_i} | \chi_{\rho_j} \rangle \Rightarrow n_i \delta_{ij} = \langle \chi_\rho | \chi_{\rho_j} \rangle \Rightarrow n_i = \langle \chi_\rho | \chi_{\rho_i} \rangle\]

\item Caso particolare interessante del fatto precedente riguarda la rappresentazione regolare di un gruppo. Difatti come sappiamo,

\[ \chi_{\mathcal{R}}(s) = 
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]
Quindi considerando una sottorappresentazione  si ha che 
\[
\langle \chi_{\mathcal{R}} | \chi_\rho \rangle = \frac{1}{|G|}|G|\chi_{\rho}(id)=\chi_{\rho}(id)=deg(\rho)
\]
In particolare se $\rho$ è una sottorappresentazione irriducibile allora
\[ deg(\rho)=dim(Hom(\mathcal{R},\rho)) \]
Quindi ottengo una conferma del teorema precedente 
\[      
\langle \chi_{\mathcal{R}} | \chi_\rho \rangle =dim(Hom(\mathcal{R},\rho))
\]

\item Se $\rho$ e $\sigma$ sono 2 rappresentazioni irriducibili allora $$\rho \cong \sigma \Leftrightarrow \chi_{\rho}=\chi_{\sigma}$$

\item $\langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = \sum_i n_i^2$.
\item Conseguenza dell'ultima osservazione è che una rappresentazione di un gruppo $\rho$ è irriducibile $\Leftrightarrow \langle \chi_\rho | \chi_\rho \rangle = |\chi_\rho|^2 = 1$ 


\end{itemize}


\begin{cor}[Corollario del lemma \ref{lemma relazioni ortogonalita}: Lemma di Burnside]

Consideriamo un'azione di un gruppo $G$ su un insieme $I$ e consideriamo la rappresentazione per permutazioni corrispondente $(\rho_I, V_{\rho_I})$. Consideriamo

\[\langle \chi_{\rho_I} | 1 \rangle = \dfrac{1}{|G|} \dsum_{s\in G} tr \rho_I(s)\]

Ma è ovvio che 

\[tr \rho_I(s) = |I^s| \qquad I^s = \{ i \in I | s \cdot i = i\} \]

Per cui lo spazio 
\[V_{\rho_I}^G = \{v\in V_{\rho_I}:\rho_I(s)v=v\ \forall s\in G\}=\{\sum_i a_i e_i :\rho_I(\sum_i a_i e_i)=\sum_i a_i e_i\ \forall s\in G\}=\]
\[ =\{\sum_i a_i e_i :\sum_i a_i e_{s\circ i}=\sum_i a_i e_i\ \forall s\in G\}=\{ \sum_i a_i e_i: \sum_i (a_{s^{-1}\circ i}-a_i)e_i=0\ \forall s\in G \}\]
sarà composto da i vettori che hanno i coefficienti $a_i$ costanti su ciascuna orbita di $G$ su $I$, proprio per lasciarlo invariante. Perciò 

\[ dimV_{\rho_I}^G = \text{numero delle orbite } = |I/G|\]

E con l'affermazione precedente si ottiene appunto il lemma di Burnside

\[ |I/G| = \dfrac{1}{|G|} \dsum_{s\in G} |I^s|\]

\qed
\end{cor}






\begin{thm} Sia $G$ un gruppo finito e siano $\rho_1, \ldots , \rho_r$ le sue rappresentazioni irriducibili. Sia inoltre 

\[Cl(G)  \]

Lo spazio delle funzioni da $G$ in $\C$ costanti sulle classi di coniugio di $G$

Chiaramente $\dim Cl(G) = $ numero di classi di coniugio di $G$ $:= s$. La tesi del teorema è che $r = s$ 
dove $r$ è il numero di rappresentazioni irriducibili. 

\textsc{Osservazione:} Per questo motivo la tabella dei caratteri sarà una tabella quadrata.

\end{thm}

\textsc{Dimostrazione:}
 
Mostriamo intanto che $r \leq s$: i caratteri di $\rho_1, \ldots , \rho_r$ sono infatti ortonormali rispetto alla forma hermitiana
definita positiva $\langle . | .\rangle$, e quindi sono indipendenti (non posso avere più di $s$ vettori linearmente indipendenti 
in uno spazio vettoriale di dimensione $s$).

Verifichiamo ora che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = {0}$. Sia $f \in Cl(G)$ e $\rho$ una rappresentazione,
definiamo $T_f= \frac{1}{|G|}\dsum_s f(s)\rho(s)$ e verifichiamo che è un omomorfismo di rappresentazioni: 
$$ T_f \circ \rho(t) = \frac{1}{|G|}\dsum_s f(s)\rho(s) \rho(t) = \frac{1}{|G|}\dsum_s \rho(t) \rho(t^{-1}) f(s)\rho(s) \rho(t) =$$

$$= \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}) \rho(s) \rho(t) = \frac{1}{|G|}\rho(t) \dsum_s f(s) \rho(t^{-1}st) = 
\frac{1}{|G|}\rho(t) \dsum_{s'} f(s') \rho(s')= \rho(t) \circ T_f$$

Abbiamo usato il fatto che $f(s)\in \C$, quindi commuta con $\rho(g)$, e che $f$ è una funzione di classe nella 
sostituzione di $s$ con $s'= t^{-1}st$ (essendo il coniugio un automorfismo, cambia solo l'ordine della somma).

Se $\rho$ è irriducibile, $T_f= \alpha I$ è uno scalare per il lemma di Schur, $\alpha= \frac{\tr(T_f)}{\deg(\rho)}$.
Si ha $$\alpha =\frac{1}{\deg(\rho)}\tr(T_f)=\frac{1}{\deg(\rho)|G|}\dsum_s f(s) \chi_\rho(s)=
\frac{1}{\deg(\rho)} \langle f | \chi_{\rho^*} \rangle = 0$$ se $f \in \langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp}$.

Generalizziamo a quando $\rho$ non è irriducibile, ossia $\rho = \sigma_1+ \ldots + \sigma_n$, con $\sigma_i$ irriducibili. Allora 
si ha $V_\rho = V_{\sigma_1}  \oplus \ldots \oplus V_{\sigma_n}$, ed essendo $T_f=0 $ su ogni $V_{\sigma_i}$, è nullo anche su $V_\rho$.

Mostriamo che questo implica $f=0$: sia $\mathcal{R}$ la rappresentazione regolare di $G$; si ha:
$$ 0= |G| T_f (e_1) = \dsum_s f(s) \mathcal{R}(s)(e_1) = \dsum_s f(s) e_s $$

Di conseguenza $f(s)=0 \quad \forall s \in G$ essendo gli $e_s$ una base di $V_R$.
In questo modo abbiamo mostrato che $\langle \chi_{\rho_1}, \ldots, \chi_{\rho_r} \rangle ^{\perp} = \{0\}$, quindi $r=s$.
\qed

















\subsection{Tabella dei caratteri}


Dato un gruppo $G$, possimo costruire la $tabella\ dei\ caratteri$ nel seguente modo:
\begin{itemize}
\item su ogni colonna mettiamo un rappresentante della classe di coniugio con sotto la cardinalità dell'orbita ovvero

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c}
\hline
$G$  & $e$ & $orb(g_1)$ & $orb(g_2)$ & \\
 & 1 & $|orb(g_1)|$ & $|orb(g_2)|$ & \\
\hline
 & &  & \\
\end{tabular}
\end{table}

\item su ogni riga mettiamo una rappresentazione irriducibile del gruppo
\item all'incrocio tra la rappresentazione $\rho_i$ e la classe di coniugio di $g_j$ inseriamo il valore di $\chi_{\rho_i(g_j)}$.


\end{itemize}



\subsection{Esempi di rappresentazioni di gruppi finiti}

\begin{exmp}[Tabella dei caratteri di $S_3$] 
La prima cosa da fare per costruire la tabella dei caratteri è vedere quanti elementi ha $S_3$, suddividerli in classi di coniugio e poi cercare le rappresentazioni irriducibili solo dopo aver fatto tutto questo. Notiamo subito che $S_3$ ha esattamente 3 classi di coniugio. La prima è ovviamente quella banale, composta solo dall'identità $e$. Poi c'è la classe delle trasposizioni $\{(1 2) ,(2 3), (1 3)\}$ che ha 3 elementi e poi ci sono i $3$cicli, ovvero $(1 2 3)$ e $(1 3 2)$. Possiamo cominciare a scrivere una tabella vuota $3\times 3$


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )  \\
 & 1 & 3 & 2 \\
\hline
 & &  & \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}



Una rappresentazione irriducibile che c'è sempre è la rappresentazione banale di grado 1, ovvero quella che manda ogni elemento nell'identità. La tabella con questa informazione diventa



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
& &  & \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}


Un'altra rappresentazione che già conosciamo è il segno, $\epsilon$, che ricordiamo vale $(-1)^{n-1}$ dove $n$ è la lunghezza del ciclo. La tabella diventa




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
& &  & \\
\hline
\end{tabular}
\end{table}

A questo punto ci sono due motivi per dire che l'ultima rappresentazione ha grado 2: il primo è che è l'unico modo di ottenere la relazione

\[ |G | = \dsum_i n_i^2 \]

Il secondo è che se fossero due rappresentazioni di grado 1 allora il gruppo avrebbe solo rappresentazioni irriducibili di grado 1 e un teorema che abbiamo fatto implicherebbe che $S_3$ sia abeliano, cosa palesemente falsa. 

Per trovare il carattere dell'ultima rappresentazione possiamo agire in più modi. Innanzitutto la tabella ora ha la forma




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 &  & \\
\hline
\end{tabular}
\end{table}


In generale ci saranno due numeri complessi $a, b$ nelle due caselle che mancano. Tuttavia noi sappiamo un sacco di teoremi che ci permettono di restringere il campo dei valori che possono avere. Per esempio noi sappiamo che 

\[\langle \rho_i | \rho_j \rangle = \delta_{ij}\]
 
Per cui imponendo che il prodotto scalare con entrambe le precedenti faccia 0 abbiamo due equazioni e due incognite, ovvero un problema risolvibile. L'altro modo è dire che

\[ \mathcal{R} = 1 + \epsilon + 2\rho\]

E dato che il carattere si comporta bene con la somma di rappresentazioni, 

\[\chi_{\mathcal{R}} = \chi_1 + \chi_\epsilon + 2 \chi_\rho  \]

Ma sappiamo anche che 

\[ \chi_{\mathcal{R}}(s) = 
\begin{cases}
|G| \quad \text{se } s = e \\
0 \quad \text{altrimenti}
\end{cases}\]

Per cui con agili conti riusciamo a completare la tabella








\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_3$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 3 & 2 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\rho$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_3$}
\label{tabella caratteri s3}
\end{table}

L'ultimo modo è cercare di scomporre un'altra rappresentazione a caso di $S_3$, cercando di trovare la rappresentazione che ci manca. Per esempio ricordiamo l'azione di $S_3$ sui vettori di base di $\mathbb{R}^3$

\[ \tau(s) e_i = e_{s(i)}\]

Ricordiamo che il sottospazio di dimensione $1$ fatto dallo span del vettore $v = e_1 + e_2 + e_3$ è un sottospazio invariante in cui $\tau(s)$ è sostanzialmente l'identità. Il suo ortogonale è un altro sottospazio invariante su cui $\rho$ è irriducibile. Di conseguenza potremo scrivere

\[ \tau = 1 + \rho\]

E siamo sicuri che l'altra rappresentazione di grado 2 sia esattamente quella che stiamo cercando proprio grazie al teorema che ci dice che tutte le rappresentazioni irriducibili di un gruppo compaiono nella sua rappresentazione regolare. (Teorema \ref{thm: teorema importantissimo})

Dato che è facile calcolare il carattere di $\tau(s)$ in quanto è uguale a $\Fix(s)$, possiamo scrivere

\[ \Fix(s) = 1 + \chi_\rho\]

Da cui si ricava subito il carattere della rappresentazione $\rho$



\end{exmp}






\begin{exmp}[Tabella dei caratteri di $S_4$]
Facciamo la prima cosa importante: dividiamo $S_4$ in classi di coniugio. Per i soliti teoremi sugli $S_n$, le classi di coniugio saranno 
\[\{e\}, \{(a b)\}, \{(a b c)\}, \{(a b c d)\}, \{(a b)(c d)\}\]

E notiamo che sono 5. Possiamo quindi cominciare a compilare la tabella dei caratteri vuota



\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


dove ho già messo la rappresentazione banale. Anche per $S_4$, essendo un gruppo simmetrico c'è la rappresentazione segno di grado 1. 




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


A questo punto bisogna fare cose a caso cercando le rappresentazioni irriducibili. Per esempio possiamo di nuovo considerare la rappresentazione per permutazioni



\[ \tau(s) e_i = e_{s(i)}\]


Che si scompone anche questa come

\[ \tau = 1 + \rho\]

Vorremmo sapere se $\rho$ è irriducibile. Potremmo invocare qualche teorema ma lo faremo con le mani calcolando il carattere di $\rho$


\[ \chi_\rho(s) = \Fix(s) - 1 = 
\begin{cases}
3 \quad \text{Se } s = e \\
1 \quad \text{Se } s = (a b) \\
0 \quad \text{Se } s = (a b c) \\
-1 \quad \text{Se } s = (a b c d ), (a b) (c d)\\
\end{cases}
\]

E andando a calcolare

\[\langle\chi_\rho |\chi_\rho\rangle = \dfrac{1}{24}\left(3^2  + 6 \cdot 1^2  + 0 + (-1)^2 \cdot (3 +6 )\right) = 1\]
 

Per cui è effettivamente irriducibile.  Aggiungiamola alla tabella.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
& &  & & & \\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}

Abbiamo appena terminato le rappresentazioni che conoscevamo di $S_4$.\\
\textbf{Ottimo consiglio:} Quando non vengono in mente altre rappresentazioni, considera due già presenti nella tabella e fanne il prodotto. Risulta utile il seguente lemma.

\begin{lemma}
Se $\rho$ e $\sigma$ sono due rappresentazioni e $deg(\rho)=1$ ( ovvero $\rho:G\rightarrow \mathbb{C}^*$), allora $\sigma$ è irriducibile $\Leftrightarrow$ $\rho\sigma$ lo è. Inoltre hanno lo stesso grado.
\end{lemma}

\textbf{Dimostrazione:} Che sia ancora a tutti gli effetti una rappresentazione si verifica esplicitamente sapendo che
\[
\forall s\in G \rho\sigma(s)=\rho(s)\sigma(s)
\]
Per dimostrare che è irriducibile si considera il fatto che
\[
\sigma \ irriducibile\ \Leftrightarrow 1=\langle\chi_{\sigma}|\chi_{\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}|\chi_{\sigma(s)}|^2
\]
Quindi...
\[
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{S\in G}|\chi_{\rho\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\chi_{\rho(s)}\chi_{\sigma(s)}|^2=\frac{1}{|G|}|\rho(s)\chi_{\sigma(s)}|^2=\frac{1}{|G|}\sum_{s\in G}|\rho(s)|^2|\chi_{\sigma(s)}|^2
\]
ed essendo $\rho(s)$ una radice $n-$esima dell'unità dove $n$ è l'ordine di $G$ si ha che 
\[1 | 
\langle \chi_{\rho\sigma}|\chi_{\rho\sigma}\rangle=\frac{1}{|G|}\sum_{s\in G}
|\chi_{\sigma(s)}|^2=\langle \chi_{\sigma}|\chi_{\sigma}\rangle
\]
Che abbiano lo stesso grado deriva dal fatto che
\[
\chi_{\rho\sigma}=\chi_{\rho}\chi_{\sigma}\Rightarrow deg(\rho\sigma)=\chi{\rho\sigma}(id)=\chi_{\rho}(id)\chi_{\sigma}(id)=deg(\rho)deg(\sigma)=deg(\sigma).
\] \\
Essendo $\epsilon$ di grado 1 e $\rho$ irriducibile allora anche $\rho\epsilon$ è un'altra rappresentazione irriducibile.


\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
& &  & & & \\
\hline
\end{tabular}
\end{table}


E a questo punto dato che $|S_4|=24$ e che $1+1+3^2+3^2=20$ si possono avere due situazioni: $S_4$ potrebbe avere ancora 4 rappresentazioni irriducibili di grado 1 oppure solo più una di grado 2. Tuttavia abbiamo visto come $S_n$ ammetta solo due rappresentazioni irriducibili di grado 1 quindi siamo nel secondo caso.\\
Dato che ce ne manca solo una possiamo usare il trucco di prima (differenza dalla rappresentazione $R$ ) e concludere:

\begin{table}[!ht] 
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 ) & $(1 2 3 4)$ & $(1 2)(3 4)$ \\
 & 1 & 6 & 8 & 6 & 3 \\
\hline
 $\rho_1$ & 1 & 1  & 1 & 1 & 1\\
\hline
$\epsilon$ & 1  & -1 & 1 & -1 & 1 \\
\hline
$\rho$& 3 & 1 & 0 & -1 & -1\\
\hline
$\rho\epsilon$& 3 & -1 & 0 & 1 & -1\\
\hline
 $\sigma$& 2&  0 & -1& 0 & 2\\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $S_4$}
\label{tabella caratteri s4}
\end{table}


\textbf{Ossevazione:} Guardiamo la tabella, in particolare il "minore" ottenuto considerando le prime due e l'ultima riga e le prime 3 colonne. 

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$S_4$  & $e$ & $(1 2)$ & (1 2 3 )    \\
 & 1 & 6 & 8 \\
\hline
 $\rho_1$ & 1 & 1  & 1 \\
\hline
$\epsilon$ & 1 & -1 & 1 \\
\hline
$\sigma$ & 2 & 0 & -1 \\
\hline
\end{tabular}
\end{table}
Se la confrontiamo con la tabella dei caratteri di $S_3$ vediamo che sono analoghe. Intuitivamente $\rho$ in $S_3$ deriva dalla rappresentazione $\sigma$ di $S_4$ mediante un omomorfismo 
\[
S_4\rightarrow S_3
\]
che corrisponde ad una azione di $S_4$ su un insieme di 3 elementi. Tale insieme è il sottogruppo di Klein privato dell'unità ovvero
\[
\{ (12)(34),(13)(24),(14)(23)\}
\]
\\
In questo caso non è servito ma potremmo trovarci in una situazione in cui i seguenti lemmi si rivela utile
\begin{lemma}
$\rho^* $ è irriducibile $ \Leftrightarrow \rho$ è irriducibile.
\end{lemma}
Infatti $\chi_{\rho^*}=\overline{\chi_\rho} $ e quindi analogamente al lemma precedente si vede che
\[
1=\langle\chi_{\rho}|\chi_{\rho}\rangle \Leftrightarrow 1=\langle\chi_{\rho^*}|\chi_{\rho^*}\rangle
\]

\begin{lemma}
Se $\rho$ è una rappresentazione di grado $d$ di $G$, come sempre gruppo finito, allora:\\
$(a)$ $|\chi_{\rho}(s)|\leq d$ \\
$(b)$ Direttamente dal punto $(a)$ si decude che, 
\[
\chi_{\rho}(s)=d\Leftrightarrow \lambda_1,...,\lambda_d=1\Leftrightarrow \rho(s)=id
\]
dove $\lambda_1,..,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$.
\end{lemma}
\textbf{Dimostrazione:} Se $\lambda_1,..,\lambda_d$ sono gli autovalori della matrice $[\rho(s)]$ allora $\chi_{\rho}(s)=\sum_{i=1}^{d}\lambda_i$. Inoltre essendo $G$ finito $|\lambda_i|=1\forall i\in \{1,...,d\}$. Se ne deduce che
\[
|\chi_{\rho}(s)|\leq \sum_{i=1}^d |\lambda_i|=d
\]



\end{exmp}





\begin{exmp}[Tabella dei caratteri di $D_5$]

La prima cosa da fare è dividere $D_5$ in classi di coniugio


FINIRE

\end{exmp}



\subsubsection{I problemi della prima lezione visti con i nuovi strumenti}
\begin{exmp}[Problema 1 prima lezione]


\end{exmp}

\begin{exmp}[Problema 2 prima lezione]


\end{exmp}

\begin{exmp}[Problema 3 prima lezione]

Consideriamo un cubo. Scriviamo un numero su ciascuna delle facce e consideriamo l'operazione $T$ che per ogni faccia sostituisce al numero presente la media dei numeri presenti sulle 4 facce del cubo adiacenti. Vogliamo studiare il comportamento dei numeri del cubo quando questa iterazione viene compiuta molte volte.


Cerchiamo di formalizzare il problema usando la teoria della rappresentazione. Possiamo considerare l'insieme $F$ delle facce del cubo\footnote{Che ha quindi 6 elementi}. Una generica configurazione del cubo sarà esprimibile come 

\[ v = \dsum_{f \in F} a_f e_f \]

Dove $a_f \in \mathbb{C}$ e $e_f$ sono una base. L'operatore che sostituisce la media è lineare ma soprattuto commuta con le simmetrie del problema. Ora spiegherò meglio questo concetto.

Consideriamo il gruppo $G$ delle rotazioni del cubo, ovvero 

\[G = \{ g \in SO(3) | g(Cubo) \subset Cubo \} \]

\'E ovvio che il problema è invariante per simmetria, ovvero se $g \in G$, allora vale

\[ T v = g^{-1}T g v \]

Che è la formula di un cambio di base. Questo si può scrivere come 

\[gT = Tg \]

Ovvero ci dice che $\forall g \in G$ le due operazioni commutano. Le due frasi precedenti sono state dette un po' alla garibaldina in quanto non è $g$ ad agire sul cubo ma è una sua rappresentazione di grado $|F| = 6$. Di conseguenza è bene scrivere in modo formale che $\tau: G \to GL(V_\tau)$ è una rappresentazione del gruppo di rotazioni del cubo in $\mathbb{C}^6$ e questa rappresentazione commuta con un operatore $T$, ovvero

\[T\tau(g) = \tau(g) T \qquad \forall g \in G  \]


L'obiettivo che ci poniamo ora è quello di riuscire a scomporre $\tau$ come somma di rappresentazioni irriducibili in quanto una volta trovata una scomposizione 

\[ V_\tau = \bigoplus_{i = i}^n V_{\rho_i} \] 

Allora potremo usare il lemma di Schur per dire che su ogni $V_{\rho_i}$ l'operatore $T$ si comporta come scalare ovvero \emph{è più che diagonalizzato}. Per riuscire a capire qualcosa di come sono fatte le rappresentazioni di questo gruppo è opportuno prima cercare di dare una struttura più chiara a questo gruppo.

\'E possibile mostrare che QUALCUNO CHE HA VOGLIA DI FARLO LO FACCIA PLS $G \cong S_4$. A questo punto noi abbiamo una rappresentazione di grado 6 di $S_4$ che cerchiamo di scomporre come somma di rappresentazioni irriducibili. Tuttavia grazie al teorema \ref{thm: teorema importantissimo} sappiamo che tutte le sottorappresentazioni di $\tau$ saranno isomorfe alle sottorappresentazioni della rappresentazione regolare $\mathcal{R}(S_4)$, di cui abbiamo preventivamente calcolato la tabella dei caratteri \ref{tabella caratteri s4}. Dato che 


\[\tau = \dsum_i n_i\rho_i \Rightarrow \chi_\tau  = \dsum_i n_i\chi_{\rho_i}\]

Andiamo a calcolare i prodotti scalari dei caratteri delle rappresentazioni irriducibili di $S_4$ con il carattere di $\tau$ per trovare quali rappresentazioni compaiono. Per farlo calcoliamo prima il carattere di $\tau$


SCRIVI CHE NON HO VOGLIA

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline


\end{tabular}
\end{table} 



Per cui si ottiene

\[\tau = 1 + \epsilon\rho + \sigma \]

Ovvero

\[V_\tau = V_1 \oplus V_{\epsilon\rho} \oplus V_{\sigma} \]


Cerchiamo quindi di capire come sono fatti questi tre spazi che hanno rispettivaemente dimensione 1,3,2. 


SCRIVI PI\'U DETTAGLIATO CHE DEVO ANDARE A LEZIONE

\[V_1 =  span(e_1 + e_2 + ... + e_6) \]
\[V_{\epsilon\rho} = \text{Le facce opposte hanno numeri opposti}\]
\[V_{\sigma} = \text{Le facce opposte hanno numeri uguali e la somma di tutti è 0}\]

Su questi spazi è facile vedere che effettivamente $T$ è scalare. In particolare

\[
\begin{cases}
T|_{V_1} = 1 \\
T|_{V_{\epsilon\rho}} = 0 \\
T|_{V_\sigma} = -\frac{1}{2}\\
\end{cases}
\]

E quindi è evidente che $T^n \to $ su ogni faccia viene la media dei numeri che c'erano all'inizio.


\end{exmp}











\newpage
\section{Rappresentazioni reali, complesse e quaternioniche}

Sono di un certo interesse le rappresentazioni su $\C$ che possono essere espresse con matrici a coefficienti reali. Diamo una definizione precisa:

\begin{defn}
Diciamo che una rappresentazione $\rho:G\to GL(V_\rho)$ del gruppo $G$ è reale se esiste una base di $V_\rho$ secondo la quale
le matrici di $\rho(g)$, al variare di $g$ nel gruppo, sono tutte a coefficienti reali.
\label{def: rappr reale}
\end{defn}

Sia $V$ un $\C-$spazio vettoriale di dimensione $n$.
Denotiamo con $V^\R$ lo spazio vettoriale su $\R$ che ha per ``insieme di vettori''
lo stesso di $V$, con la stessa somma ma chiaramente con prodotto per scalari ristretto ad $\R$.
Osserviamo che se $\{v_1, \dots, v_n\}$ è una base di $V$ allora 
$\{v_1, \imath v_1, v_2, \imath v_2,\dots, v_n, \imath v_n\}$ è una base di $V^\R$, il quale di conseguenza ha dimensione $2n$.
Se $\rho:G\to GL(V)$ è una rappresentazione del gruppo $G$, allora è definita 
in modo naturale una rappresentazione $\rho^\R:G\to GL(V^\R)$, in cui gli il gruppo $G$
agisce sullo spazio vettoriale esattamente come nella rappresentazione $\rho$ (se $G$ agisce 
in modo $\C-$lineare, a maggior ragione l'azione è $\R-$lineare).

Il fatto che $\rho$ sia una rappresentazione reale è equivalente a chiedere l'esistenza di un sottospazio reale $V_0 \subset V^\R$ che sia stabile sotto l'azione di $G$ e che abbia dimensione $n$. Cioè è possibile scomporre $V^\R$ come somma di sottorappresentazioni:
\[ V^\R = V_0 \oplus i V_0 \]

\begin{exmp}
Prendiamo come gruppo un gruppo ciclico, per esempio $\Z / 3\Z$\footnote{Che per i fisici è isomorfo a $C_3$}. Evidentemente tutte le rappresentazioni non banali di $G$ non sono reali, in quanto sono di grado 1 e sono le radici dell'unità.
\end{exmp}

Non è difficile osservare che se indichiamo con $\alpha_{i,j}$ i coefficienti della matrice $\rho(g)$
rispetto alla base $\{v_1, \dots, v_n\}$, allora la matrice di $\rho^\R(g)$ rispetto alla base
$\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$ si ottiene
ponendo al posto di $\alpha_{i,j}$ il blocchetto $2\times 2$:
\[\begin{pmatrix}
	\Re \alpha_{i,j} & -\Im \alpha_{i,j}\\
	\Im \alpha_{i,j} & \Re \alpha_{i,j}\\
\end{pmatrix}\]
dove $\Re$ e $\Im$ indicano parte reale e immaginaria.
Dunque abbiamo il seguente risultato:

\begin{prop}
	$\chi_{\rho^\R}(g) = \chi_\rho(g) + \chi_{\rho^*}(g)$
\end{prop} 
\begin{proof}
	Basta ricordare che $\chi_{\rho^*}$ è il complesso coniugato di $\chi_\rho$, dunque
	$\chi_\rho(g) + \chi_{\rho^*}(g) = 2\Re \chi_\rho(g)$ è evidentemente la traccia di $\rho^\R(g)$ per
	quanto detto sopra.
\end{proof}


\begin{prop}
	Se $\rho:G\to GL(V)$ è irriducibile ($V$ è un $\C-$spazio vettoriale), allora $\rho$ è reale se e solo se $\rho^\R$ è riducibile.
\end{prop}
\begin{proof}
	Se $\rho$ è rappresentazione reale, sia $\{v_1,\dots v_n\}$ una 
	base di $V$ tale che le matrici di $\rho(g)$ rispetto a tale base siano a coefficienti reali.
	Ora, $V^\R$ ha per base $\{v_1, \imath v_1,\dots, v_n, \imath v_n\}$.
	Indico con $V_0$ il sottospazio di $V^\R$ generato da $\{v_1,\dots v_n\}$, che ha dimensione $n$.
	Ora, $V_0$ è $G-$invariante, dunque $\rho^\R$ è riducibile.
	
	Viceversa, supponiamo che $V^\R = U \oplus W$ sia scomposizione di rappresentazioni
	con $U$ e $W$ diversi da $\{0\}$. A meno di scambiare $U$ e $W$ posso supporre $\dim U \le n$.
	Il sottospazio di $V$ generato dai vettori di $U$ (con combinazioni lineari complesse) è $G-$invariante,
	dunque essendo $\rho$ irriducibile tale sottospazio deve coincidere con $V$.
	Tutto ciò si traduce nel fatto che $V^\R = U + \imath U$, e siccome $\dim\imath U = \dim U \le n$
	deve essere necessariamente che $U$ e $\imath U$ sono in somma diretta e hanno 
	entrambi dimensione $n$. Ora basta prendere una qualsiasi base di $U$,
	la quale è anche base di $V$ (essendo costituita da $n$ vettori che generano $V$) ed è tale che la matrice di $\rho(g)$ è 
	a coefficienti reali per ogni $g\in G$, ovvero $\rho$ è reale secondo la definizione \ref{def: rappr reale}.
\end{proof}

Ora andremo a fare una classificazione delle rappresentazioni in 3 tipi diversi:
\begin{itemize}
\item Reali
\item Complesse
\item Quaternioniche
\end{itemize}
La classificazione verrà fatta in base all'esistenza o meno di forme bilineari di un certo tipo invarianti sotto $G$.
Iniziamo con alcune proprietà soddisfatte dalle rappresentazioni reali:

\begin{thm}
Prendiamo una $\rho:G\to GL(V)$ rappresentazione $/\C$ che sia reale. Allora:
\begin{enumerate}
\item $\chi_\rho(g)\in \R$ $\forall g\in G$ 
\item $V$ possiede una forma bilineare simmetrica $G-$invariante non nulla. Ovvero lo spazio delle forme bilineari simmetriche $S^2V^*\subset V^*\otimes V^*$ è tale che $(S^2V^*)^G\neq 0$.
\end{enumerate} 
\end{thm}

\begin{proof}
Abbiamo supposto la rappresentazione reale. Quindi la matrice è reale ed in particolare lo sarà anche la sua traccia. Quindi il primo punto è vero. Ora veniamo al secondo punto: esisterà un $V_0$ spazio vettoriale reale $G-$invariante che tensorizzato con $\C$ dia $V^\R$. Consideriamo ora una forma bilineare simmetrica $B_0$ non degenere $/\R$
\[ B_0 \in S^2 V_0^*\]
Possiamo ora renderla invariante sotto l'azione di $G$ con il solito metodo del fare la media. Consideriamo quindi $\tilde B_0$ definito come
\[ \tilde B_0(v_1, v_2) = \dfrac{1}{|G|} \dsum_{g\in G} B_0(\rho(g) v_1, \rho(g) v_2)\]
Questo ha le caratteristiche precedenti ed è anche invariante sotto $G$, ovvero $\tilde B_0\in (S^2V_0^*)^G$. Possiamo a questo punto estenderla a forma bilineare su $V$ complessificandola in modo ovvio. Costruiamo quindi $B \in (S^2V^*)^G$ definendola nel seguente modo:
\[B(v_1 + i v_1', v_2 + i v_2') = \left( \tilde B_0(v_1, v_2) - \tilde B_0(v_1', v_2')\right)  + i \left( \tilde B_0(v_1', v_2) + B_0(v_1, v_2')\right)\]
\`E una banale verifica controllare che rispetta le caratteristiche richieste.
\end{proof}

\begin{rem}
Supponiamo che $\rho:G\to GL(V)$ sia una rappresentazione irriducibile. Allora il fatto che esiste una forma bilineare $G-$invariante non nulla
equivale a $(V^*\otimes V^*)^G\neq 0$. Ma ricordando che $(V^*\otimes V^*)^G \cong \Hom(V,V^*)^G$ (che è l'insieme degli omomorfismi di rappresentazioni) ciò vuol dire che $\rho$ e $\rho^*$ sono isomorfi, il che si traduce in $\chi_\rho = \chi_{\rho^*}$. Ma questo avviene esattamente quando $\chi_\rho$ assume solo valori reali. Ciò significa che la seconda condizione trovata (che prevede l'esistenza di una forma bilineare simmetrica invariante) è strettamente più forte della prima (che è soddisfatta anche nel caso in cui esista una forma bilineare invariante non necessariamente simmetrica).
\end{rem}

Vediamo ora un lemma che ci servirà per la classificazione.
\begin{lemma}
Sia $V$ una $G-$rappresentazione irriducibile $/\C$. Allora ogni forma bilineare non nulla $G-$invariante è non degenere. Inoltre, se esiste, è unica a meno di scalari, ovvero $\dim (V^*\otimes V^*)^G\le1$.
\end{lemma}
\begin{proof}
Prendiamo un elemento $B \in \left( V^* \otimes V^*\right) ^G$.

Ricordiamo che $V^* \otimes V^*$ e $\Hom(V, V^*)$ sono $G-$rappresentazioni isomorfe. In particolare:
\[\left(V^* \otimes V^*\right)^G \cong \Hom(V, V^*)^G \]
A questo punto, se esiste una forma bilineare non nulla invariante, questa corrisponde ad un omomorfismo di rappresentazioni $\phi: V\to V^*$.
Per il lemma di Schur o $\phi$ è nullo o $\phi$ è un isomorfismo. Dato che la forma è non nulla, allora $\phi = \lambda Id$ con $\lambda \in \C\setminus\{0\}$.
\end{proof}


\begin{thm}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile $/\C$.\\
Definiamo $m_\rho = \dfrac{1}{|G|} \dsum_{g \in G} \chi_{\rho}(g^2)$ detto ``indicatore di Frobenius-Schur''. Allora
\begin{enumerate}
\item Se $B \in (V^* \otimes V^*)^G$ allora $B \in S^2V^*$ oppure $B \in \bigwedge ^2 V^*$
\item  $m_\rho \in \{-1, 0, 1 \}$
\item{ \begin{itemize}
   \item se $m_\rho = 0$ allora $(V^*\otimes V^*)^G = 0$
   \item se $m_\rho = 1$ allora $(S^2 V^*)^G \neq 0$
   \item se $m_\rho = -1$ allora $(\bigwedge^2 V^*)^G \neq 0$
\end{itemize}
}
\end{enumerate}
\end{thm}

\begin{proof}
Indichiamo $W = (V^*\otimes V^*)^G$ lo spazio delle forme bilineari $G-$invarianti, che è sottorappresentazione di $V^*\otimes V^*$.
Vogliamo dimostrare che $W\subseteq S^2V^*$ oppure $W\subseteq \bigwedge^2V^*$.
Dal lemma precedente sappiamo che $\dim W \le 1$. Se $\dim W = 0$ allora si ha automaticamente la tesi.
Supponiamo ora $\dim W = 1$.

Ricordando che $V^*\otimes V^* \cong S^2V^* \oplus \bigwedge^2V^*$ è scomposizione in somma di rappresentazioni,
consideriamo le proiezioni $p_{sym}:V^*\otimes V^* \to S^2V^*$ e $p_{alt}:V^*\otimes V^* \to \bigwedge^2V^*$
che in particolare sono omomorfismi di rappresentazioni (la verifica è facile, comunque questo accade ogni volta che si ha una somma di rappresentazioni).

Prendiamo ora $B\in W$ forma bilineare invariante non nulla. Allora almeno uno tra $p_{sym}(B)$ e $p_{alt}(B)$ è non nullo. Se fossero entrambi non nulli allora avremmo $\dim (V^*\otimes V^*)^G = \dim(S^2V^*)^G + \dim(\bigwedge^2V^*)^G \ge 2$, assurdo.
Dunque $B$ deve essere simmetrica oppure alternante, e ciò dimostra il punto 1.

Passiamo ora agli altri due punti. Ricordando il teorema \ref{thm:tracciasymalt} abbiamo che:
\begin{itemize}
\item $\dim(S^2V^*)^G = \langle 1|\chi_{S^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 + \chi_\rho(g^2)}{2}$
\item $\dim(\bigwedge^2V^*)^G = \langle 1|\chi_{\bigwedge^2\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\frac{\chi_\rho(g)^2 - \chi_\rho(g^2)}{2}$
\item $\dim(V^*\otimes V^*)^G = \langle 1|\chi_{\rho^*\otimes\rho^*}\rangle = \frac{1}{|G|}\dsum_{g\in G}\chi_\rho(g)^2$
\end{itemize}
dunque possiamo scrivere
$\dim(S^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G + m_\rho}{2}$;\ \ 
$\dim(\bigwedge^2V^*)^G = \frac{\dim(V^*\otimes V^*)^G - m_\rho}{2}$.

Ora, usando quanto ottenuto al punto 1 e distinguendo i casi in cui $V$ ammetta una forma simmetrica invariante, una forma antisimmetrica invariante, oppure non ammetta forme invarianti, si ottiene facilmente quanto affermato ai punti 2 e 3.
\end{proof}

\begin{defn}[Classificazione tramite l'indice di Frobenius]
Sia $\rho:G\rightarrow GL(V)$ una rappresentazione irriducibile $/\C$: essa è detta
\begin{itemize}
\item \textbf{reale} se $m_\rho = 1$  
\item \textbf{complessa} se $m_\rho = 0$ 
\item \textbf{quaternionica} se $m_\rho = -1$
\end{itemize}
\end{defn}

Ma la nuova definizione che abbiamo dato è compatibile con la \ref{def: rappr reale} data inizialmente?
La rassicurante risposta è data dal seguente lemma.
\begin{lemma}
Sia $\rho:G\to GL(V_\rho)$ una rappresentazione irriducibile su $\C$. Allora $\rho$ è reale secondo la definizione \ref{def: rappr reale} se e solo se $m_\rho = 1$.
\end{lemma}
\begin{proof}
Sappiamo già che se $\rho$ è reale nel senso \ref{def: rappr reale} allora $m_\rho = 1$, in quanto $V_\rho$ ammette una forma simmetrica invariante non nulla. Mostriamo il viceversa.

Sia $B \in (S^2V_\rho^*)^G$, con $\dim_\C V_\rho = n$.
Noi stiamo cercando un certo $V_0 \subset V_\rho$ spazio vettoriale su $\R$ tale che $V_\rho = V_0 \oplus i V_0$ .

Prendiamo ora una certa forma $h : V_\rho \times V_\rho \to \C$ hermitiana, definita positiva e $G-$invariante. Questa sicuramente esiste, è stato dimostrato nel teorema \ref{thm:esistenza hermitiana}.
Definiamo a questo punto un endomorfismo di spazi vettoriali reali $\phi: V_\rho \to V_\rho$ imponendo che soddisfi $B(x, y) = h(\phi(x), y)$.
La bontà di tale definizione è garantita dal teorema di Riesz.

Che proprietà ha $\phi$? Possiamo notare che $\phi$ è $G$-equivariante, ovvero vale $\phi(\rho(g)x) = \rho(g) \phi(x)$.
Mostriamolo rapidamente
\[ h(\phi(\rho(g)x) , y) = B(\rho(g)x, y) = B(x, \rho(g^{-1} ) y) = h (\phi(x), \rho(g^{-1} ) y) = h(\rho(g)x, y)\]
Questo non è male, in quanto se $\phi$ fosse lineare avremmo subito che $\phi$ è un omomorfismo di rappresentazioni irriducibili e potremmo usare Schur. Tuttavia
\[ \phi(z_1 x_1 + z_2 x_2) = \overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho\]
La linearità rispetto ai vettori è abbastanza evidente. L'antilinearità rispetto agli scalari deriva essenzialmente dal fatto che stiamo usando un prodotto hermitiano invece di un prodotto scalare. Infatti
\[ h( \phi(zx) , y) = B(zx, y) = B(x, zy) = h(\phi(x), zy) = \overline{z} h(\phi(x), y) \]
E quindi purtroppo $\phi$ non è davvero lineare come applicazione tra spazi vettoriali sul campo $\C$. Vediamo però che $\phi^2$ si comporta meglio:
\[\phi^2(z_1 x_1 + z_2 x_2) = \phi(\overline{z_1} \phi(x_1) + \overline{z_2} \phi(x_2))  = z_1 \phi^2(x_1) + z_2 \phi^2(x_2) \qquad \forall z_1, z_2 \in \C, \forall x_1, x_2 \in V_\rho \] 
E quindi effettivamente $\phi^2$ è un omomorfismo di rappresentazioni irriducibili. Per questo motivo possiamo applicare Schur e concludere che 
$\phi^ 2 = \lambda Id_{V_\rho}$ per un certo $\lambda \in \C$.

Cosa possiamo dire su $\lambda$? Il claim è che sia $\lambda \in \R$ e $\lambda > 0$. Vediamo come si mostra
\[h(\phi(x), y) = B(x, y) = B(y, x) = h(\phi(y), x) = \overline{h(x, \phi(y))} \]
usando questo fatto possiamo considerare 
\[ \lambda h(x, y) = h(\phi^2(x), y) = \overline{h(\phi(x), \phi(y))} = h(x, \phi^2(y)) = \overline{\lambda} h(x, y) \qquad \forall x, y \in V_\rho\]
E questo ci dice ovviamente che $\lambda \in \R$. Per mostrare ora che $\lambda > 0$ dobbiamo sfruttare il fatto che la nostra forma hermitiana sia definita positiva. Per questo motivo andiamo a considerare
\[\lambda h(x, x) = h (\phi^2(x), x) = \overline{h(\phi(x), \phi(x))} \Rightarrow \lambda = \dfrac{\overline{h(\phi(x), \phi(x))}}{ h(x,x)} \qquad \forall x \neq 0 \in V_\rho\]
E dato che $h $ è definita positiva si ha anche $\lambda > 0$

A questo punto possiamo (a meno di riscalare) scegliere $\lambda = 1$, ovvero $\phi^2 = Id$. A questo punto ci piacerebbe tornare a fare cose con $\phi$ e non $\phi^2$. Notiamo che se ci restringiamo a spazi vettoriali su $\R$, allora anche $\phi$ è lineare in quanto il coniugio non ci dà fastidio. Dato che quindi $\phi$ è un endomorfismo di uno spazio vettoriale reale tale che $\phi^2=1$, allora $\phi$ è diagonalizzabile e ha solo gli autovalori $\pm 1$.  Per questo motivo possiamo scomporre lo spazio di partenza $V_\rho = V_+ \oplus V_-$, con ovvia notazione per gli autospazi.

A questo punto ci manca poco. $V_+$ e $V_-$ sono sottospazi reali del nostro spazio di partenza. Se mostriamo che sono isomorfi, abbiamo trovato la nostra scomposizione dello spazio $V_\rho$ in due spazi $V_\rho = V_0 \oplus iV_0$.
Proprio per questo motivo è intelligente notare che vale $i V_+ = V_-$.
Mostriamo perché con il solito trucco della doppia inclusione. Prendiamo per esempio $x \in V_+$. Allora 
\[ \phi(ix) = - i \phi(x) = -i x \]
Ovvero il vettore $ix$ è autovettore di $\phi$ con autovalore $-1$. Applicando due volte questo ragionamento si ottiene facilmente
\[ V_+ \cong V_- \qquad (i V_+ = V_-)\]
Per cui a questo punto abbiamo finito
\end{proof}


\begin{lemma}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora $\dim V^G = \langle\chi_\rho|\chi_1\rangle$
\end{lemma}
\begin{proof}
Definiamo un'applicazione lineare $R:V\to V$ nel seguente modo:
\[ R = \frac{1}{|G|} \sum_{g\in G}{\rho(g)} \]
Se prendiamo $v\in V$ e $s\in G$ allora
\[ \rho(s) R(v) = \rho(s)\frac{1}{|G|} \sum_{g\in G}{\rho(g)v} = \frac{1}{|G|} \sum_{g\in G}{\rho(sg)v} = R(v)\]
ovvero $R(v)\in V^G$.
Inoltre se $w\in V^G$ allora $R(w)=w$. Di conseguenza $R^2 = R$ (cioè $R$ è una cosiddetta \emph{proiezione})
e quindi $R$ si diagonalizza con autovalori $0$ e $1$. Inoltre l'autospazio relativo a $1$ è proprio $V^G$.
Ciò implica che
\[ \dim V^G = \tr(R) = \frac{1}{|G|} \sum_{g\in G}{\chi_\rho(g)} = \langle\chi_\rho|\chi_1\rangle \]
\end{proof}

\begin{lemma} $\rho:G\rightarrow GL(V)$ rappresentazione irrid. $/_\C$: allora $\chi_\rho$ è reale sse $\rho$ è una rappresentazione reale o quaternionica. 
\end{lemma}
\textbf{Dimostrazione:} Abbiamo visto prima che $(V^*\otimes V^*)^G\neq 0\Rightarrow \chi_\rho=\chi_{\rho^*}\Rightarrow \chi_\rho$ è reale, e se $\rho$ è reale o quaternionica ha proprio $(V^*\otimes V^*)^G$ diverno da $0$. Viceversa $\chi_\rho$ reale implica $\chi_\rho=\chi_{\rho^*}$. D'altra parte dato che $\rho$ e $\rho^*$ sono entrambe rappresentazioni irriducibili allora $\rho\cong\rho^*\Leftrightarrow \chi_\rho =\chi_{\rho^*}$. Ciò significa che $Hom_G(V,V^*)\neq 0\Rightarrow \exists B\in (V^*\otimes V^*)^G\Rightarrow \rho$ può essere solo reale o quaternionica.\qed
\\ \\
\textbf{Esercizi:}
\begin{enumerate}
\item Mostrare che tutte le rappr. irrid. $/_\C$ si $S_4$ sono reali.
\item $G$ gruppo ciclico di ordine dispari: allora le rappresentazioni non banali sono complesse.
\end{enumerate} 

\begin{thm}
Sia $\rho:G\rightarrow GL(V)$ rappresentazione irrid. $/_\R$: allora $\exists \sigma:G\rightarrow GL(V_{\sigma})$ rappr. irrid. $/_\C$ tale che è vera una delle seguenti affermazioni:
\begin{enumerate}
\item $\chi_\rho=\chi_\sigma$;
\item $\chi_\rho=\chi_\sigma+\chi_{\sigma^*}$;
\item $\chi_\rho= 2\chi_\sigma$.
\end{enumerate}
\end{thm}

\textbf{Dimostrazione:} Definisco $V^\C=\C\otimes V$ la complessificazione di $V$ (è semplicemente $V$ visto però come spazio vettoriale su $\C$). Vi sono due casi:
\begin{enumerate}
\item $\rho:G\rightarrow GL(V^\C)$ (che chiamiamo $\rho^\C$ solo per ricordardi che $V$ è visto come spazio vettoriale sui complessi) è una rappr. irrid. $/_C$;
\item $\rho^\C:G\rightarrow GL(V^\C)$ è una rappr. rid. $/_C$.
\end{enumerate}
Se siamo nel primo caso $\sigma=\rho^\C$ è irrid. $/_\C$ e ovviamente $\chi_\rho=\chi_{\rho^\C}$.\\ 
Se siamo nel secondo caso allora $V^\C=U\bigoplus W$ con $U$ e $W$ sottospazi $G-$invarianti. Quindi $\rho^\C=\rho_U+\rho_W$. Passiamo ora alla loro realificazione: $(\rho_\C)^\R=$
DEVO FINIRE  




\subsection{Quaternioni}

Ovviamente le rappresentazioni quaternioniche hanno a che fare con il corpo dei quaternioni. Vediamo un po' di caratteristiche interessanti di questo oggetto.

Il corpo $\HH$ si può vedere come
\[\HH = \R \oplus i \R \oplus j \R \oplus k \R \]
Con $i,j,k$ unità immaginarie che rispettano le seguenti regole
\[ 
\begin{cases}
i^2 = j^2 = k^2 = -1 \\
ij = - ji = k \\
jk = -kj = i \\
ki = - ik = j \\
\end{cases}
\]

Vediamo un po' di proprietà interessanti. Per esempio se consideriamo 
\[ Q_8 =  \{\pm 1, \pm i, \pm j, \pm k \}\]
allora questo insieme è un gruppo se munito della moltiplicazione. Possiamo andare a vedere la tabella dei caratteri di questo gruppo. 

Per farlo possiamo utilizziamo il secondo metodo descritto nel paragrafo \eqref{par:rappr_deg_1}. Consideriamo si seguenti sottogruppi di $Q_8$
\begin{align*}
	H_1=\{ \pm1, \pm i \}\\
	H_2=\{ \pm1, \pm j \}\\
	H_3=\{ \pm1, \pm k \}
\end{align*}
\'E semplice verificare che $H_i$ è normale in $Q_8$ per $i=1,2,3$ e che $Q_8/H_i\cong \Z_2$\footnote{Visto che gli $H_i$ sono normali, $Q_8/H_i$ è un gruppo, avendo solo due elementi è necessariamente $\Z_2$.}. Di $\Z_2$ conosciamo le rappresentazioni di grado $1$: c'è la rappresentazione banale (che induce su $G$ la rappresentazione banale appunto) e $\rho:\Z_2\to \C^*$ che manda $[1]_2\to -1$. Da quest'ultima otteniamo le rappresentazioni che indicheremo con $\rho_i, \rho_j, \rho_k$ rispettivamente nel caso in cui quozientiamo per $H_1, H_2, H_3$; vediamo ora come calcolare una di queste, per esempio $\rho_i$, riportando il diagramma degli omomorfismi.
\[\tridiag {Q_8} {\pi_1} {Q_8/H_1} \rho {\C^*} {\rho_i}\]
Sia $g\in Q_8$, vogliamo capire quanto fa $\rho_i(g)$, ci sono due casi da considerare:
\begin{enumerate}
	\item se $g\in H_1$, allora $\pi(g) = eH_1$ dunque, visto che $eH_1$ è l'identità nel quoziente, $\rho_i(g)=1$
	\item se $g\not\in H_1$, allora $\pi(g) =gH_1\neq eH_1$, quindi, per la definizione di $\rho$, $\rho_i(g)=-1$
\end{enumerate}


L'ultima rappresentazione che si trova deve avere dimensione $2$ perché abbiamo già trovato $4$ rappresentazioni di grado $1$ (quella banale più le tre $\rho_i, \rho_j, \rho_k$ essendo tutte non isomorfe\footnote{La verifica è immediata}), abbiamo inoltre esaurito le rappresentazioni di grado $1$: potremmo ancora quozientare per $H=\{ 1, -1 \}$, ma si vede abbastanza facilmente che $Q_8/H\cong Z_2\times\Z_2$, metre avevamo detto che cercavamo quozienti ciclici; dalla formula $\sum n_i^2=|Q_8| = 8$ si deduce quindi che l'ultima rappresentazione deve avere grado $2$. Quest'ultima si ottiene dalla rappresentazione matriciale dei quaternioni: è noto che i quaternioni possono essere presentati come un sottoinsieme delle matrici complesse $2\times 2$:
\[
	\HH = \left\{ 
		\begin{pmatrix}
	              	z & w\\
	              	-\overline{w} & \overline{z}\\
	              \end{pmatrix}
	              \middle|\ z,w\in \C
		\right\}
\]
La rappresentazione è determinata dall'omomorfismo $\rho_{\HH}:Q_8\to GL(\C^2)$ definito sui generatori:
\[
	\rho_{\HH}(i) = \begin{pmatrix}
	          	i & 0\\
	          	0 & -i\\
	          \end{pmatrix}
	\qquad
	\rho_{\HH}(j) = \begin{pmatrix}
	          	0 & 1\\
	          	-1& 0\\
	          \end{pmatrix}
\]
Quindi vediamo che gli elementi di $Q_8$ possono essere visti come matrici $2\times2$ complesse:
\[ 
1_\HH = 
\left(
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right)
\qquad
i_\HH = 
\left(
\begin{array}{cc}
i & 0 \\
0 & -i \\
\end{array}
\right)
\qquad
j_\HH = 
\left(
\begin{array}{cc}
0 & 1 \\
-1 & 0 \\
\end{array}
\right)
\qquad
k_\HH =
\left(
\begin{array}{cc}
0 & i \\
i & 0 \\
\end{array}
\right)
\qquad
\]

Queste 4 matrici prendono il nome di matrici di spin di Pauli in quanto hanno alcune applicazioni in Fisica. Possiamo infine ricostruire quindi la tabella dei caratteri di $Q_8$

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 1 & 2 & 2 & 2 \\
$Q_8$ & 1 & -1 & $\pm i$ & $\pm j$ & $\pm k$ \\
\hline
$\rho_1$ & 1 & 1 & 1 & 1 & 1 \\
\hline
$\rho_i$ & 1 & 1 & 1 & -1 & -1 \\
\hline
$\rho_j$ & 1 & 1 & -1 & 1 & -1 \\
\hline
$\rho_k$ & 1 & 1 & -1 & -1 & 1 \\
\hline
$\rho_\HH$ & 2 & -2 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Tabella dei caratteri di $Q_8$}
\label{tab: caratteri q8}
\end{table}


\'E interessante notare che la tabella dei caratteri di $Q_8$ è uguale a quella di $D_4$, ma i due gruppi non sono isomorfi. Questo ci ricorda che la tabella dei caratteri dice tanto di un gruppo ma non tutto.





Ora vorremmo effettivamente capire il perché dei nomi dati nella classificazione delle rappresentazioni come reali, complesse e quaternioniche. Per questo motivo ci servono un paio di concetti di algebra.


\begin{defn}[Algebra]
Un'algebra su $\R$ è uno spazio vettoriale reale $A$ dotato di una moltiplicazione $\cdot : A \times A \to A$ che sia associativa e bilineare.
Inoltre imponiamo che vi sia un elemento neutro rispetto a questa moltiplicazione. Quest'ultima richiesta non fa parte della
più generale definizione di algebra (le algebre che la soddisfano si dicono \emph{unitarie}), ma noi la inseriamo nella definizione
poiché in questo corso non tratteremo mai algebre non unitarie.
\end{defn}


\begin{defn}[Algebra di divisione]
Un algebra di divisione è un'algebra in cui ogni elemento escluso lo $0$ possiede un inverso moltiplicativo.
\end{defn}

\begin{exmp}
Gli esempi più standard di algebra di divisione su $\R$ di dimensione finita sono i campi $\R$ e $\C$ come spazi vettoriali reali.
Un esempio più sofisticato è dato dal corpo dei quaternioni $\HH$ (ovviamente visto come spazio vettoriale su $\R$).
\end{exmp}


\begin{rem}
Consideriamo una rappresentazione irriducibile $\rho:G\to GL(V_\rho)$ con $V_\rho$ spazo vettoriale su $\R$. Allora l'insieme degli endomorfismi
di $\rho$ 
\[ \End_G(V_\rho)\]
è un'algebra di divisione su $R$ se dotato della composizione.
Infatti per lemma di Schur (in particolare la prima parte dell'enunciato, che vale su ogni campo) ogni elemento di $End_G(V_\rho)$ o è la funzione nulla oppure è un isomorfismo, quindi ammette inverso.
Se $\dim V_\rho = n$ allora $\End_G(V_\rho)$, essendo contenuto in $\End(V_\rho)$, ha dimensione finita, in particolare $\dim \End_G(V_\rho)\le n^2$.
Vedremo tuttavia che vale una limitazione molto più forte.
\end{rem}


Presentiamo ora un sorprendente teorema che afferma che non ci sono altre algebre di divisione di dimensione finita su $\R$ oltre a quelle che abbiamo elencato come esempi, ovvero $\R, \C, \HH$.
\begin{thm}[di Frobenius]
\label{thm: frobenius}
Sia $A$ un'algebra di divisione su $\R$ di dimensione finita. Allora si ha 
\[A \cong \R \quad \vee \quad  A \cong \C \quad \vee \quad A  \cong \HH \]
\end{thm}

\textsc{Dimostrazione:}
Indichiamo con $\bm{1}\in A$ l'elemento neutro rispetto alla moltiplicazione di $A$ (occhio: è un vettore di $A$, non un numero reale).
Il sottospazio vettoriale generato da $\bm{1}$ è chiaramente isomorfo ad $\R$ come $\R-$spazio vettoriale, ma in
realtà lo è anche come sottoalgebra: infatti se $a,b\in\R$ allora
\[a\bm{1} \cdot b\bm{1} = ab (\bm{1}\cdot \bm{1}) = ab\bm{1} \]
dove nel primo passaggio abbiamo usato la bilinearità e nel secondo il fatto che $\bm{1}$ è elemento neutro.
Dunque, con un lieve abuso di notazione, possiamo scrivere $\R \subseteq A$, intendendo per $\R$ proprio il sottospazio generato da $\bm{1}$.
Se $\dim A = 1$ allora sarebbe $A=\R$. D'ora in poi supponiamo $\dim A > 1$.

Vogliamo ora mostrare che esiste una sottoalgebra di $A$ isomorfa a $\C$.
Sia $\alpha\in A \setminus\R$ e indichiamo con $A[\alpha]$ la sottoalgebra generata da $\alpha$, ovvero consideriamo l'insieme
\[A[\alpha] = \left\{ \dsum_{n = 0} ^N a_n \alpha^n |\ N\in\mathbb{N}, \ a_i \in \R,\ \alpha \in A   \right\} \]
L'algebra $A[\alpha]$, essendo contenuta in $A$, ha necessariamente dimensione finita, quindi esisterà un certo $N$ per cui gli elementi
$\alpha^0, \alpha^1, \dots \alpha^N$ sono linearmente dipendenti. Più precisamente prendiamo il minimo $N$ per cui questo avviene.
Allora esistono dei coefficienti $a_n$ reali tali che
\[ \dsum_{n=0}^N a_n \alpha^n = 0 \]
Ovvero il polinomio a coefficienti reali
\[ p(x) = \dsum_{n = 0}^N a_n x^n \] 
è annullato da $\alpha$.
Se potessimo scomporre in modo non banale $p(x) = p_1(x)p_2(x)$ allora avremmo che $0 = p_1(\alpha)p_2(\alpha)$, dunque $\alpha$ annullerebbe uno dei due fattori (qui
stiamo usando che $A$ è un'algebra unitaria), il che sarebbe assurdo vista l'ipotesi di minimalità di $N$.
Dunque $p(x)$ deve essere irriducibile. Ma su $\R$ i polinomi irriducibili possono solo avere grado $1$ o $2$.
Vediamo rapidamente perché non può avere grado $1$.
Supponiamo per assurdo che sia
\[p(x) = a_0 + a_1 x\]
Ma ciò vorrebbe dire che $a_0+a_1\cdot\alpha=0$, ovvero $\alpha=-a_0/a_1 \in \R$, ma noi avevamo assunto $\alpha\not\in\R$.
Di conseguenza $p(x)$ ha grado esattamente $2$.
Non è difficile mostrare che $A[\alpha]$ ha esattamente dimensione $2$: se $\alpha^2$ si scrive in termini di potenze inferiori lo faranno anche tutte le potenze successive,
dunque ogni elemento di $A[\alpha]$ si può scrivere nella forma $x + \alpha y$ con $x,y$ reali e $\{1,\alpha\}$ è base di $A[\alpha]$, essendo i due elementi indipendenti.
A meno di moltiplicare $p(x)$ per una costante per renderlo monico, possiamo scrivere 
\[ p(x) = (x-s)^2+t^2\]
con $s,t$ reali, $t\neq 0$. Se definiamo
\[i = \frac{\alpha-s}{t}\]
è facile osservare che $i^2 = -1$. Ora $\{1, i\}$ è base di $A[\alpha]$ (essendo $1$ e $i$ linearmente indipendenti) e il nostro $i$
gioca esattamente lo stesso ruolo dell'unità immaginaria in $\C$. A questo punto è immediato esibire un isomorfismo
\[ A[\alpha] \cong \C \]
Se $\dim A =2 $ abbiamo finito. Supponiamo d'ora in poi $\dim A > 2$ e scriviamo $\C\subset A$ identificando $\C$ con la
sottoalgebra $A[\alpha]$. Definiamo ora un'applicazione $\varphi:A\to A$ nel modo seguente:
\[ \varphi(x) = -ixi = ixi^{-1} \]
Osserviamo che $\varphi$ è $\R-$lineare, infatti presi $a,b\in\R$ e $x,y\in A$ si ha:
\[ \varphi(ax+by) = i(ax+by)i^{-1} = i(ax)i^{-1} +i(by)i^{-1} = a\varphi(x) + b\varphi(y)\]
Inoltre osserviamo che $\varphi^2$ è l'identità. Questo implica che possiamo decomporre
\[ A = A_+ \oplus A_-\] dove $A_+$ e $A_-$ sono gli autospazi relativi rispettivamente agli autovalori $1$ e $-1$.
In particolare gli elementi di $A_+$ sono esattamente quelli che commutano con $i$.
Vogliamo ora mostrare che $A_+ = \C$.
Sia $\beta\in A_+$. Imitando il ragionamento fatto prima con $\alpha$, possiamo considerare il polinomio a coefficienti complessi di minimo grado
che si annulla in $\beta$. Come fatto prima osserviamo che deve essere irriducibile (questo passaggio richiede in realtà qualche cautela,
ma tutto funziona come deve poiché $\beta$ commuta con tutti gli elementi di $\C$. Se avessimo preso $\beta\in A_-$ il ragionamento sarebbe errato).
Ma gli unici polinomi irriducibili a coefficienti in $\C$ sono quelli di grado $1$, quindi $\beta\in\C$.

Abbiamo dunque stabilito che $A_+ = \C$. Dato che $\dim A > 2$ possiamo prendere $z\in A_-$ non nullo.
Consideriamo l'applicazione $\psi_z:A\to A$ definita da $\psi_z(x) = zx$.
Osserviamo che valgono le seguenti implicazioni:
\[ x\in A_+ \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = -zx = -\psi_z(x)\]
\[ x\in A_- \quad\implies\quad \varphi(\psi_z(x)) = \varphi(zx) = izxi^{-1} = izi^{-1}ixi^{-1} = (-z)(-x) = \psi_z(x)\]
ovvero $\psi_z$ scambia $A_+$ e $A_-$. Inoltre $\psi_z$ è bigettiva e lineare, dunque concludiamo che $A_+\cong A_-$ come $\R-$spazi vettoriali e in particolare $\dim A = 4$.

Con un ragionamento simile a quello che avevamo fatto per $\alpha$, osserviamo che $z^2$ è nel sottospazio generato da $1$ e $z$. Inoltre $z^2=\psi_z(z)\in A_+$.
Visto che $Span(1,z) \cap A_+ = \R$ deve essere per forza $z^2\in\R$.

Se fosse $z^2 \ge 0$ allora potremmo scrivere $z^2=r^2$ per qualche $r\in\R$.
Visto che $r$ e $z$ commutano sarebbe allora $(z-r)(z+r)=0$, dunque uno dei due fattori sarebbe $0$, assurdo perché $z\not\in\R$. Pertanto $z^2 < 0$ e possiamo definire
\[j = \frac{z}{\sqrt{-z^2}}\]
così $j^2 = -1$. Infine definiamo $k = ij$.

Ora $k$ e $j$ sono indipendenti e sono in $A_-$, dunque formano una base di $A_-$. Allora $\{1,i,j,k\}$ è base per $A$ ed è facile
verificare che questi quattro elementi rispettano le stesse regole di moltiplicazione dei quaternioni, pertanto si conclude che $A\cong\HH$ e la tesi è dimostrata.\qed

Dal teorema precedente si ha immediatamente che se $G\to GL(V)$ è una rappresentazione irriducibile su $\R$ allora $\End_G(V)$ è isomorfo a uno
tra $\R, \C, \HH$.

\begin{prop}
Sia $\rho:G\to GL(V)$ rappresentazione irriducibile su $\R$. Allora:
\begin{itemize}
\item $\End_G(V) \cong \R$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 1$
\item $\End_G(V) \cong \C$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 2$
\item $\End_G(V) \cong \HH$ se e solo se $\langle\chi_\rho|\chi_\rho\rangle = 4$
\end{itemize}
\end{prop}
\textsc{Dimostrazione: }
Basta ricordare che $\dim \End_G(V) = \langle\chi_\rho|\chi_\rho\rangle$ e la tesi segue subito
dal teorema di Frobenius.
\qed










\newpage
\section{Rappresentazioni indotte}
Supponiamo di avere $\rho:H\rightarrow GL(V)$ una rappresentazione di $H<G$ con $G$ gruppo. Il nostro obiettivo è quello di definire una $\widetilde{\rho}:G\rightarrow GL(V)$ rappresentazione di $G$ che estende $\rho$.

\begin{defn}Indichiamo con \emph{induzione da $H$ in $G$ di $W$} il seguente spazio vettoriale:
\[Ind_H^G(W)=\{f:G\rightarrow W: \rho(h)\circ f(gh)=f(g)\ \forall (g,h)\in G\times H\}   \]
\end{defn}

\textbf{Osservazione:} $Ind_H^G(W)$ è lo spazio vettoriale di una rappresentazione di $G$.
Si consideri $N=\{f:G\rightarrow W\}$: definisco
\[\rho_N:G\rightarrow GL(N) \textup{ tale che }(\rho_N(g)f)(g')=f(g^{-1}g')\]
Si verifica che $\rho_N$ è una rappresentazione di $G$ su $N$. Notiamo innanzitutto che $Ind_H^G(W)\subseteq N$ e che tale sottospazio è $\rho_N-$invariante poichè detta $f\in Ind_H^G(W)$ si ha che
\[ \rho_N(g)(f)(g')=f(g^{-1}g')=\rho(h)f(g^{-1}g'h)=\rho(h)\rho_N(g)(f)(g'h)\]
ovvero $\rho_N(g)f\in Ind_H^G(W)$. Questo ci dice che $Ind_H^G(W)$ è una sottorappresentazione di $\rho_N$ che chiamiamo $\rho_{ind}:G\rightarrow GL(Ind_H^G(W))$.

\begin{exmp}
$H=\{e\}$ e $W=\C$ con $\rho:H\rightarrow GL(\C)$: allora
\[Ind_H^G(W)=\{f:G\rightarrow \C:\rho(e)f(ge)=f(g)\}=\{f:G\rightarrow \C\}\]
\end{exmp}

\textbf{Notazione:} Spesso indicheremo lo spazio vettoriale $\{f:G\rightarrow \C\}$ con $\C[G]=\bigoplus_{g\in G}\C e_g$ dove gli $\{e_g\}_{g\in G}$ sono una base di $\{f:G\rightarrow \C\}$. 
In tal modo risulta che 
\[\{f:G\rightarrow \C\}=\C[G]\otimes W\]
sempre nel senso che se prendo $\{e_g\}_{g\in G}$ una base di $\C[G]$ e $\{w_i\}\in W$ una base di $W$ allora 
\[e_g\otimes w_i:G\rightarrow W \textup{ tale che } g\mapsto w_i \textup{ e } g'\mapsto 0\ \forall g'\neq g\]

Torniamo a $\rho_{ind}$: ricapitolando $\rho_{ind}:G\mapsto GL(Ind_H^G)$ tale che $g\mapsto \rho_{ind}(g)$ dove $\rho_{ind}(g)f(g')=f(g^{-1}g')$. Vediamo ora un po' delle sue caratteristiche. Poniamo per snellire la scrittura $V=Ind_H^G(W)$. Ricordandoci il nostro aim, di certo vogliamo che $W$ sia un sottospazio di $V$.

\begin{defn}
Data $f\in \{f:G\rightarrow W\}$ (che contiene $V$) definiamo il supporto di $f$ 

\[Supp(f)=\{f\in G:f(g)\neq 0\}.\]
\end{defn}

\begin{defn}
Dato $g\in G$ definiamo $V_g=\{f\in V : Supp(f)\subseteq gH \}$.
\end{defn}

\textbf{Osservazione:} se $g'\in gH$, allora $g'H=gH\Rightarrow V_g=V_{g'}$. Quindi più che dipendere dagli elementi di $G$, gli insiemi sopra definiti dipendono dalle classi laterali di $H$ in $G$. Perciò ora parleremo non più di $V_g$ bensì di $V_{gH}$.\\
\textbf{Osservazione:} dato che le classi laterali di $H$ sono una partizione di $G$, dette $\{g_iH\}_{i\in |G/H|}$ i rappresentanti delle classi laterali $gH$ al variare di $g$ in $G$ allora 
\[G=\bigsqcup_{i=1}^{|G/H|} g_iH\] 
e quindi ogni funzione la posso scrivere come combinazione lineare di $f_i\in V_{g_iH}$.
\[\Rightarrow V=\bigoplus_{i=1}^{|G/H|} V_{g_iH}\]
Voglio ora rintracciare $W$, in quanto sono interessata a trovare un sottospazio isomorfo a $W$ dentro $V$. Notiamo innanzitutto che la scrittura precedente non è detto che sia una decomposizione di rappresentazioni, ma possiamo vedere subito che si comporta bene sotto l'azione di $\rho_{ind}$. 

\begin{lemma}
$\rho_{ind}$ permuta le varie $V_{gH}$ nel senso che $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. 
\end{lemma}

\textsc{Dimostrazione:} Data $f\in V$, definisco
\[f_i(g)=\begin{cases}
f(g)& \textup{se } g\in g_iH\\
0& \textup{altrimenti}
\end{cases}\Rightarrow f=\sum_{i=1}^{|G/H|} f_i\ \textup{ e per come le ho definite } f_i\in V_{g_iH}\]
Sia ora $f\in V_{g'H}$ e fissiamo un $g\in G$ allora
\[\rho_{ind}(g)(f)(x)=f(g^{-1}x)\Rightarrow \rho_{ind}(g)(f)(x)\neq 0\leftrightarrow g^{-1}x\in Supp(f)\subseteq g'H\leftrightarrow x\in gSupp(f)\subseteq gg'H\]
Ovvero $x\in Supp(\rho_{ind}(g)(f))\Leftrightarrow x\in gSupp(f)$. Ciò implica che
\[ V_{g'H} \begin{matrix}
\overset{\rho_{ind}(g)}{\longrightarrow}\\ 
\underset{\rho_{ind}(g^{-1})}{\longleftarrow}
\end{matrix} V_{gg'H} \]
Essendo $\rho_{ind}(g),\rho_{ind}(g^{-1})\in GL(V)$ allora seguono iseguenti fatti:
\begin{enumerate}
\item $\rho_{ind}(g)V_{g'H}\subseteq V_{gg'H}$;
\item $\rho_{ind}(g^{-1})V_{gg'H}\subseteq V_{g'H}$;
\item $V_{g'H}\cong V_{gg'H}$, ovvero sono due spazi vettoriali isomorfi;
\item $\rho_{ind}(g)^{-1}=\rho_{ind}(g^{-1})$;
\end{enumerate}
Quindi $\rho_{ind}(g)V_{g'H}=V_{gg'H}$. \qed
\\ 
\begin{cor}
\[\forall i,j\in |G/H|\ \ \ V_{g_iH}\cong V_{g_jH}.\]
In particolare hanno tutti la stessa dimensione e quindi
\[dim(V)=|G/H|dim(V_{eH})=dim(Ind_H^G(W))\]
\end{cor}
Concentriamoci ora su $V_{eH}=\{f\in V:supp(f)\subseteq H\}$. Sia $h\in H$: allora $\rho_{ind}(h):V_{eH}\rightarrow V_{eH}$ in quanto $eH=hH$. Ciò significa che $\rho_{ind}|_{H}$ definisce una rappresentazione di $H$ in $V_{eH}$.
\begin{lemma}
La rappresentazione appena trovate di $H$ su $V_{eH}$ è naturalmente isomorfa a\\ $\rho:H\rightarrow GL(W)$.
\end{lemma}
\textsc{Dimostrazione:} Considero la seguente funzione: 
\[\Phi:V_{eH}\rightarrow W\ \textup{tale che } f\mapsto f(e)\]
ovvero la funzione che associa ad un $f$ la valutazione della stessa nell'elemento neutro. Verifichiamo innanzitutto che si tratta di un omomorfismo di rappresentazione. Ricordiamo chi è $V_{eH}$:
\[V_{eH}=\{f:G\rightarrow W: \rho(h)f(gh)=f(g)\ \forall h,g\ \ \textup{e } Supp(f)\subseteq H\}\]
$\Phi$ è un omomorfisimo di rappresentazioni $\Leftrightarrow$ $\Phi(\rho_{ind}(h)f)=\rho(h)\Phi(f)\ \forall h\in H$.
\[\Phi(\rho_{ind}(h)f))=\rho(h)\Phi(f) \Leftrightarrow (\rho_{ind}(h)f)(e)=\rho(h)(f(e))\Leftrightarrow f(h^{-1}e)=f(h^{-1})=\rho(h)f(e)\]
ma ciò è vero in quanto le $f\in Ind_H^G(W)$. \\
Verifichiamo che $\Phi$ è iniettiva. $Supp(f)\subseteq H$ e $\forall h\in H\ f(h)=\rho(h^{-1}f(e))$: questo mi dice che una volta che si è fissato il valore di $f(e)$ allora la funzione è univocamente determinata su $H$. D'altra parte so che in $G\setminus H$ la funzione è identicamente nulla $\Rightarrow$ quindi è univacamente determinata su tutto $G$. Allora detti $\Phi$ è iniettiva.\\
Vediamo ora che è anche surgettiva. Sia $w\in W$ vogliamo $f\in V_{eH}$ che valutata in $e$ dia $w$. Definiamo $f_w:G\rightarrow W$ nel seguente modo: 
\[f_w:=\begin{cases}
\rho(h^{-1})(w)& \textup{ se } h\in H\\
0& \forall g\in G\setminus H
\end{cases}\]
Si verifica che tale funzione soddisfa le proprietà richieste. \qed \\
Dunque possiamo ora concludere il seguente teorema
\begin{thm} Dette $g_1H,...,g_{|G/H|}H$ i rappresentanti delle classi laterali allora 
\[Ind_H^G(W)=\bigoplus_{i=1}^{|G/H|}\rho_{ind}(g_i)V_{eH}\bigl( =\bigoplus_{i=1}^{|G/H|} V_{g_iH}\bigr) \]
Inoltre $\rho_{ind}|_H$ definisce una rappresentazione di $H$ su $V_{eH}$ isomorfa a $\rho$. In particolare quindi 
\[dim(Ind_H^G(W))=|G/H|dim(W)\] 
\end{thm}
Vediamo ora degli esempi.
\begin{enumerate}
\item Abbiamo visto prima $H=\{e\}$ $W=\C$ allora $Ind_H^G(W)=\C[G]$ è la rappresentazione regolare di $G$. 
\item $H\neq \{e\},\subseteq G$ e $\rho:H\rightarrow GL(W)$ la rappresentazione banale con $W=\C$. Allora
\[Ind_H^G(\C)=\{f:G\rightarrow \C:\rho(h)f(gh)=f(gh)=f(g)\ \forall g,h\}\Rightarrow f \textup{ è costante sulle classi laterali di} H\]
\[\Rightarrow Ind_H^G(\C)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di }\ H\}=\C[G/H]\]


%% \begin{tikzpicture}
%% \centering
%% \matrix(m)[matrix of math nodes,row sep=3em,column sep=3em,minimum width=3em]
%% {
%% f:G& \C\\
%% G/H& \\};
%% \path[-stealth]
%% (m-1-1) edge (m-1-2)
%% (m-1-1) edge node [left] {$\pi$} (m-2-1)
%% (m-2-1) edge node [right] {$\ f$} (m-1-2);
%% \end{tikzpicture}\\

\[
\begin{diagram}
G          & \rTo^{f}            & \C \\
\dTo<{\pi} & \ruTo>{f \circ \pi} &    \\
G/H        &                     &    \\
\end{diagram}
\]


$G/H$ è un insieme finito dove $G$ agisce per moltiplicazione a sinistra ($g(g'H)=gg'H$). Quindi $Ind_H^G(W)$ è la rappresentazione per permutazione associata all'azione di $G$ in $G/H$. \\
\textbf{Recall:} $X$ è un $G-$insieme, sia $X= \bigsqcup X_i$ la decomposizione in $G-$orbite: allora 
\[\C[X]=\bigoplus_{i=1}^{\#-orbite} \C[X_i]\] 
\item $H\subseteq G$ e $\rho:H\rightarrow GL(W): \rho(h)=id\ \forall h\in H$. Nel caso in cui $W$ avesse dimensione 1 si torna all'esempio 1. Tuttavia anche se $dim(W)>1$, $Ind_H^G(W)=\{f:G\rightarrow \C:f \textup{ è costante sulle classi laterali di } H\}=\C[G/H]\otimes W$ 
\end{enumerate}
\textbf{Esercizio:} Supponiamo che $W=W_1\bigoplus W_2$ una rappresentazione di H non riducibile: allora 
\[Ind_H^G(W)=Ind_H^G(W_1)\bigoplus Ind_H^G(W_2)\].


\subsection{Formula di aggiunzione}
Vediamo ora una proprietà universale di queste rappresentazioni indotte. Prima abbiamo definito passando da $\rho$ a $\rho_{ind}$ sostanzialmente una applicazione
\[Ind:Rappr(H)\rightarrow Rappr(G)\]
e abbiamo anche "l'inversa" ovvero la restrizione della rappresentazione ad H
\[Res:Rappr(G)\rightarrow Rappr(H)\]
e risulta che $Res_H^G(V)=V$ visto però come una rappresentazione di $H$ ($\rho_{res}=\rho|_H$). Che legame c'è tra queste due applicazioni? 
\begin{thm}
Sia $\rho:H\rightarrow GL(W)$ una rappresentazione di $H$ e sia $\sigma:G\rightarrow GL(U)$ una rappresentazione di $G$. Allora
\[Hom_H(W,Res_H^G(U))\cong Hom_G(Ind_H^G(W),U)\]
ovvero che ogni $\phi:W\rightarrow Res_H^G(U)$ omomorfismo di $H-$rappresentazione si estende in modo unico a un $\overset{\sim}{\phi}:Ind_H^G(W)\rightarrow U$ omomorfismo di $G-$rappresentazione
\end{thm}
\textsc{Dimostrazione:} sia $\phi:W\rightarrow Res_H^G(U)$ un omomorfismo di $H-$rappresentazione e sia $V:=Ind_H^G(W)=\bigoplus_{i=1}^{\#-orbite} \rho_{ind}(g_i)V_{eH}$. Essendo $V_{eH}\cong W$ come $H-$rappresentazioni, allora $\phi:V_{eH}\rightarrow Res_H^G(U)$: voglio estenderlo a $V$. Essendo $V$ definito come una somma diretta è sufficiente definire l'omomorfismo sui blocchi e per questo considero $V_{g_iH}$: ho che
\[V_{g_iH}\overset{\rho_{g_i^{-1}}}{\rightarrow} V_{eH}\overset{\phi}{\rightarrow} Res_H^G(U)\overset{\sigma(g)}{\rightarrow} U\Rightarrow \sigma(g)\phi\rho_{ind}(g_i^{-1}):V_{g_iH}\rightarrow U\] 
Incollando tutte queste applicazioni ottengo $\overset{\sim}{\phi}:Ind_H^G(W)\rightarrow U$ dove
$\overset{\sim}{\phi}(v)=\sigma_{g}\circ \phi\circ \rho_{ind}(g^{-1})(v)$ con $v\in V_{gH}$. Si verifica che $\overset{\sim}{\phi}\in Hom_G(Ind_H^G(W),U)$. \\
Vediamo ora che questa estensione è unica. Siano $\overset{\sim}{\phi_1}$ e $\overset{\sim}{\phi_2}$ due estensioni di $\phi$: valutiamole nello stesso elemento $v\in V_{gH}$.
\[\overset{\sim}{\phi_1}(\rho_{ind}(g^{-1})v)=\phi(\rho_{ind}(g^{-1})(v))=\overset{\sim}{\phi_2}(\rho_{ind}(g^{-1})v)\]
dove l'uguaglianza è vera perchè $\rho_{ind}(g^{-1})(v)\in V_{eH}$. Quindi coincidendo sui singoli blocchi le due estensioni coincidono anche su $V$. E quindi l'estensione è unica. \qed






\subsubsection{Le rappresentazioni dei gruppi diedrali $D_{2n}$}
Per fare un esempio di come si possano usare le rappresentazioni indotte, studiamo le rappresentazioni dei gruppi diedrali. Ricordiamo che i gruppi diedrali hanno due generatori, che in accordo con quello che è stato fatto all'inizio del corso chiameremo $\sigma$ e $\tau$. Il gruppo $G = D_{2n}$ si scrive quindi

\[ D_{2n} = \{ 1, \sigma, \sigma^2, ... \sigma^{n-1}, \tau, \tau\sigma, ... \tau\sigma^{n-1} | \ \tau^2 = \sigma^n = 1, \quad, \sigma\tau = \tau\sigma^{n-1} \}\]


Consideriamo l'ovvio sottogruppo $H$ generato da $\sigma$, che è evidentemente un gruppo ciclico, isomorfo a $\Z / n \Z$. Le rappresentazioni di questo gruppo, dato che è abeliano, sono decisamente banali, in quanto sono di grado 1 e sono le radici ennesime dell'unità. In particolare la rappresentazione $\rho_k$ è univocamente determinata dal suo valore sul generatore. Indicheremo in modo naturale

\[\rho_k(\sigma) = \omega^k  \qquad \omega = e^{\frac{2\pi i}{n}}\]


Studiamo ora la rappresentazione che induce $\rho_k$, ovvero studiamo $Ind_G^H \rho_k$, che per amore di brevità indicheremo con $Ind \rho_k$. \'E abbastanza utile notare che noi conosciamo la dimensione della rappresentazione indotta, in quanto conosciamo la formula

\[ dim Ind^H_G \rho_k = |G/H| deg \rho_k = 2 \cdot 1 = 2\]

Per andare avanti è intelligente ricordare la definizione di questo spazio. Indicheremo con $W$ lo spazio su cui agisce $Ind \rho_k$ (che è ovviamente isomorfo a $\C^2$)


\[ Ind \rho_k = \{ f : G \to W \ f(gh) = \rho(h)^{-1} f(g) \quad \forall g \in G, \forall h \in H \}\]

Possiamo prendere una base di questo spazio per scrivere la matrice che le rappresenta e calcolarne poi il carattere, per esempio. Scegliamo questa base: prendiamo le funzioni $f_H, f_{\tau H}$ definite da 

\[ 
\begin{cases}
f_H (e) = 1 \\
f_H (\tau) = 0 \\
\end{cases}
\qquad
\begin{cases}
f_{\tau H} (e) = 0 \\
f_{\tau H } (\tau) = 1 \\
\end{cases}
\]



Evidentemente questa è una base e, data la definizione dello spazio, è anche sufficiente a determinare il comportamento delle funzioni su tutto il gruppo $G$.

In particolare una generica funzione $f \in Ind \rho_k$ si potrà scrivere come $\C$ combinazione lineare delle due funzioni appena definite, ovvero

\[ f = a f_H + b f_{\tau H} \qquad a,b \in \C\]


Possiamo ovviamente esprimere la funzione in modo diretto come

\[ f(g) = f(1) f_H(g) + f(\tau) f_{\tau H} (g) \]

Ora che abbiamo lo spazio dove agiscono le rappresentazioni indotte, dobbiamo trovare esplicitamente le rappresentazioni. Per ogni $\rho_k$, indicheremo con $\hat \rho_k$ la sua indotta. Per vedere come è fatta, facciamola agire sulla base dello spazio. In questo modo potremo scriverla come matrice.

Dobbiamo quindi calcolare

\[ 
\begin{cases}
\hat \rho_k (\sigma) f_H \\ 
\hat \rho_k (\sigma) f_{\tau H} \\
\end{cases}
\qquad
\begin{cases}
\hat \rho_k (\tau) f_H \\
\hat \rho_k (\tau) f_{\tau H}\\
\end{cases}
\]

Facciamo questa cosa in quanto in realtà dovremmo calcolare su ogni elemento del gruppo la matrice, ma sappiamo che $\sigma$ e $\tau$ generano il gruppo, ed è più facile moltiplicare matrici piuttosto che fare conti di questo tipo.  Per esempio


\[ 
\hat \rho_k (\sigma) f_H(1) = f_H(\sigma^{-1}) = \rho_k(\sigma) f_H(1) = \omega^k f_H(1)
\]

Con conti analoghi si ottiene


\[ 
\hat \rho_k (\sigma) = 
\left(
\begin{array}{cc}
\omega^k & 0 \\
0 & \omega^{-k} \\
\end{array}
\right)
\qquad
\hat \rho_k (\tau) = 
\left(
\begin{array}{cc}
0 & 1 \\
1 & 0 \\
\end{array}
\right)
\]


A questo punto abbiamo un sacco di rappresentazioni di $D_{2n}$. Se vogliamo costruire tutte le rappresentazioni del gruppo dobbiamo domandarci se sono irriducibili o meno e in caso decomporle. Alcuni casi particolari si vedono abbastanza facilmente. Per esempio, per $k=0$, $\hat \rho_k(\sigma) = Id$. Di conseguenza, tutte le $\hat \rho_0$ sono simultaneamente diagonalizzabili e quindi in realtà la rappresentazione non sarà irriducibile in quanto il gruppo diventa abeliano.



BISOGNA FINIRE CHE DEVO ANDARE A LEZIONE.






















\newpage
\section{Rappresentazioni di gruppi compatti}

Ci piacerebbe in qualche modo estendere quello che abbiamo fatto per i gruppi finiti ad una categoria particolare di gruppi infiniti, in particolare i gruppi compatti. Per farlo, abbiamo bisogno di un po' di definizioni che generalizzino quello che abbiamo fatto. 


\begin{defn}[Gruppo topologico] Dato un insieme $G$, una funzione $\cdot: G\times G \to G$ e un sottoinsieme delle parti di $G$, $\tau$, la terna $(G, \cdot, \tau)$ si dice gruppo topologico se valgono le seguenti proprietà:

\begin{itemize}
\item $(G, \cdot)$ è un gruppo.
\item $(G, \tau)$ è uno spazio topologico.
\item Le due operazioni $\cdot : G \times G \to G$ e $i: G \to G$, la seconda definita come la mappa che manda $g $ in $g^{-1}$,  sono continue secondo la topologia indotta da $\tau$.
\end{itemize}

\end{defn}


La definizione che abbiamo dato sopra è in un certo senso l'unica che si poteva dare per mettere insieme topologia e gruppi, in quanto le prime due sono obbligate e la terza di dice che in qualche modo vogliamo che le due operazioni di gruppo e di topologia si parlino fra di loro e che non siano indipendenti. Si possono fare diversi esempi, come


\begin{itemize}
\item $G$ finito, con la topologia discreta
\item $G$ qualsiasi, con la topologia discreta
\item Il gruppo additivo dei numeri reali $(\R, +)$ con la topologia euclidea
\item Il gruppo $GL_n (\R)$ (o anche su $\C$), con la topologia indotta da quella di $M_n(\R)$

\end{itemize}


In questo corso non parleremo di rappresentazioni di gruppi topologici in generale, ma solo di alcuni, ovvero quelli compatti. Per cui diamo la definizione

\begin{defn}
$(G, \cdot, \tau)$ si dice compatto se $(G, \tau)$ è compatto.
\end{defn}


Facciamo un po' di esempi di gruppi compatti che andremo a trattare
\begin{itemize}
\item I gruppi finiti, con la topologia discreta.
\item I gruppi $O_n, SO_n, U_n, SU_n$: dimostriamo che effettivamente lo sono (lo dimostriamo per $O_n$ perchè per $U_n$ la dimostrazione è analoga e gli altri sono sottogruppi).
Se $G\subseteq \R^m$ dotato della topologia euclidea indotta allora
\[G\ \text{compatto}\ \Leftrightarrow G\ \text{è chiuso in}\ \R^m\ \text{e limitato}\]
Considero $O_n(\R)\subseteq \R^{n^2}$: esso è così descritto
\[O_n(\R)=\{A\in M_n(\R):\ A^tA=I\}=\{A\in M_n(\R):\ \sum_{h=1}^{n}a_{ih}a_{jh}=\delta_{ij}\ \forall i,j=1,..,n\}\]
$\Rightarrow O_n$ è chiuso poichè descritto da equazioni (risulta essere l'intersezione finita di cotroimmagini di chiusi $\{1\},\{0\}$). Inoltre per $i=j\Rightarrow \sum_{h=1}^n a_{ih}^2=1\Rightarrow |a_{ij}|\leq 1\ \forall i,j\Rightarrow O_n$ è limitato.
\end{itemize}

In particolare può essere utile notare alcuni fatti noti che possono aiutare, cioé
\begin{align*}
SO_2(\R) \cong \mathbb{S}^1 \\
SU_2(\C) \cong \mathbb{S}^3
\end{align*}

A questo punto sarà opportuno dare una nozione di rappresentazione in cui entra in gioco la topologia di $G$, oltre alla sua struttura di gruppo. 
\begin{defn}[Rappresentazione continua]
Consideriamo una rappresentazione del gruppo topologico $G$, $\rho: G \to GL(V_\rho)$. Diciamo che la rappresentazione è continua se la mappa
\[ \rho: G \to GL(V_\rho)\]
è continua. Per il primo termine bisogna considerare la topologia su $G$, che è definita in quanto il gruppo è topologico, mentre su $GL(V_\rho)$ si usa la topologia euclidea. 
\end{defn}

Facciamo alcuni esempi
\begin{itemize}
\item Se $G$ è finito, ovviamente con la topologia discreta ogni rappresentazione è continua. In un certo senso questo non è così banale, in quanto ci dice che quello che andremo a fare sarà una generalizzazione di quello che abbiamo fatto per gruppi finiti.
\item Un'altra rappresentazione del tutto ovvia: la rappresentazione del gruppo $GL(V)$ in $GL(V)$ tramite l'identità, che è evidentemente continua
\item La mappa esponenziale $e^x: (\R, +) \to (\R^+, \cdot )$ che associa $x$ a $e^x$
\end{itemize}



\subsection{Le rappresentazioni di $\mathbb{S}^1$}
Per chiarirci le idee, andremo a studiare nel dettaglio un gruppo particolare, ovvero $\mathbb{S}^1$, che è ovviamente isomorfo a $SO_2(\R)$. Cerchiamo quindi innanzitutto le sue rappresentazioni irriducibili su spazi di dimensione finita sui complessi. 

\begin{lemma}
Tutte le rappr. irrid. $/_\C$ di un gruppo compatto abeliano $G$ hanno dimensione 1. Di conseguenza, essendo $\mathbb{S}^1$ un gruppo compatto abeliano, tutte le rappresentazioni di $S^1$ irriducibili su uno spazio vettoriale complesso hanno dimensione 1.
\end{lemma}
\textbf{Dimostrazione:} $G$ è un gruppo compatto abeliano e sia $\rho:G\rightarrow GL(V)$ con $V$ $\C-$sp. vettoriale: quindi
\[ gh = hg \qquad \forall g,h \in G \ \Rightarrow\ \rho(g)\rho(h) = \rho(h)\rho(g) \qquad \forall g, h \in G\]
Il che vuol dire che, fissato $h$, $\rho(h)$ è un'omomorfismo di rappresentazioni irriducibili su $\C$, ovvero appartiene a $End_{S^1}(V)$ proprio perchè commuta con $\rho(g)$ per tutte le $g$. Sia $\lambda$ un autovalore di $\rho(h)$ (esiste perchè siamo su $\C$): allora $\rho(g)-\lambda Id\in End_{S^1}(V)$ e ha un nucleo $\Rightarrow$ per Schur il nucleo coincide con $V$ essendo $V$ irriducibile. Allora $\rho(h) \equiv \lambda(h) Id$. Quello che abbiamo fatto vale per ogni $h \in G$, per cui tutti i $\rho(g) $ sono in realtà scalari. Questo ci dice che ogni base di $V$ è una base di autovettori per $\rho(g)\ \forall g\in G$ e quindi ogni autospazio è $G-$invariante. Dunque le rappresentazioni irriducibili di $G$ sono tutte di grado 1.\qed


\begin{prop} Le rappresentazioni irriducibili di $\mathbb{S}^1$ sono tutte e sole le $\rho_n:\mathbb{S}^1\to\C^*$ definite, al variare da $n$ in $\Z$, nel seguente modo:
\[ \rho_n(z) = z^n \]
\end{prop}


\textsc{Dimostrazione:}
Innanzitutto notiamo che le $\rho_n$ definite nell'enunciato sono effettivamente delle rappresentazioni irriducibili di $\mathbb{S}^1$, quindi bisogna solo verificare che non ce ne sono altre.

Sappiamo che le rappresentazioni irriducibili dovranno avere dimensione $1$, per cui saranno omomorfismi $\rho:\mathbb{S}^1\to\C^*$. Inoltre dovrà per forza essere $|\rho(z)| = 1 \quad \forall z \in \mathbb{S}^1$. Questo si dimostra abbastanza facilmente. Supponiamo per assurdo infatti che si abbia
\[ |\rho(x)| > 1\]
per qualche $x$. Allora anche $\rho(x)^n$ farebbe parte di $\rho(G)$. Tuttavia qui c'è un assurdo in quanto $\rho(G)$ è compatto perché la rappresentazione è continua (compatti vanno in compatti), mentre $\rho(x)^n$ non è limitato. Se invece per assurdo fosse $|\rho(x)| < 1$ si potrebbe usare lo stesso argomento notando che $|\rho(x^{-1})| > 1$.
Di conseguenza sappiamo che $\rho(\mathbb{S}^1) \subseteq \mathbb{S}^1$.
Consideriamo ora la mappa
\begin{align*} 
\phi : (\R, +) &\to \mathbb{S}^1 \\
x &\to  e^{ix}
\end{align*}
che è evidentemente continua ed è omomorfismo di gruppi (è un rivestimento).
Sia $\hat\rho$ l'applicazione che fa commutare il seguente diagramma:
\[\tridiag \R \phi {\mathbb{S}^1} \rho {\mathbb{S}^1} {\hat\rho}\]
Essendo composizione di omomorfismi continui, anche $\hat\rho$ è omomorfismo continuo. Si può dimostrare (ma non lo facciamo in questa sede) che esiste un'applicazione continua $\theta:\R\to\R$ che fa commutare il diagramma:
%~ Si può dimostrare che questa mappa è l'unica che soddisfi le proprietà del rivestimento universale da $\R$ a $\mathbb{S}^1$. Ci è utile perché in qualche modo scarica il problema di trovare le rappresentazioni da e verso $\mathbb{S}^1$ a cercare delle rappresentazioni da e verso $(\R, +)$. Per l'unicità del rivestimento concluderemo che sono anche tutte. 
\[
\begin{diagram}
    \R          & \rTo^{ \phi }         & \mathbb{S}^1           \\
\dTo<{\theta}   & \rdTo^{\hat \rho}     & \dTo>{\rho}            \\
    \R          &  \rTo_{\phi}          & \mathbb{S}^1           \\
\end{diagram}
\]
Ovvero che fattorizza $\hat\rho=\phi\theta$. A questo punto $\theta(0)$ deve essere un multiplo intero di $2\pi$, visto che deve risultare $e^{i\theta(0)} = \hat\rho(0) = 1$.
Quindi, a meno di traslare $\theta$ di un multiplo intero di $2\pi$ (il che non interferisce con la commutatività del diagramma), possiamo assumere $\theta(0) = 0$. Dati $x,y$ reali deve valere
\[ e^{i\theta(x+y)} = \hat \rho(x+y) = \hat \rho(x )+ \hat \rho(y) = e^{i\theta(x)+i \theta(y)}\]
per cui deve essere
\[ \theta(x+y) - \theta(x) - \theta(y) \in 2\pi \Z \qquad \forall x, y \in \R \]
Ma visto che $\theta$ è continua l'espressione sopra deve essere necessariamente costante al variare di $x$ e $y$. Ponendo $x=y=0$ ricaviamo che tale costante è $-\theta(0)=0$, dunque
\[ \theta(x + y) = \theta(x ) + \theta (y) \qquad \forall x,y\in \R\]
A questo punto si osserva (sfruttando la continuità di $\theta$) che deve essere $\theta(x) = \theta(1) x$, con $\alpha \in \R$. Non solo, possiamo dare delle informazioni a riguardo di $\theta(1)$. Infatti, dato che $\hat\rho$ è ``periodico'', deve essere
\[ 1 = \hat\rho(0) = \hat\rho(2\pi) = e^{i\theta(1) 2\pi}\]
per cui in realtà si ha $\theta(1) \in \Z$. 
Ricapitolando, abbiamo mostrato che per ogni $x\in\R$ vale che
\[e^{i 2\pi\theta(1)x} = \rho(e^{ix}) \]
dove $\theta(1)$ è un intero fissato. Ovvero per ogni $z\in\mathbb{S}^1$ vale
\[ \rho_{\theta(1)}(z) = z^{\theta(1)} = \rho(z) \]
che è quello che volevamo mostrare.
\qed

A questo punto il nostro obiettivo sarebbe quello di cercare la stessa cosa che abbiamo fatto per i gruppi finiti, ovvero scomporre una rappresentazione generica come somme di rappresentazioni irriducibili. Per farlo in sostanza abbiamo inventato quel prodotto hermitiano invariante che da solo ci ha permesso praticamente di fare tutto. Sarebbe molto bello avere una cosa simile anche per questi gruppi topologici. In particolare per i gruppi compatti possiamo andare a cercare di definire qualcosa di molto simile.


\begin{defn}[Integrazione su un gruppo]
Sia $G$ un gruppo topologico. Consideriamo un'applicazione $I: C_{\R}(G) \to \R$
dove con $C_\R$ si intendono le funzioni continue da $G$ in $\R$\footnote{Tutto funziona allo stesso modo se al posto di $\R$ mettiamo $\C$. In futuro non si baderà a questa distinzione.}. La mappa $I$ si dice integrazione se rispetta le seguenti proprietà.
\begin{itemize}
\item \`E lineare: $I(a f + b g) = aI(f) + bI(g)$.
\item Se la funzione $f$ è positiva ($ > 0$), allora anche l'integrale deve essere positivo ($> 0$).
\end{itemize}
\end{defn}


A questo punto, in analogia con quello che abbiamo fatto con i prodotti finiti, cerchiamo di trovare quello invariante. 

\begin{defn} Sia $G$ un gruppo compatto e $\rho: G \to GL(V_\rho)$ una sua rappresentazione continua. Sia $I$ una integrazione su $G$. Si dice che $I$ è invariante sotto $\rho$ se 
\[ I(f(x)) = I(f(\rho(g) x)) \qquad \forall g \in G\]
\end{defn}

Se questa integrazione esiste, allora in analogia a quanto fatto per i gruppi finiti possiamo sperare che le rappresentazioni di un gruppo compatto siano completamente irriducibili.


\begin{lemma}
  Se esiste l'integrazione invariante su un gruppo, allora esiste una forma hermitiana definita positiva invariante sotto l'azione del gruppo.
\end{lemma}
\begin{proof}
  Sia $h$ una forma hermitiana definita positiva su $V_\rho$. Consideriamo la quantità
  \[ h_G(v,w) = I( h(\rho(g) v, \rho(g) w)) \]

  Evidentemente è di nuovo una forma hermitiana definita positiva ed è invariante sotto $G$.


\end{proof}




\begin{thm} Sia $G$ un gruppo compatto e $\rho : G \to GL(V_\rho)$ una sua rappresentazione continua, con $V_\rho$ di dimensione finita. Se esiste una integrazione invariante $I$, allora la rappresentazione è completamente riducibile.

\end{thm}

\begin{proof}
  La dimostrazione è identica alla dimostrazione \ref{thm:gruppo finito completamente riducibile}, usando la forma hermitiana invariante appena definita. 
\end{proof}


\begin{thm}[Teorema di Haar]
Per ogni gruppo compatto esiste l'integrazione invariante. 
\end{thm}

Di questo teorema non daremo la dimostrazione in questo corso. \\

\textbf{Esempi:}















\newpage
\subsection{Le rappresentazioni di $SU(2)$}



Dopo aver studiato in dettaglio $\mathbb{S}^1$, vediamo come si comporta un suo parente stretto, $SU(2)$. Ricordiamo che $SU(2)$ è definito come un sottoinsieme di $GL(\C^2)$. In particolare ogni matrice di $SU(2)$ si può scrivere come 

 \[ SU(2) = \left\{  \left(\begin{array}{cc} z & w \\ -\overline{w} & \overline z \end{array}\right) \quad z, w \in \C  , \qquad |z_1|^2 + |z_2|^2 = 1\right\} \]

 È abbastanza naturale considerare $\C^2 \cong \R^4$ e accorgersi che in sostanza $SU(2) \cong \mathbb{S}^3$, in quanto tutti gli elementi rispettano l'equazione

 \[ x_1 ^2 + x_2^2 + x_3^2 + x_4^2 = 1 \]

 E scrivendo in questo modo ci accorgiamo subito che quindi $SU(2)$ è un gruppo compatto e connesso.

 Come abbiamo fatto per $\mathbb{S}^1$, vorremmo andare a studiare tutte le sue rappresentazioni irriducibili. Per farlo, innanzitutto sarebbe intelligente farsi un'idea di come può essere fatta l'integrazione invariante su questo gruppo. Se lo immergiamo in $\R^4$, possiamo sfruttare l'integrazione lì definita e quindi fare un integrale triplo

 \[ I(f) = \dint_{\mathbb{S}^3} f(g) dg = \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi\]


 L'invarianza dell'integrazione sotto l'azione di $G$ deriva sostanzialmente dal fatto che quando si cambia variabili bisogna moltiplicare l'integrando per il determinante dello Jacobiano. Tuttavia $det(SU(2)) = 1$ e quindi non ci sono problemi.


 Vediamo ora le rappresentazioni di classificare tutte le rappresentazioni irriducibili di $SU(2)$. Consideriamo per esempio un'azione di $SU(2)$ sullo spazio $V_m = \C[xy]_m$ dei polinomi omogenei su $\C$ si grado $m$ definita in questo modo 

 \[
 \rho_m(A) f(x, y) = f(a_{11} x + a_{21} y , a_{12} x + a_{22} y) \qquad \forall f \in V_m, \forall A \in SU(2)
 \]



 Per farci un'idea di come sia fatta questa rappresentazione possiamo per esempio studiarla qualitativamente su un sottogruppo di $SU(2)$ che già conosciamo bene, ovvero $\mathbb{S}^1$. Una matrice di $\mathbb{S}^1 \subset SU(2)$ si potrà scrivere come

 \[
 A = 
 \left(
 \begin{array}{cc}
   e^{i\theta} & 0 \\
   0 & e^{-i\theta} \\
 \end{array} 
 \right)
 \]


 Possiamo prendere una base di $V_m$, la più scontata che ci viene in mente, e vedere come agisce su $V_m$ il nostro gruppo. La base più banale che ci può venire in mente è sicuramente $\{ x^ky^{m-k} \}_{k \in \{0, ..., n\}}$.

 \[ \rho(A(\theta)) (x^ky^{m-k}) = e^{i(m-2k)} x^k y^{m-k} \]

 Per cui abbiamo avuto un grosso colpo di fortuna in quanto abbiamo sparato una base a caso ed è già di autovettori per il sottogruppo che stiamo guardando.

 BISOGNA FINIRE DI SCRIVERE

 \begin{prop}
   \label{prop:rapp su2}
   Le rappresentazioni irriducibili di $SU(2)$ sono tutte e sole le $\rho_m$ definite poco fa.
 \end{prop}

 \begin{proof}
   QUALCUNO LA SCRIVA.
 \end{proof}
 

 \paragraph{Le classi di coniugio di $SU(2)$}
 Dato che abbiamo dato tanto peso alla teoria del carattere per i gruppi finiti, probabilmente anche per i gruppi compatti ci saranno applicazioni interessanti. Dato che il carattere è una funzione di classe, è intelligente andare a studiare nel dettaglio le classi di coniugio di $SU(2)$.


 Abbiamo già visto che possiamo immergere $\mathbb{S}^1$ in $SU(2)$. Vediamo come questo abbia a che fare con le classi di coniugio.


 Ricordiamo che possiamo scrivere un generico punto di $\mathbb{S}^3$ come

 \[
 \begin{cases}
 x_1 = \cos\theta \\
 x_2 = \sin\theta\cos\phi \\
 x_3 = \sin\theta\sin\phi\cos\psi \\
 x_4 = \sin\theta\sin\phi\sin\psi \\
 \end{cases}
 \]

 Per cui possiamo scrivere il generico elemento di $SU(2)$ come

 \[
 g(\theta, \phi, \psi) = 
 \left(
 \begin{array}{cc}
   \cos\theta + i \sin\theta \cos\phi & \sin\theta\sin\phi( \cos\psi + i \sin\psi) \\
   -\sin\theta\sin\phi(\cos\psi - i \sin\psi) & \cos\theta - i \sin\theta\cos\phi  \\

 \end{array}
 \right)
 \]
 

 Dato che le matrici di $SU(2)$ sono normali, possiamo usare il teorema spettrale normale per dire che sono tutte diagonalizzabili. Per questo motivo le classi di coniugio saranno tutte matrici di Jordan diagonali, senza 1 sulla sopradiagonale. Per questo motivo la classe di coniugio sarà univocamente determinata dagli autovalori della matrice. Il polinomio caratteristico di una matrice $2\times 2$ è

 \[ p(t) = t^2 - \tr(M) t + det(M) \]

 Ma dato che siamo in $SU(2)$ il determinante è 1. La traccia è invece con una banale somma $2\cos\theta$. I due autovalori hanno modulo 1 e quindi in sostanza sono $e^{i\theta}$ e $e^{-i\theta}$. Per questo motivo un generico elemento di $SU(2)$ è


 \[
 g(\theta, \phi, \psi) \sim \left(
 \begin{array}{cc}
   e^{i\theta} & 0 \\
   0 & e^{-i\theta} \\
 \end{array}
 \right) = g(\theta, 0 , 0)
 \]




 Per questo motivo la funzione $\frac{1}{2} \tr(g) : SU(2) \to \R$ è una funzione suriettiva in $[-1,1]$ e le sue fibre sono le classi di coniugio.

 \paragraph{Il carattere delle rappresentazioni irriducibili di $SU(2)$}


 Prendiamo una funzione $f$

 \[ f \in \C(SU(2)) ^\# = \{ \text{Funzioni di classe } f: SU(2) \to \C  \} \]

 Sappiamo che se questa funzione è di classe, allora non dipende dal rappresentante della classe di coniugio. Per questo motivo possiamo quindi dire che

 \[ f(g(\theta, \phi, \psi)) = f(g(\theta, 0 , 0)) \]
 Vorremmo andare a studiare come si comporta l'integrazione invariante su questa funzione e poi considerare il caso particolare in cui questa funzione sia per l'appunto il carattere di $\rho$, una generica rappresentazione di $SU(2)$.

 \begin{align*}
 \dint_{\mathbb{S}^3} f(g) dg &= \dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, \phi, \psi) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &=\dfrac{1}{2\pi^2} \dint_0^\pi \dint_0^{\pi} \dint_0 ^{2\pi} f(\theta, 0, 0) \sin^2\theta\sin\phi \ d\theta \ d\phi \ d\psi = \\
 &= \dfrac{2}{\pi} \dint_0^\pi f(\theta, 0, 0) \sin^2\theta \ d\theta = \\
  &= \dfrac{1}{\pi} \dint_0^{2\pi} f(\theta, 0, 0) \sin^2\theta \ d\theta 
 \end{align*}
 
 Quindi l'espressione si semplifica parecchio. Prendiamo ora per l'appunto una rappresentazione di $\rho$ e vediamo come si comporta il carattere. In particolare prendiamo una delle $\rho_m$ irriducibili che abbiamo trovato prima. Sappiamo che

 \[ \rho_m: SU(2) \to GL(V_m) \qquad V_m = \bigoplus_{k=0}^m V_{m,k}\]
 
 In particolare siamo già riusciti a diagonalizzare $\rho_m$. Avremo che

 \begin{align*}
   \chi_{\rho_m} (\theta, \phi, \psi) &= \chi_{\rho_m}(\theta) = \chi_{\rho_m}(e^{i\theta}) = \\
   &= \dsum_{k=0}^m \rho_{m-2k}^{\mathbb{S}^1}(e^{i\theta}) = \dsum_{k=0}^m e^{i(m-2k)} \\
   &= \dfrac{e^{i(m+i)\theta} - e^{-i(m+1)\theta}}{e^{i\theta} - e^{-i\theta}} = \dfrac{\sin((m+1)\theta)}{\sin\theta}
 \end{align*} 
 


 Per cui, in analogia a quanto fatto per i gruppi finiti, diamo la definizione di

 \begin{defn}[Prodotto hermitiano di caratteri]

   \[
   \langle \chi_\rho | \chi_\sigma \rangle = \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg
   \]

 \end{defn}

 Che andiamo prontamente a calcolare in questo caso

 \[
 \langle \chi_{\rho_n} | \chi_{\rho_m} \rangle = \dfrac{1}{\pi}\dint_0^{2\pi} \dfrac{\sin((n+1)\theta)}{\sin\theta} \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta \ d\theta = \delta_{mn}
 \]


 Cosa molto simile a quanto era già successo per i gruppi finiti. A questo punto viene il sospetto che possa esserci qualcosa di più profondo sotto. Vedremo fra poco dei teoremi che generalizzano questi risultati. 








 \begin{thm}
   \label{thm:ortogonalita compatto}
   Sia $G$ un gruppo compatto  e siano $\rho$ e $\sigma$ due sue rappresentazioni irriducibili. Allora vale
   \[
   \langle \chi_\rho | \chi_\sigma \rangle =
   \begin{cases}
     1 \qquad \text{se } \sigma \cong \rho \\
     0 \qquad \text{altrimenti} \\
   \end{cases}
   \]
 \end{thm}

 Per dimostrare il teorema appena enunciato ci servono un paio di lemmi:

 \begin{lemma}
   Siano $\rho: G \to GL(V)$ e $\sigma: G \to GL(W)$ due rappresentazioni\footnote{Non necessariamente irriducibili} di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$ un omomorfismo di spazi vettoriali\footnote{Per gli amici, un'applicazione lineare.}. Allora possiamo definire un $\phi^G \in \Hom_G(V, W) $ nel seguente modo

   \[ \langle \phi^G(v) | w \rangle = \dint_G \langle \sigma(g) \phi (\rho(g) v) | w\rangle dg\]

   Dove $\langle \cdot | \cdot \rangle : W \times W \to \C$ è una forma hermitiana definita positiva.
   
 \end{lemma}

 \begin{proof}
   Dobbiamo in sostanza dimostrare che l'omomorfismo che abbiamo definito è anche un omomorfismo di rappresentazioni, ovvero che è $G-$equivariante, in quanto la linearità è assicurata dal prodotto hermitiano. In formule, dobbiamo mostrare che
   \[ \phi^G (\rho(g) v) = \sigma(g) \phi^G(v) \qquad \forall g \in G, \forall v \in V \Leftrightarrow \phi^G(v) = \sigma(g) ^{-1} \phi^G(\rho(g) v) \qquad \forall g \in G, \forall v \in V \]

   In sostanza la tesi segue dalla definizione sfruttando il fatto che l'integrazione su $G$ sia invariante


   \[ \langle \sigma(g) ^{-1} \phi^G(\rho(g) v) | w \rangle  = \dint_G \langle \sigma(hg) ^{-1} \phi^G(\rho(hg) v) | w \rangle dh = \langle \phi(v) | w \rangle\]

   Dove ho saltato un paio di passaggi ma il succo è che essendo l'integrazione invariante per traslazioni di $G$, è evidente che l'oggetto definito in questo modo diventa invariante sotto $G$
 \end{proof}

 \begin{lemma}
   Siano $\rho: G \to V$ e $\sigma: G \to W$ rappresentazioni irriducibili di un gruppo compatto $G$ e sia $\phi \in \Hom(V, W)$. Allora

   \[
   \begin{cases}
     \phi^G = 0 \qquad \text{se } \rho \ncong \sigma \\
     \phi^G = \frac{\tr \phi}{dim V}Id_V \qquad \text{altrimenti} \\
   \end{cases}
   \]

 \end{lemma}

 \begin{proof}
   Nel caso in cui le due rappresentazioni non siano isomorfe, la tesi segue banalmente dal lemma di Schur. Nell'altro caso, possiamo di nuovo usare Schur e dire che $V \cong W$ come spazi vettoriali, ma anche che in un certo senso sono lo stesso spazio, in quanto $\phi = \lambda Id$, sempre per Schur. L'unica cosa che rimane da mostrare è che vale proprio $\tr \phi^G = \tr \phi$. Per mostrarlo, possiamo fissare una base ortonormale di $V$, $\{ e_n\}_{n\in I}$ e notare che in questa base

   \[ \tr(\psi) = \dsum_{n \in I}\langle \psi e_n | e_n \rangle \qquad \forall \psi \in \End(V)\]
   Per cui,

   \begin{align*} \tr \phi^G &= \dsum_{n \in I}\langle \phi^G e_n | e_n \rangle = \dsum_{n \in I} \dint_G \langle \rho(g)^{-1} \phi(\rho(g) e_n) | e_n \rangle dg = \\
     &= \dsum_{n \in I}\dint_G \langle \phi(\rho(g) e_n) | \rho(g) e_n \rangle  dg = \dsum_{n \in I} \dint_G \langle \phi e_n |  e_n \rangle dg = \\
     &= \dsum_{n\in I} \langle \phi e_n | e_n \rangle = \tr \phi
   \end{align*}

   
 \end{proof}


 \textsc{Dimostrazione del teorema \ref{thm:ortogonalita compatto}:}
 Fissiamo una base ortonormale di $V$, $v_n$ e una base ortonormale di $W$, $w_n$. In base a quanto detto poco fa, avremo che

 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle &= \dint_G \chi_\rho(g) \overline{\chi_\sigma(g)} dg = \dint_G \tr(\rho(g)) \tr(\sigma(g)^{-1}) dg = \\
   &= \dint_G \dsum_{i \in I} \left\langle \rho(g) v_i | v_i \rangle \dsum_{j \in J} \langle \sigma(g)^{-1} w_j | w_j \right\rangle dg = \\
   &= \dsum_{i,j} \dint_G \langle \rho(g) \langle \sigma(g)^{-1} w_j | w_j \rangle v_i | v_i \rangle dg 
 \end{align*} 

 Se definiamo

 \[ \phi_{ij} \in \Hom(W, V), \quad \phi(w) = \langle w | w_j \rangle v_i \]

 e con quello che abbiamo fatto nel lemma poco sopra definiamo anche $\phi^G_{ij}$, possiamo notare che in effetti l'espressione poco sopra è proprio $\phi^G_{ij}$, ovvero

 \begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | w_j \rangle
 \end{align*}

 Se $\rho$ e $\sigma$ non sono isomorfe, allora si ha identicamente $\phi_{ij}^G = 0$ per ogni $i$ e $j$. Di conseguenza $\langle \chi_\rho | \chi_\sigma \rangle = 0$. Altrimenti, al solito posso identificare $V \cong W$ e possiamo usare l'altra parte del lemma precedente per dire che

\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j} \langle \phi_{ij}^G v_i | v_j \rangle = \dsum_{i,j} \dfrac{\tr \phi_{ij}}{dimV} \langle v_i | v_j \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{dimV} \delta_{ij} 
 \end{align*}

Tuttavia in sostanza la matriche che rappresenta $\phi_{ij}$ nelle base $v_i$ è evidentemente una matrice che ha tutti zeri tranne uno che è uno, esattamente nella posizione $(i,j)$, per cui


\begin{align*}
   \langle \chi_\rho | \chi_\sigma \rangle = \dsum_{i,j } \dfrac{\tr\phi_{ij}}{dimV} \delta_{ij} = \dsum_{i=j} \dfrac{1}{dimV} = 1
 \end{align*}

 \qed

 \begin{cor}
   Sia $G$ un gruppo compatto e $\rho$ una sua rappresentazione. Allora $\rho$ è irriducibile se e solo se $\langle \chi_\rho | \chi_\rho \rangle = 1$
 \end{cor}

 
 A questo punto possiamo concludere la dimostrazione del fatto che le rappresentazioni che abbiamo trovato prima di $SU(2)$ sono tutte e sole quelle irriducibili.

 
 \begin{thm}
   Sia $\rho$ una rappresentazione irriducibile di $SU(2)$. Allora $\exists m \in \N$ tale che $\rho \cong \rho_m$. Chiaramente le $\rho_m$ sono quelle definite nella proposizione \ref{prop:rapp su2}
 \end{thm}

 \begin{proof}
   Supponiamo per assurdo che sia $\langle \chi_\rho | \chi_{\rho_m} \rangle = 0 \quad \forall m \in \N$. Vogliamo mostrare che deve essere $\rho = 0$. 


   \begin{align*}
     0 &= \langle \chi_\rho | \chi_{\rho_m} \rangle = \dint_G \chi_\rho(g) \overline{\chi_{\rho_m} (g)} dg = \\
     &= \dfrac{1}{\pi} \dint_0^{2\pi} \chi_\rho(\theta) \dfrac{\sin((m+1)\theta)}{\sin\theta} \sin^2\theta  \ d\theta \\
     &= \dfrac{1}{\pi} \dint_{0}^{2\pi} \left(\chi_\rho (\theta) \sin\theta \right) \sin(  (m+1) \theta) \ d\theta
   \end{align*}

   A questo punto possiamo ricordare che in realtà il carattere è una funzione di classe e in particolare si ha $\chi_\rho(\theta) = \chi_\rho(-\theta)$. La funzione $f(\theta) = \chi_\rho(\theta) \sin\theta \in C_\R[0,2\pi]$ è quindi dispari. Inoltre, è evidentemente anche $\mathbb{L}^2 [0,2\pi]$. Ma noi sappiamo dall'analisi che $\{\sin(nx)\}_{n\in \N}$ è una base delle funzioni dispari di $\mathbb{L}^2[0,2\pi]$. Di conseguenza si ha $f(\theta) = 0 \quad q.o.$, ma dato che è continua, $f(\theta) = 0$ identicamente. Per lo stesso motivo, si deve avere $\chi_\rho(\theta) = 0$
   
 \end{proof}


 \newpage
 \subsubsection{Le rappresentazioni irriducibili di $SO(3)$}

 Abbiamo studiato in dettaglio due gruppi importanti, $\mathbb{S}^1$ e $SU(2)$. Sarebbe bello a questo punto sfruttare quello che abbiamo fatto per trovare le rappresentazioni di $SO(3)$, che ha notevole rilevanza fisica. In particolare, se riuscissimo a trovare un omomorfismo continuo $\phi$ da $SO(3)$ a $SU(2)$ potremmo notare sul seguente diagramma

 \[
 \begin{diagram}
   SU(2)       & \rTo^{\rho_m}        & GL(V_m) \\
   \uTo<{\phi} & \ruTo>{\hat{\rho}_m} & \\
   SO(3)       &                      & \\
 \end{diagram}
 \]

 \noindent che otteniamo gratis un sacco di rappresentazioni continue anche per $SO(3)$. Andiamo quindi a costruire questo omomorfismo.

 \begin{defn}
   Definiamo lo spazio vettoriale su $\R$, $\mathbb{E}$ in questo modo

   \[ \mathbb{E } = \{ \text{Le matrici autoaggiunte a traccia nulla }\} = \left\{
   \left(
   \begin{array}{cc}
     x_1 & x_2 + i x_3 \\
     x_2 - i x_3 & -x_1 \\
   \end{array}
   \right)
   \  \  x_i \in \R
   \right\}\]

 \end{defn}

 Ci piacerebbe far agire in modo sensato $SU(2)$ su $\mathbb{E}$, sperando di vedere la struttura di $SO(3)$. Il primo tentativo può essere quello di definire

 \[ \phi(g) x = gx \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 Tuttavia questa azione non funziona in quanto non è per niente detto che la matrice in arrivo abbia di nuovo traccia nulla. Per mantenere la traccia invariata, la cosa più sensata da fare è agire per coniugio

 \[ \phi(g) x = gxg^{-1} \qquad \forall g \in SU(2) , x \in \mathbb{E}\]

 In questo caso è evidente che la matrice in arrivo ha traccia nulla. Controlliamo che sia anche autoaggiunta

 \[ \left( \phi(g) x \right)^\dag = \left( g x g^{-1}\right)^\dag = (g^{-1})^\dag x^\dag g ^\dag = g x g^{-1} = \phi(g) x\]

 In quanto $x$ è autoaggiunta e vale $g^{-1} = g^\dag \forall g \in SU(2)$. A questo punto se riuscissimo a trovare un prodotto scalare invariante sotto l'azione di $SU(2)$ potremmo dire di aver in qualche modo mappato $SU(2)$ in $O(3)$ e ci staremmo avvicinando all'obiettivo. In particolare notiamo che la funzione

 \[ f(x) = -det (x) = x_1 ^2 + x_2^2 + x_3^2 \qquad \forall x \in \mathbb{E}\]

 È in effetti una forma quadratica definita positiva su $\mathbb{E} \cong \R^3$ ed è anche invariante sotto $SU(2)$ in sostanza per la formula di Binet. Abbiamo quindi definito $\phi: SU(2) \in O(3)$ che è in sostanza un omomorfismo continuo di gruppi compatti. Inoltre, dato che $SU(2)$ è connesso, anche $\phi(SU(2))$ sarà connesso e conterrà l'identità. Per questo motivo possiamo quindi dire che

 \[ \phi(SU(2)) \subseteq SO(3) \] 

 A questo punto, è decisamente il caso di capire meglio come $\phi$ mappi un gruppo nell'altro in modo da sfruttare quello che sappiamo per uno anche per l'altro.



\begin{lemma}
  \begin{enumerate}
    \item L'omomorfismo $\phi : SU(2) \to SO(3)$ sopra definito è surgettivo. 
    \item Il nucleo dell'omomorfismo $\phi$ è composto da solo due matrici in $SU(2)$
    \item
      \[ SU(2) / Ker(\phi) \cong SO(3) \]
  \end{enumerate}
  \end{lemma}

 \begin{proof}
   \begin{enumerate}
     \item{}
     \item{}
     \item La tesi segue dalle proposizioni precedenti applicate insieme al primo teorema di omomorfismo.
   \end{enumerate}
   \end{proof}
 

\end{document}
